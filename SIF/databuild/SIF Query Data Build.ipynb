{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import struct\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tensorflow.core.example import example_pb2\n",
    "\n",
    "sys.path.append('../src')\n",
    "import data_io, params, SIF_embedding\n",
    "\n",
    "pd.options.display.max_rows = 125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embed(weightpara=1e-3, param=None, rmpc=0,\n",
    "               wordfile = '/home/francisco/GitHub/SIF/data/glove.840B.300d.txt', \n",
    "               weightfile='/home/francisco/GitHub/SIF/auxiliary_data/enwiki_vocab_min200.txt'):\n",
    "    '''\n",
    "    wordfile:   : location of embedding data (e.g., glove embedings, can be downloaded from GloVe website)\n",
    "    weightfile: : location of TF data for words, each line is a word and its frequency\n",
    "    weightpara: : the parameter in the SIF weighting scheme, usually in range [3e-5, 3e-3]\n",
    "    rmpc:       : number of principal components to remove in SIF weighting scheme\n",
    "    '''\n",
    "    # load word vectors\n",
    "    (words, Weights) = data_io.getWordmap(wordfile)\n",
    "\n",
    "    # load word weights\n",
    "    word2weight = data_io.getWordWeight(weightfile, weightpara) # word2weight['str'] is the weight for the word 'str'\n",
    "    weight4ind = data_io.getWeight(words, word2weight) # weight4ind[i] is the weight for the i-th word\n",
    "\n",
    "    # set parameters\n",
    "    param.rmpc = rmpc\n",
    "\n",
    "    return Weights, words, word2weight, weight4ind\n",
    "\n",
    "def return_sif(sentences, words, weight4ind, param, Weights):\n",
    "    # x is the array of word indices, m is the binary mask indicating whether there is a word in that location\n",
    "    x, m = data_io.sentences2idx(sentences, words)\n",
    "    w = data_io.seq2weight(x, m, weight4ind) # get word weights\n",
    "    # get SIF embedding\n",
    "    embeddings = SIF_embedding.SIF_embedding(Weights, x, w, param) # embedding[i,:] is the embedding for sentence i\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def embed_sentences(inputpath, wordfile, weightfile, weightpara, param, rmpc, file_list):\n",
    "    Weights, words, word2weight, weight4ind = load_embed(wordfile, weightfile, weightpara, param, rmpc)\n",
    "\n",
    "    print('embeddings loaded...')\n",
    "    for file_i in file_list:\n",
    "        input_file = open(os.path.join(inputpath, file_i), 'rb')\n",
    "        c = 0\n",
    "        while input_file:\n",
    "            try:\n",
    "                clean_abstract, clean_article = return_bytes(input_file)\n",
    "            except:\n",
    "                input_file = None\n",
    "\n",
    "            print('article cleaned...')\n",
    "            break\n",
    "            embeddings = return_sif(clean_article, words, weight4ind, param, Weights)\n",
    "\n",
    "            sdf = pd.DataFrame(clean_article, columns=['sentence'])\n",
    "            sdf['clean_sentence'] = [' '.join([s for s in x if s.isalnum()]) for x in sdf['sentence'].str.split(\" \")]\n",
    "            sdf['summary'] = clean_abstract\n",
    "            sdf.ix[1:, 'summary'] = ''\n",
    "\n",
    "            embcols = ['emb_%i'%i for i in range(embeddings.shape[1])]\n",
    "            emb = pd.DataFrame(embeddings, columns = embcols)\n",
    "\n",
    "            sdf = pd.concat([sdf, emb], axis=1)\n",
    "            sdf = sdf[['summary', 'sentence', 'clean_sentence'] + sdf.columns[3:].tolist()]\n",
    "            newfile = file_i.replace(\".bin\", \"\").split(\"/\")[-1]\n",
    "            #sdf.to_csv(\"/home/francisco/GitHub/DQN-Event-Summarization/data/sif/%s_%i.csv\" % (\n",
    "            #         newfile, c\n",
    "            #         )\n",
    "            #    )\n",
    "            sdf.to_csv(\"/home/francisco/GitHub/DQN-Event-Summarization/data/testsif/%s_%i.csv\" % (\n",
    "                     newfile, c\n",
    "                     )\n",
    "                )\n",
    "            if (c % 100) == 0:\n",
    "                 print(\"Data exported to %s_%i.csv\" % (newfile, c))\n",
    "            c+= 1\n",
    "            \n",
    "def embedCNNQuery(idf, queryid, Weights, words, word2weight, weight4ind):\n",
    "    sdf = idf[idf['query_id'] == queryid].reset_index(drop=True)\n",
    "    for i in range(sdf.shape[0]):\n",
    "        tempsentence = sdf['sentence'][i]\n",
    "        ab = sent_tokenize(tempsentence)\n",
    "        clean_sentence = '. '.join([' '.join(s for s in x.split() if s.isalnum()) for x in ''.join(ab).replace(\"<s>\",\"\").split(\"</s>\")]).strip()\n",
    "        sembedding = return_sif([clean_sentence], words, weight4ind, myparams, Weights)\n",
    "        if i == 0:\n",
    "            tempsentence = sdf['query'][0]\n",
    "            true_summary = sdf['true_summary'][0]\n",
    "            ab = sent_tokenize(tempsentence)\n",
    "            clean_sentence = '. '.join([' '.join(s for s in x.split() if s.isalnum()) for x in ''.join(ab).replace(\"<s>\",\"\").split(\"</s>\")]).strip()\n",
    "            qembedding = return_sif([clean_sentence], words, weight4ind, myparams, Weights)\n",
    "            qedf = pd.DataFrame(qembedding)\n",
    "            sedf = pd.DataFrame(sembedding)\n",
    "        else:\n",
    "            sedf = pd.concat([sedf, pd.DataFrame(sembedding)], axis=0)\n",
    "\n",
    "    sedf.columns = ['embeding_%i' % i for i in range(sedf.shape[1])]\n",
    "    qedf.columns =  ['embeding_%i' % i for i in range(qedf.shape[1])]\n",
    "    \n",
    "    return {'query_embeddings': qedf, 'sentence_embeddings': sedf}\n",
    "\n",
    "def rougeScores(genSummary, refSummary):\n",
    "    genTotal, refTotal, intersection = 0., 0., 0.\n",
    "    for token in list(set(list(refSummary.keys()) + list(genSummary.keys()) )):\n",
    "        intersection += min(refSummary[token], genSummary[token])\n",
    "        refTotal += refSummary[token]\n",
    "        genTotal += genSummary[token]\n",
    "\n",
    "    recall = intersection / refTotal if refTotal > 0. else 0.\n",
    "    prec   = intersection / genTotal if genTotal > 0. else 0.\n",
    "    f1 = (2. * recall * prec) / (recall + prec) if (recall + prec) > 0. else 0.\n",
    "    \n",
    "    return recall, prec, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing data and SIF parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputfile = \"/home/francisco/GitHub/DQN-Event-Summarization/data/cnn_tokenized/cnn_data_corpus.csv\"\n",
    "inputdict = \"/home/francisco/GitHub/DQN-Event-Summarization/data/cnn_tokenized/cnn_total_corpus_smry.csv\"\n",
    "\n",
    "qdf = pd.read_csv(inputfile)\n",
    "qdict = pd.read_csv(inputdict)\n",
    "corpus_dict = dict(zip(qdict['id'], qdict['token']))\n",
    "\n",
    "df1 = pd.read_csv('/home/francisco/GitHub/DQN-Event-Summarization/data/1-output/cnn_trainingstreams.csv')\n",
    "mainpath = '/home/francisco/GitHub/DQN-Event-Summarization/'\n",
    "\n",
    "myparams = params.params()\n",
    "wp = 1e-3\n",
    "rp = 0\n",
    "query_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "Weights, words, word2weight, weight4ind = load_embed(wp, myparams, rp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = embedCNNQuery(df1, query_id, Weights, words, word2weight, weight4ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "simMatrix = pd.DataFrame(results['query_embeddings'].values.dot(results['sentence_embeddings'].values.T)).T\n",
    "\n",
    "best_sentences = [i for i, x in enumerate(simMatrix.values) if x > 5]\n",
    "sentences = [df1[df1['query_id'] == 0]['sentence'].reset_index(drop=True)[x] for x in best_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_summary = ' '.join(sentences)\n",
    "true_summary = df1['true_summary'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lead3 = ' '.join(sdf['sentence'][0:3])\n",
    "\n",
    "finalsummary = rougeScores(ts_tokenized, Counter(curr_summary.split(\" \")))\n",
    "baseline = rougeScores(ts_tokenized, Counter(lead3.split(\" \")))\n",
    "\n",
    "print(\"lead-3  recall = %.3f; precision = %.3f; f1-score = %.3f \" % (baseline[0], baseline[1], baseline[2]))\n",
    "\n",
    "print(\"learned recall = %.3f; precision = %.3f; f1-score = %.3f \" % (finalsummary[0], finalsummary[1], finalsummary[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
