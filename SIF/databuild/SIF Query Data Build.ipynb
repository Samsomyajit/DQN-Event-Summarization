{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import struct\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tensorflow.core.example import example_pb2\n",
    "\n",
    "sys.path.append('../src')\n",
    "import data_io, params, SIF_embedding\n",
    "\n",
    "pd.options.display.max_rows = 125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embed(weightpara=1e-3, param=None, rmpc=0,\n",
    "               wordfile = '/home/francisco/GitHub/SIF/data/glove.840B.300d.txt', \n",
    "               weightfile='/home/francisco/GitHub/SIF/auxiliary_data/enwiki_vocab_min200.txt'):\n",
    "    '''\n",
    "    wordfile:   : location of embedding data (e.g., glove embedings, can be downloaded from GloVe website)\n",
    "    weightfile: : location of TF data for words, each line is a word and its frequency\n",
    "    weightpara: : the parameter in the SIF weighting scheme, usually in range [3e-5, 3e-3]\n",
    "    rmpc:       : number of principal components to remove in SIF weighting scheme\n",
    "    '''\n",
    "    # load word vectors\n",
    "    (words, Weights) = data_io.getWordmap(wordfile)\n",
    "\n",
    "    # load word weights\n",
    "    word2weight = data_io.getWordWeight(weightfile, weightpara) # word2weight['str'] is the weight for the word 'str'\n",
    "    weight4ind = data_io.getWeight(words, word2weight) # weight4ind[i] is the weight for the i-th word\n",
    "\n",
    "    # set parameters\n",
    "    param.rmpc = rmpc\n",
    "\n",
    "    return Weights, words, word2weight, weight4ind\n",
    "\n",
    "def return_sif(sentences, words, weight4ind, param, Weights):\n",
    "    # x is the array of word indices, m is the binary mask indicating whether there is a word in that location\n",
    "    x, m = data_io.sentences2idx(sentences, words)\n",
    "    w = data_io.seq2weight(x, m, weight4ind) # get word weights\n",
    "    # get SIF embedding\n",
    "    embeddings = SIF_embedding.SIF_embedding(Weights, x, w, param) # embedding[i,:] is the embedding for sentence i\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def embed_sentences(inputpath, wordfile, weightfile, weightpara, param, rmpc, file_list):\n",
    "    Weights, words, word2weight, weight4ind = load_embed(wordfile, weightfile, weightpara, param, rmpc)\n",
    "\n",
    "    print('embeddings loaded...')\n",
    "    for file_i in file_list:\n",
    "        input_file = open(os.path.join(inputpath, file_i), 'rb')\n",
    "        c = 0\n",
    "        while input_file:\n",
    "            try:\n",
    "                clean_abstract, clean_article = return_bytes(input_file)\n",
    "            except:\n",
    "                input_file = None\n",
    "\n",
    "            print('article cleaned...')\n",
    "            embeddings = return_sif(clean_article, words, weight4ind, param, Weights)\n",
    "\n",
    "            sdf = pd.DataFrame(clean_article, columns=['sentence'])\n",
    "            sdf['clean_sentence'] = [' '.join([s for s in x if s.isalnum()]) for x in sdf['sentence'].str.split(\" \")]\n",
    "            sdf['summary'] = clean_abstract\n",
    "            sdf.ix[1:, 'summary'] = ''\n",
    "\n",
    "            embcols = ['emb_%i'%i for i in range(embeddings.shape[1])]\n",
    "            emb = pd.DataFrame(embeddings, columns = embcols)\n",
    "\n",
    "            sdf = pd.concat([sdf, emb], axis=1)\n",
    "            sdf = sdf[['summary', 'sentence', 'clean_sentence'] + sdf.columns[3:].tolist()]\n",
    "            newfile = file_i.replace(\".bin\", \"\").split(\"/\")[-1]\n",
    "            #sdf.to_csv(\"/home/francisco/GitHub/DQN-Event-Summarization/data/sif/%s_%i.csv\" % (\n",
    "            #         newfile, c\n",
    "            #         )\n",
    "            #    )\n",
    "            sdf.to_csv(\"/home/francisco/GitHub/DQN-Event-Summarization/data/testsif/%s_%i.csv\" % (\n",
    "                     newfile, c\n",
    "                     )\n",
    "                )\n",
    "            if (c % 100) == 0:\n",
    "                 print(\"Data exported to %s_%i.csv\" % (newfile, c))\n",
    "            c+= 1\n",
    "            \n",
    "def embedCNNQuery(sdf, params, Weights, words, word2weight, weight4ind):\n",
    "    # sdf = idf[idf['query_id'] == queryid].reset_index(drop=True)\n",
    "    for i in range(sdf.shape[0]):\n",
    "        tempsentence = sdf['sentence'][i]\n",
    "        ab = sent_tokenize(tempsentence)\n",
    "        clean_sentence = '. '.join([' '.join(s for s in x.split() if s.isalnum()) for x in ''.join(ab).replace(\"<s>\",\"\").split(\"</s>\")]).strip()\n",
    "        sembedding = return_sif([clean_sentence], words, weight4ind, params, Weights)\n",
    "        if i == 0:\n",
    "            tempsentence = sdf['query'][0]\n",
    "            true_summary = sdf['true_summary'][0]\n",
    "            ab = sent_tokenize(tempsentence)\n",
    "            clean_sentence = '. '.join([' '.join(s for s in x.split() if s.isalnum()) for x in ''.join(ab).replace(\"<s>\",\"\").split(\"</s>\")]).strip()\n",
    "            qembedding = return_sif([clean_sentence], words, weight4ind, params, Weights)\n",
    "            qedf = pd.DataFrame(qembedding)\n",
    "            sedf = pd.DataFrame(sembedding)\n",
    "        else:\n",
    "            sedf = pd.concat([sedf, pd.DataFrame(sembedding)], axis=0)\n",
    "\n",
    "    sedf.columns = ['embedding_%i' % i for i in range(sedf.shape[1])]\n",
    "    qedf.columns =  ['embedding_%i' % i for i in range(qedf.shape[1])]\n",
    "    \n",
    "    return {'query_embeddings': qedf, 'sentence_embeddings': sedf, 'true_summary': true_summary}\n",
    "\n",
    "def rougeScores(genSummary, refSummary):\n",
    "    genTotal, refTotal, intersection = 0., 0., 0.\n",
    "    for token in list(set(list(refSummary.keys()) + list(genSummary.keys()) )):\n",
    "        intersection += min(refSummary[token], genSummary[token])\n",
    "        refTotal += refSummary[token]\n",
    "        genTotal += genSummary[token]\n",
    "\n",
    "    recall = intersection / refTotal if refTotal > 0. else 0.\n",
    "    prec   = intersection / genTotal if genTotal > 0. else 0.\n",
    "    f1 = (2. * recall * prec) / (recall + prec) if (recall + prec) > 0. else 0.\n",
    "    \n",
    "    return recall, prec, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing data and SIF parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francisco/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "inputfile = \"/home/francisco/GitHub/DQN-Event-Summarization/data/cnn_tokenized/cnn_data_corpus.csv\"\n",
    "inputdict = \"/home/francisco/GitHub/DQN-Event-Summarization/data/cnn_tokenized/cnn_total_corpus_smry.csv\"\n",
    "\n",
    "qdf = pd.read_csv(inputfile)\n",
    "qdict = pd.read_csv(inputdict)\n",
    "corpus_dict = dict(zip(qdict['id'], qdict['token']))\n",
    "\n",
    "df1 = pd.read_csv('/home/francisco/GitHub/DQN-Event-Summarization/data/1-output/cnn_trainingstreams.csv')\n",
    "mainpath = '/home/francisco/GitHub/DQN-Event-Summarization/'\n",
    "\n",
    "myparams = params.params()\n",
    "wp = 1e-3\n",
    "rp = 0\n",
    "query_id = 0\n",
    "outfile = '/home/francisco/GitHub/DQN-Event-Summarization/data/testsif/sifquerydict_%i.pkl' % query_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "Weights, words, word2weight, weight4ind = load_embed(wp, mNot everyone was up in arms with the approach Haddish took in responding to the loaded question as many took to Twitter to voice their support for the comedian for her honest response.\n",
    "\n",
    "yparams, rp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = df1[df1['query_id'] == query_id].reset_index(drop=True)\n",
    "\n",
    "results = embedCNNQuery(tdf, myparams, Weights, words, word2weight, weight4ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "simMatrix = pd.DataFrame(results['query_embeddings'].values.dot(results['sentence_embeddings'].values.T)).T\n",
    "\n",
    "best_sentences = [i for i, x in enumerate(simMatrix.values) if x > 5]\n",
    "sentences = [tdf['sentence'][x] for x in best_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf['query'].values[1:] = ''\n",
    "tdf['true_summary'].values[1:] = ''\n",
    "tdf = tdf[['query_id', 'sentence_idx', 'label', 'query', 'true_summary', 'sentence']]\n",
    "final = {'embeddings': results, 'data': tdf}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_summary = ' '.join(sentences)\n",
    "true_summary = tdf['true_summary'][0]\n",
    "lead3 = ' '.join(tdf['sentence'][0:3])\n",
    "\n",
    "ts_tokenized = Counter(true_summary.split(\" \"))\n",
    "ps_tokenized = Counter(curr_summary.split(\" \"))\n",
    "l3_tokenized = Counter(lead3.split(\" \"))\n",
    "\n",
    "finalsummary = rougeScores(ts_tokenized, ps_tokenized)\n",
    "baseline = rougeScores(ts_tokenized, l3_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lead-3  recall = 1.000; precision = 0.148; f1-score = 0.258 \n",
      "learned recall = 0.604; precision = 0.116; f1-score = 0.194 \n"
     ]
    }
   ],
   "source": [
    "print(\"lead-3  recall = %.3f; precision = %.3f; f1-score = %.3f \" % (baseline[0], baseline[1], baseline[2]))\n",
    "\n",
    "print(\"learned recall = %.3f; precision = %.3f; f1-score = %.3f \" % (finalsummary[0], finalsummary[1], finalsummary[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(final, open(outfile, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dumping a set of 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query 0 written to /home/francisco/GitHub/DQN-Event-Summarization/data/testsif/sifquerydict_0.pkl\n",
      "query 1 written to /home/francisco/GitHub/DQN-Event-Summarization/data/testsif/sifquerydict_1.pkl\n",
      "query 2 written to /home/francisco/GitHub/DQN-Event-Summarization/data/testsif/sifquerydict_2.pkl\n",
      "query 3 written to /home/francisco/GitHub/DQN-Event-Summarization/data/testsif/sifquerydict_3.pkl\n",
      "query 4 written to /home/francisco/GitHub/DQN-Event-Summarization/data/testsif/sifquerydict_4.pkl\n",
      "query 5 written to /home/francisco/GitHub/DQN-Event-Summarization/data/testsif/sifquerydict_5.pkl\n",
      "query 6 written to /home/francisco/GitHub/DQN-Event-Summarization/data/testsif/sifquerydict_6.pkl\n",
      "query 7 written to /home/francisco/GitHub/DQN-Event-Summarization/data/testsif/sifquerydict_7.pkl\n",
      "query 8 written to /home/francisco/GitHub/DQN-Event-Summarization/data/testsif/sifquerydict_8.pkl\n",
      "query 9 written to /home/francisco/GitHub/DQN-Event-Summarization/data/testsif/sifquerydict_9.pkl\n"
     ]
    }
   ],
   "source": [
    "for query_id in range(10):\n",
    "    myparams = params.params()\n",
    "    myparams.rmpc = 0\n",
    "    outfile = '/home/francisco/GitHub/DQN-Event-Summarization/data/testsif/sifquerydict_%i.pkl' % query_id\n",
    "    tdf = df1[df1['query_id'] == query_id].reset_index(drop=True)\n",
    "    tdf['query'].values[1:] = ''\n",
    "    tdf['true_summary'].values[1:] = ''\n",
    "    \n",
    "    tdf = tdf[['query_id', 'sentence_idx', 'label', 'query', 'true_summary', 'sentence']]\n",
    "    results = embedCNNQuery(tdf, myparams, Weights, words, word2weight, weight4ind)\n",
    "    final = {'embeddings': results, 'data': tdf}    \n",
    "    pickle.dump(final, open(outfile, 'wb'))\n",
    "    print('query %i written to %s' % (query_id, outfile))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
