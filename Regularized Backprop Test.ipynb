{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "...Utils file loaded\t\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "require 'optim'\n",
    "require 'io'\n",
    "require 'torch'\n",
    "require 'nn'\n",
    "require 'rnn'\n",
    "require 'csvigo'\n",
    "require 'cutorch'\n",
    "require 'cunn'\n",
    "require 'cunnx'\n",
    "\n",
    "dl = require 'dataload'\n",
    "cmd = torch.CmdLine()\n",
    "dofile(\"Code/utils.lua\")\n",
    "dofile(\"Code/utilsNN.lua\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Running on a subset of 20 observations\t\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "...query loaded\t\n",
       "Running LSTM model to learn f1\t\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Using adaptive regularization\t\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapath = 'data/0-output/'\n",
    "\n",
    "Tensor = torch.Tensor\n",
    "LongTensor = torch.LongTensor\n",
    "ByteTensor = torch.ByteTensor\n",
    "\n",
    "use_cuda = false\n",
    "\n",
    "thresh = 0\n",
    "edim = 10\n",
    "metric = 'f1'\n",
    "inputs = loadMetadata(datapath .. \"dqn_metadata.csv\")                                               \n",
    "stoplist = loadStopdata(datapath .. 'stopwordids.csv')\n",
    "adapt = true\n",
    "metric ='f1'\n",
    "use_cuda = false\n",
    "embeddingSize = 20\n",
    "\n",
    "nepochs = 2\n",
    "gamma= 0.\n",
    "learning_rate = 0.1\n",
    "epsilon = 1\n",
    "stopwordlist = stopwords \n",
    "mem_size = 100\n",
    "optimParams = { learningRate = learning_rate }\n",
    "\n",
    "vocabSize, query_data = initialize_variables(inputs, 20, datapath, 5, 30, stoplist, thresh, use_cuda)\n",
    "\n",
    "print(\"...query loaded\")\n",
    "model = buildModel(model, vocabSize, embeddingSize, metric, adapt, usecuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maskLayer = nn.MaskedSelect()\n",
    "\n",
    "SKIP = 1\n",
    "SELECT = 2\n",
    "\n",
    "math.randomseed(420)\n",
    "torch.manualSeed(420)\n",
    "criterion = nn.MSECriterion()\n",
    "\n",
    "if adapt then \n",
    "    criterion = nn.ParallelCriterion():add(nn.MSECriterion()):add(nn.BCECriterion())\n",
    "end\n",
    "\n",
    "\n",
    "if use_cuda then\n",
    "    criterion = criterion:cuda()\n",
    "    model = model:cuda()\n",
    "end\n",
    "params, gradParams = model:getParameters()\n",
    "\n",
    "query_id = 1 \n",
    "memory, rougeRecall, rougePrecision, rougeF1, qValues = forwardpass(\n",
    "                query_data, query_id, model, epsilon, gamma, \n",
    "                metric, thresh, stopwordlist, adapt, use_cuda\n",
    ")\n",
    "fullmemory = memory\n",
    "-- Stacking data\n",
    "fullmemory = stackMemory(memory, fullmemory, mem_size, adapt, use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- xinput = fullmemory[1]\n",
    "-- reward = fullmemory[2]\n",
    "-- actions_in = fullmemory[3]\n",
    "\n",
    "xinput = memory[1]\n",
    "actions_in = memory[3]\n",
    "reward = memory[2]:resize(20, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xinput = {xinput[1]:double(), xinput[2]:double(), xinput[3]:double()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  1 : DoubleTensor - size: 20x5\n",
       "  2 : DoubleTensor - size: 20x3\n",
       "  3 : DoubleTensor - size: 20x30\n",
       "}\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xinput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31\t\n",
       "31\t\n",
       "31\t\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "...javierarceo/torch/install/share/lua/5.1/nn/Container.lua:67: \nIn 1 module of nn.ConcatTable:\nIn 1 module of nn.Sequential:\nIn 2 module of nn.ParallelTable:\nIn 3 module of nn.Sequential:\n...iscojavierarceo/torch/install/share/lua/5.1/rnn/LSTM.lua:184: assertion failed!\nstack traceback:\n\t[C]: in function 'assert'\n\t...iscojavierarceo/torch/install/share/lua/5.1/rnn/LSTM.lua:184: in function '_updateGradInput'\n\t...eo/torch/install/share/lua/5.1/rnn/AbstractRecurrent.lua:59: in function 'updateGradInput'\n\t...avierarceo/torch/install/share/lua/5.1/rnn/Sequencer.lua:121: in function <...avierarceo/torch/install/share/lua/5.1/rnn/Sequencer.lua:106>\n\t[C]: in function 'xpcall'\n\t...javierarceo/torch/install/share/lua/5.1/nn/Container.lua:63: in function 'rethrowErrors'\n\t...avierarceo/torch/install/share/lua/5.1/nn/Sequential.lua:55: in function <...avierarceo/torch/install/share/lua/5.1/nn/Sequential.lua:50>\n\t[C]: in function 'xpcall'\n\t...javierarceo/torch/install/share/lua/5.1/nn/Container.lua:63: in function 'rethrowErrors'\n\t...erarceo/torch/install/share/lua/5.1/nn/ParallelTable.lua:19: in function 'updateGradInput'\n\t...\n\t[C]: in function 'xpcall'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:210: in function <...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:174>\n\t...ojavierarceo/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:389: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x010490bb90\n\nWARNING: If you see a stack trace below, it doesn't point to the place where this error occurred. Please use only the one above.\nstack traceback:\n\t[C]: in function 'error'\n\t...javierarceo/torch/install/share/lua/5.1/nn/Container.lua:67: in function 'rethrowErrors'\n\t...vierarceo/torch/install/share/lua/5.1/nn/ConcatTable.lua:35: in function 'backward'\n\t[string \"model:forget()...\"]:26: in main chunk\n\t[C]: in function 'xpcall'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:210: in function <...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:174>\n\t...ojavierarceo/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:389: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x010490bb90",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "...javierarceo/torch/install/share/lua/5.1/nn/Container.lua:67: \nIn 1 module of nn.ConcatTable:\nIn 1 module of nn.Sequential:\nIn 2 module of nn.ParallelTable:\nIn 3 module of nn.Sequential:\n...iscojavierarceo/torch/install/share/lua/5.1/rnn/LSTM.lua:184: assertion failed!\nstack traceback:\n\t[C]: in function 'assert'\n\t...iscojavierarceo/torch/install/share/lua/5.1/rnn/LSTM.lua:184: in function '_updateGradInput'\n\t...eo/torch/install/share/lua/5.1/rnn/AbstractRecurrent.lua:59: in function 'updateGradInput'\n\t...avierarceo/torch/install/share/lua/5.1/rnn/Sequencer.lua:121: in function <...avierarceo/torch/install/share/lua/5.1/rnn/Sequencer.lua:106>\n\t[C]: in function 'xpcall'\n\t...javierarceo/torch/install/share/lua/5.1/nn/Container.lua:63: in function 'rethrowErrors'\n\t...avierarceo/torch/install/share/lua/5.1/nn/Sequential.lua:55: in function <...avierarceo/torch/install/share/lua/5.1/nn/Sequential.lua:50>\n\t[C]: in function 'xpcall'\n\t...javierarceo/torch/install/share/lua/5.1/nn/Container.lua:63: in function 'rethrowErrors'\n\t...erarceo/torch/install/share/lua/5.1/nn/ParallelTable.lua:19: in function 'updateGradInput'\n\t...\n\t[C]: in function 'xpcall'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:210: in function <...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:174>\n\t...ojavierarceo/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:389: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x010490bb90\n\nWARNING: If you see a stack trace below, it doesn't point to the place where this error occurred. Please use only the one above.\nstack traceback:\n\t[C]: in function 'error'\n\t...javierarceo/torch/install/share/lua/5.1/nn/Container.lua:67: in function 'rethrowErrors'\n\t...vierarceo/torch/install/share/lua/5.1/nn/ConcatTable.lua:35: in function 'backward'\n\t[string \"model:forget()...\"]:26: in main chunk\n\t[C]: in function 'xpcall'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:210: in function <...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:174>\n\t...ojavierarceo/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:389: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x010490bb90"
     ]
    }
   ],
   "source": [
    "model:forget()\n",
    "\n",
    "predTotal = model:forward(xinput)\n",
    "predQ = predTotal[1]\n",
    "predReg = predTotal[2]\n",
    "print(model:get(1):get(1):get(1):get(3).module.step)\n",
    "print(model:get(1):get(1):get(2):get(3).module.step)\n",
    "print(model:get(1):get(1):get(3):get(3).module.step)\n",
    "-- model:reset()\n",
    "predQOnActions = maskLayer:forward({predQ, actions_in})\n",
    "\n",
    "-- ones = torch.ones(predQ:size(1))\n",
    "\n",
    "ones = torch.ones(predQ:size(1)):resize(predQ:size(1), 1)\n",
    "lossf = criterion:forward({predQOnActions, predReg}, {reward, ones})\n",
    "\n",
    "gradOutput = criterion:backward({predQOnActions, predReg}, {reward, ones})\n",
    "gradMaskLayer = maskLayer:backward({predQ, actions_in}, gradOutput[1] )\n",
    "\n",
    "-- predQ:size(), actions_in:size(), reward:size(), ones:size()\n",
    "-- {xinput, gradMaskLayer, gradOutput}\n",
    "-- print(model:get(1):get(1):get(1):get(3).module.step)\n",
    "-- print(model:get(1):get(1):get(2):get(3).module.step)\n",
    "-- print(model:get(1):get(1):get(3):get(3).module.step)\n",
    "-- print(model)\n",
    "model:backward(xinput, {gradMaskLayer[1], gradOutput[2]} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for k, xin, reward in dataloader:sampleiter(batch_size, memsize) do\n",
    "    xinput = xin[1]\n",
    "    actions_in = xin[2]\n",
    "    predTotal = model:forward(xinput)                \n",
    "    \n",
    "    predQ = predTotal[1]\n",
    "    predReg = predTotal[2]\n",
    "    print(xinput, predTotal, predQ:size(1))\n",
    "    \n",
    "    predQOnActions = maskLayer:forward({predQ, actions_in}) \n",
    "\n",
    "    ones = torch.ones(predQ:size(1)):resize(predQ:size(1), 1)\n",
    "    lossf = criterion:forward({predQOnActions, predReg}, {reward, ones})\n",
    "    \n",
    "    gradOutput = criterion:backward({predQOnActions, predReg}, {reward, ones})\n",
    "    gradMaskLayer = maskLayer:backward({predQ, actions_in}, gradOutput[1])\n",
    "    \n",
    "--     model:backward(xinput, {gradMaskLayer[1], gradOutput[1]} )\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k, xin, reward in dataloader:sampleiter(batch_size, memsize) do\n",
    "    xinput = xin[1]\n",
    "    actions_in = xin[2]\n",
    "\n",
    "    if use_cuda then\n",
    "        maskLayer = nn.MaskedSelect():cuda()\n",
    "        actions_in = torch.CudaByteTensor(#actions_in):copy(actions_in)\n",
    "    end\n",
    "\n",
    "    local function feval(params)\n",
    "        gradParams:zero()\n",
    "        if adapt then\n",
    "            local predTotal = model:forward(xinput)                \n",
    "            local predQ = predTotal[1]\n",
    "            local predReg = predTotal[2]\n",
    "            local predQOnActions = maskLayer:forward({predQ, actions_in}) \n",
    "            local ones = torch.ones(predQ:size(1)):resize(predQ:size(1))\n",
    "            lossf = criterion:forward({predQOnActions, predReg}, {reward, ones})\n",
    "            local gradOutput = criterion:backward({predQOnActions, predReg}, {reward, ones})\n",
    "            local gradMaskLayer = maskLayer:backward({predQ, actions_in}, gradOutput[1])\n",
    "            print(#gradMaskLayer[2], #gradOutput[1])\n",
    "            model:backward(xinput, {gradMaskLayer[1], gradOutput[2]})\n",
    "        else \n",
    "            local predQ = model:forward(xinput)\n",
    "            local predQOnActions = maskLayer:forward({predQ, actions_in}) \n",
    "            lossf = criterion:forward(predQOnActions, reward)\n",
    "            local gradOutput = criterion:backward(predQOnActions, reward)\n",
    "            local gradMaskLayer = maskLayer:backward({predQ, actions_in}, gradOutput)\n",
    "            print(#gradOutput)\n",
    "            model:backward(xinput, gradMaskLayer[1])\n",
    "        end \n",
    "        return lossf, gradParams\n",
    "    end\n",
    " --- optim.rmsprop returns \\theta, f(\\theta):= loss function\n",
    " _, lossv  = optim.rmsprop(feval, params, optimParams)\n",
    "end\n",
    "print(lossv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = backProp(fullmemory, params, gradParams, optimParams, model, \n",
    "    criterion, batch_size, mem_size, adapt, use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "  1 : DoubleTensor - size: 20x5\n",
       "  2 : DoubleTensor - size: 20x3\n",
       "  3 : DoubleTensor - size: 20x30\n",
       "}\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xinput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = buildModel(model, vocabSize, embeddingSize, metric, adapt, usecuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function buildModel(model, vocabSize, embeddingSize, metric, adapt, use_cuda)\n",
    "    -- Small experiments seem to show that the Tanh activations performed better\\\n",
    "    --      than the ReLU for the bow model\n",
    "    if model == 'bow' then\n",
    "        print(string.format(\"Running bag-of-words model to learn %s\", metric))\n",
    "        sentenceLookup = nn.Sequential()\n",
    "                    :add(nn.LookupTableMaskZero(vocabSize, embeddingSize))\n",
    "                    :add(nn.Sum(2, 3, true)) -- Not averaging blows up model so keep this true\n",
    "                    :add(nn.Tanh())\n",
    "    else\n",
    "        print(string.format(\"Running LSTM model to learn %s\", metric))\n",
    "        sentenceLookup = nn.Sequential()\n",
    "                    :add(nn.LookupTableMaskZero(vocabSize, embeddingSize))\n",
    "                    :add(nn.SplitTable(2))\n",
    "                    :add(nn.Sequencer(nn.LSTM(embeddingSize, embeddingSize)))\n",
    "                    :add(nn.SelectTable(-1))            -- selects last state of the LSTM\n",
    "                    :add(nn.Linear(embeddingSize, embeddingSize))\n",
    "                    :add(nn.ReLU())\n",
    "    end\n",
    "    local queryLookup = sentenceLookup:clone(\"weight\", \"gradWeight\") \n",
    "    local summaryLookup = sentenceLookup:clone(\"weight\", \"gradWeight\")\n",
    "    local pmodule = nn.ParallelTable()\n",
    "                :add(sentenceLookup)\n",
    "                :add(queryLookup)\n",
    "                :add(summaryLookup)\n",
    "\n",
    "    if model == 'bow' then\n",
    "        predict = nn.Sequential()\n",
    "                :add(nn.JoinTable(2))\n",
    "                :add(nn.Tanh())\n",
    "                :add(nn.Linear(embeddingSize * 3, 2))\n",
    "    else\n",
    "        predict = nn.Sequential()\n",
    "                :add(nn.JoinTable(2))\n",
    "                :add(nn.ReLU())\n",
    "                :add(nn.Linear(embeddingSize * 3, 2))\n",
    "    end\n",
    "\n",
    "    if adapt then \n",
    "        print(\"Using adaptive regularization\")\n",
    "        local predictmodule = nn.Sequential()\n",
    "                    :add(pmodule)\n",
    "\n",
    "        local regmodel = nn.Sequential()\n",
    "            :add(nn.JoinTable(2))\n",
    "            :add(nn.Linear(embeddingSize * 3, 1))\n",
    "            :add(nn.LogSigmoid())\n",
    "            :add(nn.SoftMax())\n",
    "\n",
    "        local qmodel = nn.Sequential()\n",
    "            :add(nn.JoinTable(2))\n",
    "            :add(nn.Linear(embeddingSize * 3, 2))\n",
    "\n",
    "        local final = nn.ConcatTable()\n",
    "            :add(regmodel)\n",
    "            :add(qmodel)\n",
    "\n",
    "        nnmodel = final\n",
    "        else\n",
    "            pmodule:add(predict)\n",
    "    end\n",
    "\n",
    "    if use_cuda then\n",
    "        return nnmodel:cuda()\n",
    "    end\n",
    "    return nnmodel\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model:forget()\n",
    "\n",
    "predTotal = model:forward(xinput)\n",
    "predQ = predTotal[1]\n",
    "predReg = predTotal[2]\n",
    "\n",
    "-- model:reset()\n",
    "predQOnActions = maskLayer:forward({predQ, actions_in})\n",
    "\n",
    "-- ones = torch.ones(predQ:size(1))\n",
    "ones = torch.ones(predQ:size(1)):resize(predQ:size(1), 1)\n",
    "\n",
    "lossf = criterion:forward({predQOnActions, predReg}, {reward, ones})\n",
    "\n",
    "gradOutput = criterion:backward({predQOnActions, predReg}, {reward, ones})\n",
    "gradMaskLayer = maskLayer:backward({predQ, actions_in}, gradOutput[1] )\n",
    "model:backward(xinput, {gradMaskLayer[1], gradOutput[2]} )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
