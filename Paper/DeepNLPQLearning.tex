\documentclass[12pt]{article}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{ bm }
\usepackage{amsmath }
\usepackage{hyperref}

\usepackage{tikz}
\usetikzlibrary{shapes,arrows,shadows}
%\usepackage{amsmath,bm,times}
\newcommand{\mx}[1]{\mathbf{\bm{#1}}} % Matrix command
\newcommand{\vc}[1]{\mathbf{\bm{#1}}} % Vector command


\usepackage{algorithm,algorithmic,amsmath}
%\usepackage[linesnumbered,ruled]{algorithm2e}


\title{Deep Q-Learning for Event Summarization }

\author{
	Francisco Javier Arceo \\ \href{mailto: fja2114@columbia.edu}{\small fja2114@columbia.edu} 
		\and  
	Chris Kedzie \\ \href{mailto: kedzie@cs.columbia.edu}{\small kedzie@cs.columbia.edu} 
	}

\begin{document}
\maketitle

\begin{abstract}
We present a streaming extraction policy for massive document summarization learned using a deep reinforcement learning recurrent neural network architecture. The rewards of our network are evaluated through manually reviewed summaries, which allow us to specify an end-to-end framework to optimize our extractive policy.  By using raw text from our articles and mapping words into a higher embedding dimension through our recurrent neural network, we are able to learn a more robust representation than traditional n-gram models. We benchmark our model against random decisions, bag-of-words, and bag-of-bigrams models.\footnote{ \url{https://github.com/franciscojavierarceo/DQN-Event-Summarization} }
\end{abstract}

\bibliographystyle{plain}

%\begin{abstract}
%\end{abstract}

\section{Introduction}

Crisis informatics is becoming an increasingly popular area of study in machine learning with more recent research focusing on extractive summarization (e.g., \cite{kedzie2015predicting} and \cite{kedzieextractive}) of multi-document summarization. The approach by \cite{kedzie2015predicting}  has shown that it is possible to select relevant sentences from a massive number of documents on the web to create summaries with meaningful content by adapting classifiers to maximize search policies. These systems operate in a streaming fashion and are capable of evaluating each sentence within each article to decide whether or not to include or ignore the sentence. 

Unfortunately, these systems have still been shown to fall short of heuristic algorithms [citation needed], which may be due to inadequate capturing of the rich structure and often idiosyncratic information by traditional n-gram language models. \cite{bengio2003neural} have shown that natural language processing can be mapped to a higher order dimension to represent more powerful features for various language modeling tasks. These embeddings have proven incredibly powerful on a variety of different natural language processing tasks [citations needed].

In this paper we show that a deep recurrent neural network with a long short term memory (LSTM) is able to successfully learn an extractive summary policy by encoding the action, state, and reward into a Q-Learning model, similar to that of  \cite{hausknecht2015deep}. We show that on a variety of different metrics, our specification is able to reach state-of-the-art performance.

\section{Related Work}

\section{Extractive Streaming Summarization}

  In the extractive streaming summarization task, we are given as input
  a query, e.g. a short text description of a topic or an event, and 
  document stream, a time ordered set of sentences 
  relevant to the query. Starting with an initially empty summary,  
  an extractive, streaming summarization algorithm is intended to 
  examine each sentence in order and when new and important (relative to the 
  query) information is identified, add that sentence to the summary. 

  Implicit to this problem is the notion of system time -- the 
  summarization algorithm can only examine sentences that occur in the stream
  before the current system time. Advancing the system time gives the algorithm
  access to more sentences, although in practice the stream is sufficiently 
  large enough that choices have to be made about how much history can be 
  kept in memory. For many domains, e.g. crisis informatics, it is preferable
  for a summarization algorithm to identify important information as early as 
  possible, and so the objective function should penalize a large disparity
  between the time a piece of information is first available to the algorithm
  and the system time at which that information is actually added to the 
  summary.

  Previous work in this area has either incremented the system time in fixed
  increments (e.g. an hour) 
  \cite{mccreadie2014incremental,kedzie2015predicting} or operated in
  a fully online setting \cite{guo2013updating,kedzie2016real}. In both cases 
  explicitly 
  modeling the current state of the summary, the stream, and their relationship
  with the query is quite complicated and exhibits non-linear dynamics that 
  are difficult characterize in traditional feature based models.

  Additionaly, the structured nature of the sentence selection task 
  (sentence selection is
  highly dependent on the current summary state) suggests that imitation 
  or reinforcement learning are necessary to obtain parity between training
  and testing feature distributions. 

  This leads us to explore deep Q networks (DQN) for two reasons. 
  First, both the 
  representation and mode of interaction between the stream, summary, and 
  query can be learned. 
  Second, the learned Q function (plus random noise) controls the state 
  space that is explored, ensuring more consistency between train and test 
  distributions than for example naive imitation learning 
  (possibly -- I'm not totally happy with this sentence).

\subsection{Problem Definition}
   
    DQN learns by using an a $\epsilon$-greedy search policy to generate a 
    sequence of state, action, next state, reward 4-tuples using the current
    learned Q-network to evaluate candidate actions. These tuples are sampled
    from and used to estimate the true Q function.

    \textbf{States} 
    In our setup a state $s(x_{t},\tilde{y}_{t}, q)$ is a function
    of the stream $X$ observed up to the current system time $t$, the state
    of the current summary $\tilde{Y}$ at system time $t$, and the query $q$.
    For brevity we will use $s(t,q)$ where the dependence on 
    $x_{t},\tilde{Y}_{t}$ is assumed.
    $s(t,q)$ is itself three recurrent neural networks, one for encoding 
    the summary, the stream, and the query respectively.

    \textbf{Actions} 
    The set of possible actions at each time step is 
    $\mathcal{A} = \{select, skip\}$ where $select$ corresponds to adding the 
    current sentence $x_t$ to the summary and incrementing the current system
    time, 
    \begin{align*}
        \tilde{y}_{t + 1} &= \tilde{Y}_{t} \cup \{ x_t \} \\
        t &= t + 1    
    \end{align*}
    or $skip$ where only $t$ is incremented without changing the current 
    summary
    \begin{align*}
        \tilde{y}_{t + 1} &= \tilde{y}_{ t}  \\
        t &= t + 1.
    \end{align*}


    \textbf{Reward} 
    The reward for a given action will be measured by relative gain in 
    ROUGE-2 F1 score of the predicted summary $\tilde{y}_{t}$ measured
    against a gold standard summary $Y$.


     
    When only one gold summary reference is used, ROUGE-N Recall is calculated
    as 

    \[ \textrm{ROUGE-NR}(\tilde{y}, Y) = 
        \frac{\sum_{g \in \textrm{ngrams}(Y,N)} 
        \min \left(\textrm{count}(g, \tilde{y}), \textrm{count}(g, Y)\right)}{
        \sum_{g \in \textrm{ngrams}(Y,N)} 
        \textrm{count}(g, Y)
        }
    \]

    where $\textrm{ngrams}(Y, N)$ returns the set of ngrams of order $N$ in 
    the summary $Y$ and $\textrm{count}(g, Y)$ is the count of occurences of
    ngram $g$ in $Y.$

    Similarly, ROUGE-N Precision is calculated as 
    \[ \textrm{ROUGE-NP}(\tilde{Y}, Y) = 
        \frac{\sum_{g \in \textrm{ngrams}(Y,N)} 
        \min \left(\textrm{count}(g, \tilde{Y}), \textrm{count}(g, Y)\right)}{
            \sum_{g \in \textrm{ngrams}(\tilde{Y},N)} 
            \textrm{count}(g, \tilde{Y})
        }
    \]

    and the $F_1$ is simply the harmonic mean of the two:
    \[ \textrm{ROUGE-NF1}(\tilde{Y}, Y) = \frac{ 2 \times 
    \textrm{ROUGE-NP}(\tilde{Y}, Y) \times \textrm{ROUGE-NR}(\tilde{Y}, Y)
    }{ \textrm{ROUGE-NP}(\tilde{Y}, Y) + \textrm{ROUGE-NR}(\tilde{Y}, Y) } \]


    The reward $r$ at time $t$ is then:
    \[ r_t = \textrm{ROUGE-NF1}(\tilde{y}_{t}, Y) - 
    \textrm{ROUGE-NF1}(\tilde{y}_{t-1}, Y) \]

%    TODO: Think about how to incorporate a time penalty into the reward.

\subsection{Architecture}

The architecture for our DQN-LSTM is nearly identical to that of  \cite{narasimhan2015language} with modifications where appropriate. We learn the parameters using stochastic gradient descent with RMSprop \cite{hinton2012lecture}, where we minimize the loss function below: 

\begin{equation}
\mathcal{L}_i(\theta_i) =  \textrm{E}_{\hat{s}, \hat{a}}[ \big (y_i - Q(\hat{s}, \hat{a}; \theta_i) \big )^2]
\end{equation}

where 
\begin{equation}
y_i = \textrm{E}_{\hat{s}, \hat{a}}[ r_i + \gamma \textrm{max}_{a'} Q(s', a' ; \theta_{i-1}) | s', a' ].
\end{equation}

The updates are computed using the gradient update for $L_i(\theta_i)$

\begin{equation}
\Delta L_i(\theta_i) = \textrm{E}_{\hat{s}, \hat{a}}[ 2  \big (y_i - Q(\hat{s}, \hat{a}; \theta_i) \big ) \Delta_{\theta_{i}}  Q(\hat{s}, \hat{a}; \theta_i)].
\end{equation}



\center
% Define the layers to draw the diagram
\pgfdeclarelayer{background}
\pgfdeclarelayer{foreground}
\pgfsetlayers{background,main,foreground}
    \tikzstyle{sensor}=[draw, fill=blue!10, text width=5em,  text centered, minimum height=2.0em,drop shadow]
    \tikzstyle{term}=[draw, fill=gray!10, text width=5em,  text centered, minimum height=2.0em,drop shadow]
    \tikzstyle{wa} = [sensor, fill=gray!10, text width=5em,  minimum height=2em, rounded corners, drop shadow]
    \tikzstyle{wa2} = [sensor, fill=gray!10, text width=6em,  minimum height=12em, rounded corners, drop shadow]
    \tikzstyle{om} = [draw, fill=red!10, text width=5em,  text centered, minimum height=2.0em,drop shadow]

    \begin{tikzpicture}
        \node (wa) [wa2] {Joined \textbf{Linear Layer}  \\ \scriptsize (3 x Embedding) };
        \path (wa.west)+(-3.0 ,  2.0) node (asr1)[sensor] {$Query$ \textbf{LSTM}};
        \path (wa.west)+(-3.0 ,  0.0) node (asr2)[sensor] {$Sentence_t$  \textbf{LSTM}};
        \path (wa.west)+(-3.0 , -2.0) node (asr3)[sensor] {$Summary_t$  \textbf{LSTM}};
        \path (wa.east)+(3.0,  1) node (vote) [term] {\textbf{ReLU} \\ \scriptsize $\{Select\}$ };
        \path (wa.east)+(3.0, -1) node (vote2) [term] {\textbf{ReLU} \\ \scriptsize $\{Skip\}$ } ;
        \path (wa.east)+(6.5,  0) node (output) [om] {\textbf{Max}} ;

        \path [draw, ->] (asr1.south) -- node [above] {}     (asr2.north) ;
        \path [draw, ->] (asr2.north) -- node [above] {}     (asr1.south) ;
        \path [draw, ->] (asr2.south) -- node [above] {}     (asr3.north) ;
        \path [draw, ->] (asr3.north) -- node [above] {}     (asr2.south) ;
        \path [draw, ->] (asr1.east) -- node [above] {}     (wa.125) ;
        \path [draw, ->] (asr2.east) -- node [above] {}     (wa.180);
        \path [draw, ->] (asr3.east) -- node [above] {}     (wa.235);
        \path [draw, ->] (wa.east) -- node [above] {}       (vote.west);
        \path [draw, ->] (wa.east) -- node [above] {}       (vote2.west);
        \path [draw, ->] (vote.east) -- node [above] {}     (output.175);
        \path [draw, ->] (vote2.east) -- node [above] {}   (output.185);

     \begin{pgfonlayer}{background}[transparency group,opacity=.5]
            \path (asr1.north  |- asr1.south)+(-2.0, 1.6) node (a) {};
            \path (asr3.west  |- vote.east)+(+10.0, -3.5) node (b) {};
            \path (asr3.west  |- vote.east)+(+10.0, -2.5) node (c) {};
            \path (asr3.east  |- output.east)+(+13, -4.50) node (d) {};
            \path[fill=white!20,rounded corners, draw=black!100, dashed]         (a) rectangle (d);
        \end{pgfonlayer}

     \begin{pgfonlayer}{background}[transparency group,opacity=.5]
            \path (asr1.north  |- asr1.south)+(-1.7, 1.3) node (a) {};
            \path (asr3.west  |- asr3.east)+(+10.0, -3.5) node (b) {};
            \path (asr3.west  |- asr3.east)+(+10.0, -2.5) node (c) {};
            \path (asr3.west  |- asr3.east)+(+2.9, -1.50) node (d) {};
            \path[fill=white!20,rounded corners, draw=blue!100, dashed]         (a) rectangle (d);
        \end{pgfonlayer}

     \begin{pgfonlayer}{background}[transparency group,opacity=.5]
            \path (asr1.north  |- asr1.south)+(2.7, 1.3) node (a) {};
            \path (asr3.west  |- asr3.east)+(+10.0, -3.5) node (b) {};
            \path (asr3.west  |- asr3.east)+(+10.0, -2.5) node (c) {};
            \path (asr3.west  |- asr3.east)+(+11.5, -1.50) node (d) {};
            \path[fill=white!20,rounded corners, draw=gray!100, dashed]         (a) rectangle (d);
        \end{pgfonlayer}

     \begin{pgfonlayer}{background}[transparency group,opacity=.5]
            \path (asr1.north  |- asr1.south)+(10.7, 1.3) node (a) {};
            \path (asr3.west  |- asr3.east)+(+10.0, -3.5) node (b) {};
            \path (asr3.west  |- asr3.east)+(+10.0, -2.5) node (c) {};
            \path (asr3.west  |- asr3.east)+(+15.1, -1.50) node (d) {};
            \path[fill=white!20,rounded corners, draw=red!50, dashed]         (a) rectangle (d);
        \end{pgfonlayer}

        \path (wa.west) + (-2.90, -3.2)  node (vote.east) {Input State}; %{\textbf{Input State}};
        \path (wa.west) + (3.5,    -3.2)  node (vote.east) {Summary-Action Representation }; %{\textbf{Summary-Action Representation} };
        \path (wa.west) + (9.30,  -3.2)  node (vote.east) {Output State}; %{\textbf{Output State}};
        \path (wa.east) + (0.5, -4.2)  node (vote.east) {\textbf{Figure 1: DQN-LSTM Architecture}};
\end{tikzpicture}

\begin{algorithm}
    \bf{Input:} { \rm  \{$\mathcal{Q}$: Event queries, $X_q$: Input sentences, $N$: Number of epochs\} } \\
    \underline{$\bf{Output: }$ \rm \{$\hat{\pi}$: extraction policy, $\tilde{Y}$: event summary for query $q$\} }
\begin{algorithmic}[1]
\STATE \rm Initialize memory $\Gamma =  \{\emptyset \}^{\mathcal{|Q|}}_{q=1}$
\STATE \rm Initialize summaries $\tilde{Y} = \{\emptyset \}^{\mathcal{|Q|}}_{q=1} $
\STATE \rm Initialize action-value function $\hat{Q}$ with random weights
\FOR{$epoch=1,..,N\ $}
	\FOR{$q \in \mathcal{Q}$}
		\STATE $X_{q}$= \{Extract $t=1,...,T_q$ sentences for query $q$\}
		\STATE $\tilde{Y}_{q}$= \{Extract $t=1,...,T_q$ summaries for query $q$\}
		\FOR{$(x_t, \tilde{y}_t) \in (X_q, \tilde{Y}_q), $ }
			\STATE Set $s_t = s(x_t, \tilde{y}_t, q)$
			\STATE $ \forall a_t \in \mathcal{A}(s_t) = \{select, skip\}$ \textrm{compute} $\hat{Q}(s_t, a_t)$
			\STATE Select $a^{*}_t =$ argmax$_{a_{t}}\hat{Q}(s_t, a_t)$
			\IF{$random() < \epsilon$}
				\STATE  Set $a^{*}_t $ to random action with $\Pr(a_t) =\frac{1}{| \mathcal{A} |} $
			\ENDIF
			\IF{$a^{*}_t = \{select\}$}
				\STATE Update $\tilde{y}_{t+1} = \tilde{y}_t \cup  \{ x_t \} $
			\ENDIF	
			\STATE Execute action $a^{*}_t$ and observe reward $r_t$ and new state $s_{t+1}$
			\STATE Update $\Gamma_q = \Gamma_q \cup \{ [s_t, a^{*}_t, r_t, s_{t+1}]\}$
		\ENDFOR
	\ENDFOR
		\FOR{ $j=1,...,J$ transitions sampled from $\Gamma$}
		\STATE \[\textrm{Set } y_j= 
					\begin{cases}
						r_j              								& \text{if $s_{j+1}$ is terminal } \\
					     	r_j + \gamma $max$_{a'}\hat{Q}(s_{j+1}, a'; \theta) 	& \text{if $s_{j+1}$ is non-terminal } 
					\end{cases} 
					\]
        		\STATE Perform gradient step on $\mathcal{L}(\theta) = (y_j - \hat{Q}(s_j, a_j; \theta))^2$
		\ENDFOR
\ENDFOR
  \end{algorithmic}
    \caption{DQN-LSTM for Event Summarization Training Procedure}
\end{algorithm}


\begin{tabular}{ l | c | c  }
	\hline
	Metric  & Model  & Performance\\ \hline \hline
  	ROUGUE-1F1 & DQN-LSTM  & X \\
  	ROUGUE-1F1 & DQN-BOW  & X \\
  	ROUGUE-1F1 & DQN-BiBOW  & X \\
  	ROUGUE-1F1 & Random  & X \\ \hline
  	ROUGUE-2F1 & DQN-LSTM  & X \\
  	ROUGUE-2F1 & DQN-BOW  & X \\
  	ROUGUE-2F1 & DQN-BiBOW  & X \\
  	ROUGUE-2F1 & Random  & X \\ \hline
	\hline
\end{tabular}



\newpage

\bibliography{DeepNLPQLearning}

\end{document}
