\documentclass[12pt]{article}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{ bm }
\usepackage{amsmath }
\usepackage{hyperref}

\usepackage{algorithm,algorithmic,amsmath}
%\usepackage[linesnumbered,ruled]{algorithm2e}


\title{DQN-LSTM for Event Summarization}

\author{
	Chris Kedzie \\ \href{mailto: kedzie@cs.columbia.edu}{\small kedzie@cs.columbia.edu} 
		\and  
	Francisco Javier Arceo \\ \href{mailto: fja2114@columbia.edu}{\small fja2114@columbia.edu} 
	}
	
\begin{document}
\maketitle

\bibliographystyle{plain}

%\begin{abstract}
%\end{abstract}

\section{Notes/Questions}
\begin{itemize}
\item Word-level RNN
\item for first part we will ignore time
\item Okay, so we have 3 separate LSTMs
	\begin{enumerate}
		\item Query level  (we have 44 queries)
		\item Input Stream/Sentence (each query has ~200k)
		\item Current Summary (initially empty)
	\end{enumerate}
\item From there it gets interesting...because they're different sizes
\item For all of the DQN games we have a terminal iteration and observed outcome, what about for us?
	\begin{enumerate}
		\item We will process all queries
	\end{enumerate}
\item We process all of the embeddings
\end{itemize}

\subsection{Thoughts}

Here's a poorly worded brain dump of how I think this should go
\begin{itemize}
\item Randomly initialize the Query and Stream word vectors
\item Initialize the action for each stream (p=0.5 to select or skip)
%\item Initialize the updated summary with 0s
%	\begin{enumerate}
%		\item Actually, we can probably just pull from the initial random choices
%	\end{enumerate}
% \item Set estimated current summary to empty
\item Sample $T$ streams from data (in mini-batches) by randomly choosing to select or skip streams (for a single query)
	\begin{enumerate}
		\item Probably makes sense to sample from streams by weight exponentially as a function of time (older streams will be penalized directly)
		\item Use epsilon-greedy strategy that decays to a small percent for random sampling  actions
		\item If improvement in $ROUGE>0$ then 1, if not 0
		\item Sum $ROUGE$ for all sentences, make that label
	\end{enumerate}
\item Employ pooling Feedforward ReLU network to learn $E[Total ROUGE]$
\item Now given current summary, evaluate summary on current metrics
\item Back prop
\end{itemize}
\section{Extractive Streaming Summarization}

  In the extractive streaming summarization task, we are given as input
  a query, e.g. a short text description of a topic or an event, and 
  document stream, a time ordered set of sentences 
  relevant to the query. Starting with an initially empty summary,  
  an extractive, streaming summarization algorithm is intended to 
  examine each sentence in order and when new and important (relative to the 
  query) information is identified, add that sentence to the summary. 

  Implicit to this problem is the notion of system time -- the 
  summarization algorithm can only examine sentences that occur in the stream
  before the current system time. Advancing the system time gives the algorithm
  access to more sentences, although in practice the stream is sufficiently 
  large enough that choices have to be made about how much history can be 
  kept in memory. For many domains, e.g. crisis informatics, it is preferable
  for a summarization algorithm to identify important information as early as 
  possible, and so the objective function should penalize a large disparity
  between the time a piece of information is first available to the algorithm
  and the system time at which that information is actually added to the 
  summary.

  Previous work in this area has either incremented the system time in fixed
  increments (e.g. an hour) 
  \cite{mccreadie2014incremental,kedzie2015predicting} or operated in
  a fully online setting \cite{guo2013updating,kedzie2016real}. In both cases 
  explicitly 
  modeling the current state of the summary, the stream, and their relationship
  with the query is quite complicated and exhibits non-linear dynamics that 
  are difficult characterize in traditional feature based models.

  Additionaly, the structured nature of the sentence selection task 
  (sentence selection is
  highly dependent on the current summary state) suggests that imitation 
  or reinforcement learning are necessary to obtain parity between training
  and testing feature distributions. 

  This leads us to explore deep Q networks (DQN) for two reasons. 
  First, both the 
  representation and mode of interaction between the stream, summary, and 
  query can be learned. 
  Second, the learned Q function (plus random noise) controls the state 
  space that is explored, ensuring more consistancy between train and test 
  distribtuions than for example naive imitation learning 
  (possibly -- I'm not totally happy with this sentence).

\subsection{Problem Definition - FJA}
We specify our DQN architecture by estimating the Q function, $Q^*$, with 3 separate RNN-LSTMs representing the query, $t^{th}$ stream, and current predicted summary, which is then fed into a feedforward Multilayer Perceptron. After evaluation of our current state in our DQN, the model is backpropagated in the standard way, we refer to our architecture as a DQN-LSTM. Similar to \cite{narasimhan2015language} we use LSTM-DQN.

\subsection{Algorithm}

% Notation here is not very clear, need to clean it up
\begin{algorithm}
    \bf{Input:} { \rm $\{ x_q, \pi^{*}_q \}_{\forall q \in \mathcal{Q} }$, number of iterations $N$, mixture parameter $\epsilon$  } \\
    \underline{\bf{Output:}  $\hat{\pi}$ }
  \begin{algorithmic}[1]
  \STATE The notation is not very clear -- WORK IN PROGRESS
  \STATE \rm Initialize experience memory $\Gamma$
  \STATE \rm Initialize action-value function $\mathcal{Q}$ with random weights
    \STATE \rm $i \leftarrow \emptyset$ 
    \FOR{$n \in \{0,1,.., N-1\} $}
      \FOR{$q \in \mathcal{Q}$}
      	\FOR{$t \in \{0, 1, ..., T-1\}$ }
        \STATE \rm Execute $\pi_i$ $t$ times and reach $s_t$
        \FOR{ $a \in \mathcal{A}(s_t) $}
        		\STATE \rm With probability $\epsilon$ set $\pi^0 = \pi^{*}_q$, else $\pi_i$
		\STATE Compute $c_t(a)$ by executing $\pi^0$
		\STATE $\Gamma \leftarrow \Gamma \cup \{ [\phi(s_t), a, c_t(a)]\}$
		\ENDFOR
        \ENDFOR
        \STATE Sample random minibatches of transitions $\gamma_j$ from $\Gamma$
        \STATE Set $y_j = r_j$ for terminal $\phi_{j+1}$
        \STATE Perform gradient step on $(y_j - Q(\phi_j, a_j; \theta))^2$
        \STATE $i \leftarrow i + 1$
      \ENDFOR
    \ENDFOR
  \end{algorithmic}
    \caption{Streaming DQN-LSTM}
\end{algorithm}


% Notation here is not very clear, need to clean it up
\begin{algorithm}
    \bf{Input:} { \rm $\{ x_q, \pi^{*}_q \}_{\forall q \in \mathcal{Q} }$, number of iterations $N$, mixture parameter $\epsilon$  } \\
    \underline{\bf{Output:}  $\hat{\pi}$ }
  \begin{algorithmic}[1]
  \STATE The notation is not very clear -- WORK IN PROGRESS
  \STATE \rm Initialize $\pi_0$, $\Gamma$
  \STATE \rm Initialize action-value function $\mathcal{Q}$ with random weights
    \STATE \rm $i \leftarrow \emptyset$ 
    \FOR{$n \in \{0,1,.., N-1\} $}
      \FOR{$q \in \mathcal{Q}$}
      	\FOR{$t \in \{0, 1, ..., T-1\}$ }
        \STATE \rm Execute $\pi_i$ $t$ times and reach $s_t$
        \FOR{ $a \in \mathcal{A}(s_t) $}
        		\STATE \rm With probability $\epsilon$ set $\pi^0 = \pi^{*}_q$, else $\pi_i$
		\STATE Compute $c_t(a)$ by executing $\pi^0$
		\STATE $\Gamma \leftarrow \Gamma \cup \{ [\phi(s_t), a, c_t(a)]\}$
		\ENDFOR
        \ENDFOR
        \STATE Sample random minibatches of transitions $\gamma_j$ from $\Gamma$
        \STATE Set $y_j = r_j$ for terminal $\phi_{j+1}$
        \STATE Perform gradient step on $(y_j - Q(\phi_j, a_j; \theta))^2$
        \STATE $i \leftarrow i + 1$
      \ENDFOR
    \ENDFOR
  \end{algorithmic}
    \caption{Streaming DQN-LSTM}
\end{algorithm}


\subsection{Problem Definition}
   
    DQN learns by using an a $\epsilon$-greedy search policy to generate a 
    sequence of state, action, next state, reward 4-tuples using the current
    learned Q-network to evaluate candidate actions. These tuples are sampled
    from and used to estimate the true Q function.

    \textbf{States} 
    In our setup a state $s(X_{\le t},\tilde{Y}_{\le t}, q)$ is a function
    of the stream $X$ observed up to the current system time $t$, the state
    of the current summary $\tilde{Y}$ at system time $t$, and the query $q$.
    For brevity we will use $s(t,q)$ where the dependence on 
    $X_{\le t},\tilde{Y}_{\le t}$ is assumed.
    $s(t,q)$ is itself three recurrent neural networks, one for encoding 
    the summary, the stream, and the query respectively.

    \textbf{Actions} 
    The set of possible actions at each time step is 
    $\mathcal{A} = \{select, skip\}$ where $select$ corresponds to adding the 
    current sentence $x_t$ to the summary and incrementing the current system
    time, 
    \begin{align*}
        \tilde{Y}_{\le t + 1} &= \tilde{Y}_{\le t} \cup \{ x_t \} \\
        t &= t + 1    
    \end{align*}
    or $skip$ where only $t$ is incremented without changing the current 
    summary
    \begin{align*}
        \tilde{Y}_{\le t + 1} &= \tilde{Y}_{\le t}  \\
        t &= t + 1.
    \end{align*}


    \textbf{Reward} 
    The reward for a given action will be measured by relative gain in 
    ROUGE-2 F1 score of the predicted summary $\tilde{Y}_{\le t}$ measured
    against a gold standard summary $Y$.


     
    When only one gold summary reference is used, ROUGE-N Recall is calculated
    as 

    \[ \textrm{ROUGE-NR}(\tilde{Y}, Y) = 
        \frac{\sum_{g \in \textrm{ngrams}(Y,N)} 
        \min \left(\textrm{count}(g, \tilde{Y}), \textrm{count}(g, Y)\right)}{
        \sum_{g \in \textrm{ngrams}(Y,N)} 
        \textrm{count}(g, Y)
        }
    \]

    where $\textrm{ngrams}(Y, N)$ returns the set of ngrams of order $N$ in 
    the summary $Y$ and $\textrm{count}(g, Y)$ is the count of occurences of
    ngram $g$ in $Y.$

    Similarly, ROUGE-N Precision is calculated as 
    \[ \textrm{ROUGE-NP}(\tilde{Y}, Y) = 
        \frac{\sum_{g \in \textrm{ngrams}(Y,N)} 
        \min \left(\textrm{count}(g, \tilde{Y}), \textrm{count}(g, Y)\right)}{
            \sum_{g \in \textrm{ngrams}(\tilde{Y},N)} 
            \textrm{count}(g, \tilde{Y})
        }
    \]

    and the $F_1$ is simply the harmonic mean of the two:
    \[ \textrm{ROUGE-NF1}(\tilde{Y}, Y) = \frac{ 2 \times 
    \textrm{ROUGE-NP}(\tilde{Y}, Y) \times \textrm{ROUGE-NR}(\tilde{Y}, Y)
    }{ \textrm{ROUGE-NP}(\tilde{Y}, Y) + \textrm{ROUGE-NR}(\tilde{Y}, Y) } \]
              %\textrm{ROUGE-NP}(\tilde{Y}, Y) \times 
              %\textrm{ROUGE-NR}(\tilde{Y}, Y)
             %   }{
             % \textrm{ROUGE-NP}(\tilde{Y}, Y) +
             % \textrm{ROUGE-NR}(\tilde{Y}, Y)
        
       % }


    The reward $r$ at time $t$ is then:
    \[ r_t = \textrm{ROUGE-NF1}(\tilde{Y}_{\le t+1}, Y) - 
    \textrm{ROUGE-NF1}(\tilde{Y}_{\le t}, Y) \]

    TODO: Think about how to incorporate a time penalty into the reward.

\section{Useful Links}

Just adding some links to useful things 

\url{http://trec.nist.gov/}

\url{http://icml.cc/2016/tutorials/deep_rl_tutorial.pdf}

\section{To-Do's}
\center
\begin{tabular}{ l | c | c  }
	\hline
	Task  & Status  & Person\\ \hline
  	Read DQN Paper \cite{MnihKSGAWR13}  & Complete  & Francisco \\
	Read Asynchronous RL Paper \cite{DBLP:journals/corr/MnihBMGLHSK16} & Briefed  & Francisco \\
	Read  Regularization Paper \cite{chen2012marginalized} & Complete  & Francisco \\
	Read  Disaster Summarization Paper \cite{kedzie2015predicting} & Complete  & Francisco \\
	Read  Sequential Decision Making Paper \cite{kedzie2016real} & Complete  & Francisco \\
	Install Torch & Complete  & Francisco \\
	Small simulation of MLP in Torch & Complete  & Francisco \\
	Small simulation of DQN in Torch & Incomplete  & Francisco \\
	Download data & Incomplete  & Francisco \\
	Explore data & In-Progress  & Francisco \\
	\hline
\end{tabular}

\bibliography{DeepNLPQLearning}

\end{document}
