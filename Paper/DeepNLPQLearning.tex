\documentclass[12pt]{article}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{ bm }
\usepackage{amsmath }
\usepackage{hyperref}

\usepackage{tikz}
\usetikzlibrary{shapes,arrows,shadows}
%\usepackage{amsmath,bm,times}
\newcommand{\mx}[1]{\mathbf{\bm{#1}}} % Matrix command
\newcommand{\vc}[1]{\mathbf{\bm{#1}}} % Vector command


\usepackage{algorithm,algorithmic,amsmath}
%\usepackage[linesnumbered,ruled]{algorithm2e}


\title{DQN-LSTM for Event Summarization }

\author{
	Chris Kedzie \\ \href{mailto: kedzie@cs.columbia.edu}{\small kedzie@cs.columbia.edu} 
		\and  
	Francisco Javier Arceo \\ \href{mailto: fja2114@columbia.edu}{\small fja2114@columbia.edu} 
	}
	
\begin{document}
\maketitle

\begin{abstract}
I will insert something here\footnote{ \url{https://github.com/franciscojavierarceo/DQN-Event-Summarization} }
\end{abstract}

\bibliographystyle{plain}

%\begin{abstract}
%\end{abstract}

\section{Introduction}

\cite{kedzieextractive}

\section{Extractive Streaming Summarization}

  In the extractive streaming summarization task, we are given as input
  a query, e.g. a short text description of a topic or an event, and 
  document stream, a time ordered set of sentences 
  relevant to the query. Starting with an initially empty summary,  
  an extractive, streaming summarization algorithm is intended to 
  examine each sentence in order and when new and important (relative to the 
  query) information is identified, add that sentence to the summary. 

  Implicit to this problem is the notion of system time -- the 
  summarization algorithm can only examine sentences that occur in the stream
  before the current system time. Advancing the system time gives the algorithm
  access to more sentences, although in practice the stream is sufficiently 
  large enough that choices have to be made about how much history can be 
  kept in memory. For many domains, e.g. crisis informatics, it is preferable
  for a summarization algorithm to identify important information as early as 
  possible, and so the objective function should penalize a large disparity
  between the time a piece of information is first available to the algorithm
  and the system time at which that information is actually added to the 
  summary.

  Previous work in this area has either incremented the system time in fixed
  increments (e.g. an hour) 
  \cite{mccreadie2014incremental,kedzie2015predicting} or operated in
  a fully online setting \cite{guo2013updating,kedzie2016real}. In both cases 
  explicitly 
  modeling the current state of the summary, the stream, and their relationship
  with the query is quite complicated and exhibits non-linear dynamics that 
  are difficult characterize in traditional feature based models.

  Additionaly, the structured nature of the sentence selection task 
  (sentence selection is
  highly dependent on the current summary state) suggests that imitation 
  or reinforcement learning are necessary to obtain parity between training
  and testing feature distributions. 

  This leads us to explore deep Q networks (DQN) for two reasons. 
  First, both the 
  representation and mode of interaction between the stream, summary, and 
  query can be learned. 
  Second, the learned Q function (plus random noise) controls the state 
  space that is explored, ensuring more consistancy between train and test 
  distribtuions than for example naive imitation learning 
  (possibly -- I'm not totally happy with this sentence).

\subsection{Algorithm}

% Notation here is not very clear, need to clean it up
\begin{algorithm}
    \bf{Input:} { \rm $\{ x_q, \pi^{*}_q \}_{\forall q \in \mathcal{Q} }$, number of iterations $N$, mixture parameter $\epsilon$  } \\
    \underline{\bf{Output:}  $\hat{\pi}$ }
  \begin{algorithmic}[1]
  \STATE The notation is not very clear -- WORK IN PROGRESS
  \STATE \rm Initialize experience memory $\Gamma$
  \STATE \rm Initialize action-value function $\mathcal{Q}$ with random weights
    \STATE \rm $i \leftarrow \emptyset$ 
    \FOR{$n \in \{0,1,.., N-1\} $}
      \FOR{$q \in \mathcal{Q}$}
      	\FOR{$t \in \{0, 1, ..., T-1\}$ }
        \STATE \rm Execute $\pi_i$ $t$ times and reach $s_t$
        \FOR{ $a \in \mathcal{A}(s_t) $}
        		\STATE \rm With probability $\epsilon$ set $\pi^0 = \pi^{*}_q$, else $\pi_i$
		\STATE Compute $c_t(a)$ by executing $\pi^0$
		\STATE $\Gamma \leftarrow \Gamma \cup \{ [\phi(s_t), a, c_t(a)]\}$
		\ENDFOR
        \ENDFOR
        \STATE Sample random minibatches of transitions $\gamma_j$ from $\Gamma$
        \STATE Set $y_j = r_j$ for terminal $\phi_{j+1}$
        \STATE Perform gradient step on $(y_j - Q(\phi_j, a_j; \theta))^2$
        \STATE $i \leftarrow i + 1$
      \ENDFOR
    \ENDFOR
  \end{algorithmic}
    \caption{Streaming DQN-LSTM}
\end{algorithm}


% Notation here is not very clear, need to clean it up
\begin{algorithm}
    \bf{Input:} { \rm $\{ x_q, \pi^{*}_q \}_{\forall q \in \mathcal{Q} }$, number of iterations $N$, mixture parameter $\epsilon$  } \\
    \underline{\bf{Output:}  $\hat{\pi}$ }
  \begin{algorithmic}[1]
  \STATE The notation is not very clear -- WORK IN PROGRESS
  \STATE \rm Initialize $\pi_0$, $\Gamma$
  \STATE \rm Initialize action-value function $\mathcal{Q}$ with random weights
    \STATE \rm $i \leftarrow \emptyset$ 
    \FOR{$n \in \{0,1,.., N-1\} $}
      \FOR{$q \in \mathcal{Q}$}
      	\FOR{$t \in \{0, 1, ..., T-1\}$ }
        \STATE \rm Execute $\pi_i$ $t$ times and reach $s_t$
        \FOR{ $a \in \mathcal{A}(s_t) $}
        		\STATE \rm With probability $\epsilon$ set $\pi^0 = \pi^{*}_q$, else $\pi_i$
		\STATE Compute $c_t(a)$ by executing $\pi^0$
		\STATE $\Gamma \leftarrow \Gamma \cup \{ [\phi(s_t), a, c_t(a)]\}$
		\ENDFOR
        \ENDFOR
        \STATE Sample random minibatches of transitions $\gamma_j$ from $\Gamma$
        \STATE Set $y_j = r_j$ for terminal $\phi_{j+1}$
        \STATE Perform gradient step on $(y_j - Q(\phi_j, a_j; \theta))^2$
        \STATE $i \leftarrow i + 1$
      \ENDFOR
    \ENDFOR
  \end{algorithmic}
    \caption{Streaming DQN-LSTM}
\end{algorithm}


\subsection{Problem Definition}
   
    DQN learns by using an a $\epsilon$-greedy search policy to generate a 
    sequence of state, action, next state, reward 4-tuples using the current
    learned Q-network to evaluate candidate actions. These tuples are sampled
    from and used to estimate the true Q function.

    \textbf{States} 
    In our setup a state $s(X_{\le t},\tilde{Y}_{\le t}, q)$ is a function
    of the stream $X$ observed up to the current system time $t$, the state
    of the current summary $\tilde{Y}$ at system time $t$, and the query $q$.
    For brevity we will use $s(t,q)$ where the dependence on 
    $X_{\le t},\tilde{Y}_{\le t}$ is assumed.
    $s(t,q)$ is itself three recurrent neural networks, one for encoding 
    the summary, the stream, and the query respectively.

    \textbf{Actions} 
    The set of possible actions at each time step is 
    $\mathcal{A} = \{select, skip\}$ where $select$ corresponds to adding the 
    current sentence $x_t$ to the summary and incrementing the current system
    time, 
    \begin{align*}
        \tilde{Y}_{\le t + 1} &= \tilde{Y}_{\le t} \cup \{ x_t \} \\
        t &= t + 1    
    \end{align*}
    or $skip$ where only $t$ is incremented without changing the current 
    summary
    \begin{align*}
        \tilde{Y}_{\le t + 1} &= \tilde{Y}_{\le t}  \\
        t &= t + 1.
    \end{align*}


    \textbf{Reward} 
    The reward for a given action will be measured by relative gain in 
    ROUGE-2 F1 score of the predicted summary $\tilde{Y}_{\le t}$ measured
    against a gold standard summary $Y$.


     
    When only one gold summary reference is used, ROUGE-N Recall is calculated
    as 

    \[ \textrm{ROUGE-NR}(\tilde{Y}, Y) = 
        \frac{\sum_{g \in \textrm{ngrams}(Y,N)} 
        \min \left(\textrm{count}(g, \tilde{Y}), \textrm{count}(g, Y)\right)}{
        \sum_{g \in \textrm{ngrams}(Y,N)} 
        \textrm{count}(g, Y)
        }
    \]

    where $\textrm{ngrams}(Y, N)$ returns the set of ngrams of order $N$ in 
    the summary $Y$ and $\textrm{count}(g, Y)$ is the count of occurences of
    ngram $g$ in $Y.$

    Similarly, ROUGE-N Precision is calculated as 
    \[ \textrm{ROUGE-NP}(\tilde{Y}, Y) = 
        \frac{\sum_{g \in \textrm{ngrams}(Y,N)} 
        \min \left(\textrm{count}(g, \tilde{Y}), \textrm{count}(g, Y)\right)}{
            \sum_{g \in \textrm{ngrams}(\tilde{Y},N)} 
            \textrm{count}(g, \tilde{Y})
        }
    \]

    and the $F_1$ is simply the harmonic mean of the two:
    \[ \textrm{ROUGE-NF1}(\tilde{Y}, Y) = \frac{ 2 \times 
    \textrm{ROUGE-NP}(\tilde{Y}, Y) \times \textrm{ROUGE-NR}(\tilde{Y}, Y)
    }{ \textrm{ROUGE-NP}(\tilde{Y}, Y) + \textrm{ROUGE-NR}(\tilde{Y}, Y) } \]
              %\textrm{ROUGE-NP}(\tilde{Y}, Y) \times 
              %\textrm{ROUGE-NR}(\tilde{Y}, Y)
             %   }{
             % \textrm{ROUGE-NP}(\tilde{Y}, Y) +
             % \textrm{ROUGE-NR}(\tilde{Y}, Y)
        
       % }


    The reward $r$ at time $t$ is then:
    \[ r_t = \textrm{ROUGE-NF1}(\tilde{Y}_{\le t+1}, Y) - 
    \textrm{ROUGE-NF1}(\tilde{Y}_{\le t}, Y) \]

    TODO: Think about how to incorporate a time penalty into the reward.

\center
\begin{tabular}{ l | c | c  }
	\hline
	Task  & Status  & Person\\ \hline
  	Read DQN Paper \cite{MnihKSGAWR13}  & Complete  & Francisco \\
	Read Asynchronous RL Paper \cite{DBLP:journals/corr/MnihBMGLHSK16} & Briefed  & Francisco \\
	Read  Regularization Paper \cite{chen2012marginalized} & Complete  & Francisco \\
	Read  Disaster Summarization Paper \cite{kedzie2015predicting} & Complete  & Francisco \\
	Read  Sequential Decision Making Paper \cite{kedzie2016real} & Complete  & Francisco \\
	Install Torch & Complete  & Francisco \\
	Small simulation of MLP in Torch & Complete  & Francisco \\
	Small simulation of DQN in Torch & Incomplete  & Francisco \\
	Download data & Incomplete  & Francisco \\
	Explore data & In-Progress  & Francisco \\
	\hline
\end{tabular}

    % Define the layers to draw the diagram
    \pgfdeclarelayer{background}
    \pgfdeclarelayer{foreground}
    \pgfsetlayers{background,main,foreground}

    \tikzstyle{sensor}=[draw, fill=blue!20, text width=5em,  text centered, minimum height=2.5em,drop shadow]
    \tikzstyle{term}=[draw, fill=gray!20, text width=5em,  text centered, minimum height=2.5em,drop shadow]
    \tikzstyle{wa} = [sensor, text width=10em, fill=red!20,  minimum height=3em, rounded corners, drop shadow]
    
    \def\blockdist{2.3}
    \def\edgedist{2.5}

    \begin{tikzpicture}
        \node (wa) [wa] {\textbf{Linear Layer} };
        \path (wa.west)+(-3.2 ,  2.0) node (asr1)[sensor] {$LSTM_1$};
        \path (wa.west)+(-3.2 ,  0.5) node (asr2)[sensor] {$LSTM_2$};
        \path (wa.west)+(-3.2 , -1.0) node (asr3)[sensor] {$LSTM_3$};
        \path (wa.west)+(-3.2 , -2.5) node (asr4)[sensor] {$MLP$};
        \path (wa.east)+(\blockdist,0) node (vote) [term] {\textbf{Output}};

        \path [draw, ->] (asr1.east) -- node [above] {}         (wa.170) ;
        \path [draw, ->] (asr2.east) -- node [above] {}         (wa.175);
        \path [draw, ->] (asr3.east) -- node [above] {}         (wa.185);
        \path [draw, ->] (asr4.east) -- node [above] {}         (wa.190);
        \path [draw, ->] (wa.east) -- node [above] {}       (vote.west);

     \begin{pgfonlayer}{background}
            \path (asr1.north  |- asr1.south)+(-2.5, 1.5) node (a) {};
            \path (asr4.west  |- vote.east)+(+13.0, -3.5) node (d) {};
            \path[fill=white!20,rounded corners, draw=black!50, dashed]         (a) rectangle (d);
        \end{pgfonlayer}
        \path (wa.east) + (-2.3, -3.0)  node (vote.east) {DQN-LSTM for Event Summarization};
    \end{tikzpicture}


\bibliography{DeepNLPQLearning}

\end{document}
