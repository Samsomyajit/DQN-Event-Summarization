\documentclass[12pt]{article}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{ bm }
\usepackage{amsmath }
\usepackage{hyperref}

\usepackage{tikz}
\usetikzlibrary{shapes,arrows,shadows}
%\usepackage{amsmath,bm,times}
\newcommand{\mx}[1]{\mathbf{\bm{#1}}} % Matrix command
\newcommand{\vc}[1]{\mathbf{\bm{#1}}} % Vector command


\usepackage{algorithm,algorithmic,amsmath}
%\usepackage[linesnumbered,ruled]{algorithm2e}


\title{DQN-LSTM for Event Summarization }

\author{
	Francisco Javier Arceo \\ \href{mailto: fja2114@columbia.edu}{\small fja2114@columbia.edu} 
		\and  
	Chris Kedzie \\ \href{mailto: kedzie@cs.columbia.edu}{\small kedzie@cs.columbia.edu} 
	}

\begin{document}
\maketitle

\begin{abstract}
I will insert something here\footnote{ \url{https://github.com/franciscojavierarceo/DQN-Event-Summarization} }
\end{abstract}

\bibliographystyle{plain}

%\begin{abstract}
%\end{abstract}

\section{Introduction}

Crisis informatics is becoming an increasingly popular area of study in machine learning with more recent research focusing on extractive summarization (e.g., \cite{kedzie2015predicting} and \cite{kedzieextractive}) of multi-document summarization. The approach by \cite{kedzie2015predicting}  has shown that it is possible to select relevant sentences from a massive number of documents on the web to create summaries with meaningful content by adapting classifiers to maximize search policies. These systems operate in a streaming fashion and are capable of evaluating each sentence within each article to decide whether or not to select or skip the sentence. 

Unfortunately, these systems have still been shown to fall short of heuristic algorithms [citation needed], which may be due to inadequate capturing of the rich structure and often idiosyncractic information by traditional n-gram models  in language modeling. \cite{bengio2003neural} have shown that natural language processing can be mapped to a higher order dimension to represent more powerful features for various language modeling tasks. These embeddings have proven incredibly powerful on a variety of different natural language processing tasks [citations needed].

In this paper we show that a deep recurrent neural network (DQN) with a long short term memory (LSTM) is able to successfully learn an extractive summary policy by encoding the action, state, and reward into our DQN-LSTM similar to that of  \cite{hausknecht2015deep}. We show that on a variety of different metrics, our specification is able to reach state-of-the-art performance.

\section{Related Work}

\section{Extractive Streaming Summarization}

  In the extractive streaming summarization task, we are given as input
  a query, e.g. a short text description of a topic or an event, and 
  document stream, a time ordered set of sentences 
  relevant to the query. Starting with an initially empty summary,  
  an extractive, streaming summarization algorithm is intended to 
  examine each sentence in order and when new and important (relative to the 
  query) information is identified, add that sentence to the summary. 

  Implicit to this problem is the notion of system time -- the 
  summarization algorithm can only examine sentences that occur in the stream
  before the current system time. Advancing the system time gives the algorithm
  access to more sentences, although in practice the stream is sufficiently 
  large enough that choices have to be made about how much history can be 
  kept in memory. For many domains, e.g. crisis informatics, it is preferable
  for a summarization algorithm to identify important information as early as 
  possible, and so the objective function should penalize a large disparity
  between the time a piece of information is first available to the algorithm
  and the system time at which that information is actually added to the 
  summary.

  Previous work in this area has either incremented the system time in fixed
  increments (e.g. an hour) 
  \cite{mccreadie2014incremental,kedzie2015predicting} or operated in
  a fully online setting \cite{guo2013updating,kedzie2016real}. In both cases 
  explicitly 
  modeling the current state of the summary, the stream, and their relationship
  with the query is quite complicated and exhibits non-linear dynamics that 
  are difficult characterize in traditional feature based models.

  Additionaly, the structured nature of the sentence selection task 
  (sentence selection is
  highly dependent on the current summary state) suggests that imitation 
  or reinforcement learning are necessary to obtain parity between training
  and testing feature distributions. 

  This leads us to explore deep Q networks (DQN) for two reasons. 
  First, both the 
  representation and mode of interaction between the stream, summary, and 
  query can be learned. 
  Second, the learned Q function (plus random noise) controls the state 
  space that is explored, ensuring more consistency between train and test 
  distributions than for example naive imitation learning 
  (possibly -- I'm not totally happy with this sentence).

\subsection{Problem Definition}
   
    DQN learns by using an a $\epsilon$-greedy search policy to generate a 
    sequence of state, action, next state, reward 4-tuples using the current
    learned Q-network to evaluate candidate actions. These tuples are sampled
    from and used to estimate the true Q function.

    \textbf{States} 
    In our setup a state $s(X_{\le t},\tilde{Y}_{\le t}, q)$ is a function
    of the stream $X$ observed up to the current system time $t$, the state
    of the current summary $\tilde{Y}$ at system time $t$, and the query $q$.
    For brevity we will use $s(t,q)$ where the dependence on 
    $X_{\le t},\tilde{Y}_{\le t}$ is assumed.
    $s(t,q)$ is itself three recurrent neural networks, one for encoding 
    the summary, the stream, and the query respectively.

    \textbf{Actions} 
    The set of possible actions at each time step is 
    $\mathcal{A} = \{select, skip\}$ where $select$ corresponds to adding the 
    current sentence $x_t$ to the summary and incrementing the current system
    time, 
    \begin{align*}
        \tilde{Y}_{\le t + 1} &= \tilde{Y}_{\le t} \cup \{ x_t \} \\
        t &= t + 1    
    \end{align*}
    or $skip$ where only $t$ is incremented without changing the current 
    summary
    \begin{align*}
        \tilde{Y}_{\le t + 1} &= \tilde{Y}_{\le t}  \\
        t &= t + 1.
    \end{align*}


    \textbf{Reward} 
    The reward for a given action will be measured by relative gain in 
    ROUGE-2 F1 score of the predicted summary $\tilde{Y}_{\le t}$ measured
    against a gold standard summary $Y$.


     
    When only one gold summary reference is used, ROUGE-N Recall is calculated
    as 

    \[ \textrm{ROUGE-NR}(\tilde{Y}, Y) = 
        \frac{\sum_{g \in \textrm{ngrams}(Y,N)} 
        \min \left(\textrm{count}(g, \tilde{Y}), \textrm{count}(g, Y)\right)}{
        \sum_{g \in \textrm{ngrams}(Y,N)} 
        \textrm{count}(g, Y)
        }
    \]

    where $\textrm{ngrams}(Y, N)$ returns the set of ngrams of order $N$ in 
    the summary $Y$ and $\textrm{count}(g, Y)$ is the count of occurences of
    ngram $g$ in $Y.$

    Similarly, ROUGE-N Precision is calculated as 
    \[ \textrm{ROUGE-NP}(\tilde{Y}, Y) = 
        \frac{\sum_{g \in \textrm{ngrams}(Y,N)} 
        \min \left(\textrm{count}(g, \tilde{Y}), \textrm{count}(g, Y)\right)}{
            \sum_{g \in \textrm{ngrams}(\tilde{Y},N)} 
            \textrm{count}(g, \tilde{Y})
        }
    \]

    and the $F_1$ is simply the harmonic mean of the two:
    \[ \textrm{ROUGE-NF1}(\tilde{Y}, Y) = \frac{ 2 \times 
    \textrm{ROUGE-NP}(\tilde{Y}, Y) \times \textrm{ROUGE-NR}(\tilde{Y}, Y)
    }{ \textrm{ROUGE-NP}(\tilde{Y}, Y) + \textrm{ROUGE-NR}(\tilde{Y}, Y) } \]
              %\textrm{ROUGE-NP}(\tilde{Y}, Y) \times 
              %\textrm{ROUGE-NR}(\tilde{Y}, Y)
             %   }{
             % \textrm{ROUGE-NP}(\tilde{Y}, Y) +
             % \textrm{ROUGE-NR}(\tilde{Y}, Y)
        
       % }


    The reward $r$ at time $t$ is then:
    \[ r_t = \textrm{ROUGE-NF1}(\tilde{Y}_{\le t+1}, Y) - 
    \textrm{ROUGE-NF1}(\tilde{Y}_{\le t}, Y) \]

    TODO: Think about how to incorporate a time penalty into the reward.

\subsection{Architecture}

The architecture for our DQN-LSTM is nearly identical to that of  \cite{narasimhan2015language} with modifications where appropriate. We learn the parameters using stochastic gradient descent with RMSprop \cite{hinton2012lecture}, where we minimize the following loss function: 

\begin{equation}
\mathcal{L}_i(\theta_i) =  \textrm{E}_{\hat{s}, \hat{a}}[ \big (y_i - Q(\hat{s}, \hat{a}; \theta_i) \big )^2].
\end{equation}

where 
\begin{equation}
y_i = \textrm{E}_{\hat{s}, \hat{a}}[ r + \gamma \textrm{max}_{a'} Q(s', a' ; \theta_{i-1}) | s', a' ]
\end{equation}

And the updates are computed using the following gradient update of $L_i(\theta_i)$:

\begin{equation}
\Delta L_i(\theta_i) = \textrm{E}_{\hat{s}, \hat{a}}[ 2  \big (y_i - Q(\hat{s}, \hat{a}; \theta_i) \big ) \Delta_{\theta_{i}}  Q(\hat{s}, \hat{a}; \theta_i)].
\end{equation}


\center
    % Define the layers to draw the diagram
\pgfdeclarelayer{background}
\pgfdeclarelayer{foreground}
\pgfsetlayers{background,main,foreground}
    \tikzstyle{sensor}=[draw, fill=blue!20, text width=5em,  text centered, minimum height=2.5em,drop shadow]
    \tikzstyle{term}=[draw, fill=gray!20, text width=5em,  text centered, minimum height=2.5em,drop shadow]
    \tikzstyle{wa} = [sensor, text width=10em, fill=red!20,  minimum height=3em, rounded corners, drop shadow]
    \def\blockdist{2.3}
    \def\edgedist{2.5}
    \begin{tikzpicture}
        \node (wa) [wa] {\textbf{Linear} };
        \path (wa.west)+(-3.2 ,  2.0) node (asr1)[sensor] {$LSTM$};
        \path (wa.west)+(-3.2 ,  0.0) node (asr2)[sensor] {$LSTM$};
        \path (wa.west)+(-3.2 , -2.0) node (asr3)[sensor] {$LSTM$};
        \path (wa.east)+(\blockdist,0) node (vote) [term] {\textbf{ReLU}};

        \path [draw, ->] (asr1.east) -- node [above] {}         (wa.175) ;
        \path [draw, ->] (asr2.east) -- node [above] {}         (wa.180);
        \path [draw, ->] (asr3.east) -- node [above] {}         (wa.185);
        \path [draw, ->] (wa.east) -- node [above] {}       (vote.west);

     \begin{pgfonlayer}{background}
            \path (asr1.north  |- asr1.south)+(-2.5, 1.5) node (a) {};
            \path (asr3.west  |- vote.east)+(+13.0, -3.5) node (d) {};
            \path[fill=white!20,rounded corners, draw=black!50, dashed]         (a) rectangle (d);
        \end{pgfonlayer}
        \path (wa.east) + (-2.3, -3.0)  node (vote.east) {DQN-LSTM for Event Summarization};
\end{tikzpicture}


\begin{algorithm}
    \bf{Input:} { \rm  \{$X_q$: Input sentences, $N$: Number of iterations, $\epsilon$: Mixture parameter\} } \\
    \underline{$\bf{Output: }$ \rm \{$\hat{\pi}$: policy\} }
\begin{algorithmic}[1]
\STATE \rm Initialize action-value function $Q$ with random weights
\STATE \rm Initialize $\tilde{Y}_q = \{\} $, $\forall q \in \mathcal{Q}$
\FOR{$epoch=1, N\ $}
	\FOR{$q \in \mathcal{Q}$}
		\STATE $X_q$= \{Extract sentences for query $q$\}
		\STATE $\tilde{Y}_q$ = \{Extract summary for query $q$\}
		\FOR{$\tilde{y}_t, x_t \in (X_q, \tilde{Y}_q) $}
			\STATE Set $s_t = s(x_t, \tilde{y}_t, q)$
			\STATE  \textrm{Compute} $Q(s_t)$,  $ \forall a_t \in \mathcal{A}(s_t) = \{select, skip\}$
			\STATE Select $a^{*}_t =$ argmax$_{a}Q(s_t)$
			\IF{$random() < \epsilon$}
				\STATE  Set $a^{*}_t $ to random action with $\Pr(a_t) =0.5$
			\ENDIF
			\IF{$a^{*}_t = \{select\}$}
				\STATE $\tilde{y}_{t+1} = \tilde{y}_t \cup  \{ x_t \} $
			\ENDIF	
			\STATE Execute action $a^{*}_t$ and observe reward $r_t$ and new state $s_{t+1}$
%			\STATE Update $\Gamma = \Gamma \cup \{ [s_t, a, r_t, s_{t+1}]\}$
		\ENDFOR
	\ENDFOR
		\STATE Sample random minibatches of transitions $\gamma_j$ from $\Gamma$
		\STATE 	\[ 		\textrm{Set } y_j= 
					\begin{cases}
						r_j              								& \text{if $s_{j+1}$ is terminal } \\
					     	r_j + \gamma $max$_{a'}Q(s_{j+1}, a'; \theta) 	& \text{if $s_{j+1}$ is non-terminal } 
					\end{cases} 
					\]
        		\STATE Perform gradient step on $\mathcal{L}(\theta) = (y_j - Q(s_j, a_j; \theta))^2$
\ENDFOR
  \end{algorithmic}
    \caption{DQN-LSTM for Event Summarization Training Procedure}
\end{algorithm}


\begin{tabular}{ l | c | c  }
	\hline
	Metric  & Model  & Performance\\ \hline \hline
  	ROUGUE-NF1 & DQN-LSTM  & X \\
  	ROUGUE-NF1 & DQN-BOW  & X \\
  	ROUGUE-NF1 & Random  & X \\ \hline
  	OTHER METRIC & DQN-LSTM  & X \\
  	OTHER METRIC & DQN-BOW  & X \\
  	OTHER METRIC & Random  & X \\ \hline
	\hline
\end{tabular}



\newpage

\bibliography{DeepNLPQLearning}

\end{document}
