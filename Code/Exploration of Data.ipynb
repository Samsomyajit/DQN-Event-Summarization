{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps to build pipeline\n",
    "\n",
    "1. Tokenize the query\n",
    "2. Tokenize the streams\n",
    "3. Tokenize the stream\n",
    "\n",
    "Run LSTM on query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.chdir('/Users/franciscojavierarceo/GitHub/DeepNLPQLearning/DO_NOT_UPLOAD_THIS_DATA/corpus-data/')\n",
    "os.listdir('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('./2012_aurora_shooting.tsv.gz', sep='\\t')\n",
    "df2 = pd.read_csv('./2012_pakistan_garment_factory_fires.tsv.gz', sep='\\t')\n",
    "df3 = pd.read_csv('./hurricane_sandy.tsv.gz', sep='\\t')\n",
    "df4 = pd.read_csv('./wisconsin_sikh_temple_shooting.tsv.gz', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Title is the title of the article\n",
    "print df1[df1['stream id']=='1342767285-e9b3e7dc6a9e31c8b2301848ab082117'].title[1]\n",
    "# Each text is a stream (i.e., a sentence from the article)\n",
    "print df1[df1['stream id']=='1342767285-e9b3e7dc6a9e31c8b2301848ab082117'].text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.listdir('/Users/franciscojavierarceo/GitHub/DeepNLPQLearning/DO_NOT_UPLOAD_THIS_DATA/trec-2013-data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.chdir('/Users/franciscojavierarceo/GitHub/DeepNLPQLearning/DO_NOT_UPLOAD_THIS_DATA/trec-2013-data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df5 = pd.read_csv('./nuggets.tsv.gz', sep='\\t')\n",
    "df6 = pd.read_csv('./matches.tsv.gz', sep='\\t')\n",
    "df7 = pd.read_csv('./updates.tsv.gz', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df5.head() # Nuggets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df6.head() # Matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df7.head() # Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df7['query_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('./trec2013-ts-topics-test.xml', 'rb')\n",
    "\n",
    "out = f.readlines()\n",
    "ox = BeautifulSoup(''.join(out),'lxml').contents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i.findAll('title')[0].text, ',',i.findAll('query')[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Wiki article, Query\"\n",
    "qs = []\n",
    "c = 0\n",
    "for i in ox.findAll('event'):\n",
    "    qs.append(i.findAll('query')[0].text)\n",
    "    print c, i.findAll('title')[0].text, ',',i.findAll('query')[0].text\n",
    "    c+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only have ten events, so we should build the code to be more general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the stream data for Aurora -- all the same query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This is the aurora shooting, query #2 in the xml\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## these are the nuggets for all of the trek data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df5[df5['nugget_id']=='VMTS13.01.077'] # Nuggets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index in df6 (matches) is concatenation of (queryid, streamid, and sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df6.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## These are the matches of the nuggets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df7.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This section tokenizes each dataset into a list of numeric indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import csv\n",
    "reload(sys)\n",
    "import pandas as pd\n",
    "from gensim import corpora\n",
    "from collections import defaultdict\n",
    "\n",
    "os.chdir('/Users/franciscojavierarceo/GitHub/DeepNLPQLearning/DO_NOT_UPLOAD_THIS_DATA/corpus-data/')\n",
    "\n",
    "def BuildIndexFiles(infile_list, outfile_list):\n",
    "    \"\"\"\n",
    "    :type  infilename: str\n",
    "    :param infilename: Fulll name of input file \n",
    "\n",
    "    :type  outfilename: str\n",
    "    :param outfilename: Export filename, without the '.csv'\n",
    "    \"\"\"\n",
    "    sys.setdefaultencoding('utf-8')\n",
    "    all_tokens = []\n",
    "    for idx, (infilename, outfilename) in enumerate(zip(infile_list, outfile_list)):\n",
    "        print('Loading %s %i of %i' % (infilename, idx, len(infile_list)) )\n",
    "\n",
    "        df = pd.read_csv(infilename, sep='\\t', encoding='latin-1')\n",
    "        texts = [ t.split(\" \") for t in df['text'] ]\n",
    "\n",
    "        frequency = defaultdict(int)\n",
    "        for text in texts:\n",
    "            for token in text:\n",
    "                frequency[token] += 1\n",
    "        texts = [ [token for token in text ]  for text in texts]\n",
    "        # Collecting all the list of tokens\n",
    "        all_tokens.append(texts)\n",
    "\n",
    "    # Getting the dictionary with token info\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    # Mapping to numeric list\n",
    "    text_numindex = [ [word2idx[i] for i in t] for t in texts]\n",
    "\n",
    "\n",
    "    # Exporting the dictionaries\n",
    "    print(\"Exporting word to index and dictionary to word indices\")\n",
    "    output = open('./0-output/LSTMDQN_Dic_token2id.pkl', 'ab+')\n",
    "    pickle.dump(dictionary.token2id, output)\n",
    "    output.close()\n",
    "\n",
    "    output = open('./0-output/LSTMDQN_Dic_id2token.pkl', 'ab+')\n",
    "    pickle.dump(dictionary.id2token, output)\n",
    "    output.close()\n",
    "\n",
    "    odf = pd.DataFrame()\n",
    "    odf.to_csv('./0-output/LSTMDQN_TermFrequency.csv')\n",
    "\n",
    "    for idx, (infilename, outfilename) in enumerate(zip(infile_list, outfile_list)):\n",
    "        df = pd.read_csv(infilename, sep='\\t', encoding='latin-1')\n",
    "        texts = [ t.split(\" \") for t in df['text'] ]\n",
    "\n",
    "        frequency = defaultdict(int)\n",
    "        for text in texts:\n",
    "            for token in text:\n",
    "                frequency[token] += 1\n",
    "        texts = [ [token for token in text ]  for text in texts]\n",
    "        # Exporting files\n",
    "        print('...file export to %s' % outfilename)\n",
    "        with open(outfilename+'_numtext.csv', 'wb') as csvfile:\n",
    "            data = csv.writer(csvfile, delimiter=' ', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "            data.writerow(['Text'])\n",
    "            data.writerows(text_numindex)\n",
    "\n",
    "        csvfile.close()\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "infilelist = [\n",
    "            './2012_aurora_shooting.tsv.gz', \n",
    "            './2012_pakistan_garment_factory_fires.tsv.gz',\n",
    "            './hurricane_sandy.tsv.gz',\n",
    "            './wisconsin_sikh_temple_shooting.tsv.gz'\n",
    "]\n",
    "outfilelist = [\n",
    "            './0-output/2012_aurora_shooting',\n",
    "            './0-output/2012_pakistan_garment_factory_fires',\n",
    "            './0-output/hurricane_sandy',\n",
    "            './0-output/wisconsin_sikh_temple_shooting'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mycorpora = BuildIndexFiles(infilelist, outfilelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merging the dictionaries toa pandas data frame with summary info\n",
    "odf0 = pd.DataFrame.from_dict(mycorpora.dfs, orient='index').reset_index()\n",
    "odf1 = pd.DataFrame.from_dict(mycorpora.id2token, orient='index').reset_index()\n",
    "odf0.columns = ['id', 'frequency']\n",
    "odf1.columns = ['id', 'token']\n",
    "odf = pd.merge(left=odf0, right=odf1, on='id')\n",
    "odf = odf[['id','token', 'frequency']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Saving both\n",
    "mycorpora.save('./0-output/total_corpus.mm')\n",
    "odf.to_csv('./0-output/total_corpus_smry.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>token</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td>7444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>rays</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>'s</td>\n",
       "      <td>38876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>supply</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>electricity</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id        token  frequency\n",
       "0   0            A       7444\n",
       "1   1         rays         13\n",
       "2   2           's      38876\n",
       "3   3       supply        149\n",
       "4   4  electricity         60"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odf.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
