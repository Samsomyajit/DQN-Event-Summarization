{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps to build pipeline\n",
    "\n",
    "1. Tokenize the query\n",
    "2. Tokenize the streams\n",
    "3. Tokenize the stream\n",
    "\n",
    "Run LSTM on query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.chdir('/Users/franciscojavierarceo/GitHub/DeepNLPQLearning/DO_NOT_UPLOAD_THIS_DATA/corpus-data/')\n",
    "os.listdir('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('./2012_aurora_shooting.tsv.gz', sep='\\t')\n",
    "df2 = pd.read_csv('./2012_pakistan_garment_factory_fires.tsv.gz', sep='\\t')\n",
    "df3 = pd.read_csv('./hurricane_sandy.tsv.gz', sep='\\t')\n",
    "df4 = pd.read_csv('./wisconsin_sikh_temple_shooting.tsv.gz', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Title is the title of the article\n",
    "print df1[df1['stream id']=='1342767285-e9b3e7dc6a9e31c8b2301848ab082117'].title[1]\n",
    "# Each text is a stream (i.e., a sentence from the article)\n",
    "print df1[df1['stream id']=='1342767285-e9b3e7dc6a9e31c8b2301848ab082117'].text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.listdir('/Users/franciscojavierarceo/GitHub/DeepNLPQLearning/DO_NOT_UPLOAD_THIS_DATA/trec-2013-data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.chdir('/Users/franciscojavierarceo/GitHub/DeepNLPQLearning/DO_NOT_UPLOAD_THIS_DATA/trec-2013-data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df5 = pd.read_csv('./nuggets.tsv.gz', sep='\\t')\n",
    "df6 = pd.read_csv('./matches.tsv.gz', sep='\\t')\n",
    "df7 = pd.read_csv('./updates.tsv.gz', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df5.head() # Nuggets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df6.head() # Matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df7.head() # Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df7['query_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('./trec2013-ts-topics-test.xml', 'rb')\n",
    "\n",
    "out = f.readlines()\n",
    "ox = BeautifulSoup(''.join(out),'lxml').contents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i.findAll('title')[0].text, ',',i.findAll('query')[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Wiki article, Query\"\n",
    "qs = []\n",
    "c = 0\n",
    "for i in ox.findAll('event'):\n",
    "    qs.append(i.findAll('query')[0].text)\n",
    "    print c, i.findAll('title')[0].text, ',',i.findAll('query')[0].text\n",
    "    c+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only have ten events, so we should build the code to be more general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the stream data for Aurora -- all the same query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This is the aurora shooting, query #2 in the xml\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## these are the nuggets for all of the trek data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df5[df5['nugget_id']=='VMTS13.01.077'] # Nuggets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index in df6 (matches) is concatenation of (queryid, streamid, and sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df6.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## These are the matches of the nuggets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df7.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This section tokenizes each dataset into a list of numeric indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import pickle\n",
    "import csv\n",
    "import pandas as pd\n",
    "from gensim import corpora\n",
    "from collections import defaultdict    \n",
    "\n",
    "def BuildIndexFiles(infile_list):\n",
    "    \"\"\"\n",
    "    :type  infilename: str\n",
    "    :param infilename: Fulll name of input file \n",
    "\n",
    "    :type  outfilename: str\n",
    "    :param outfilename: Export filename, without the '.csv'\n",
    "    \"\"\"\n",
    "    reload(sys)\n",
    "    sys.setdefaultencoding('utf-8')\n",
    "    all_tokens = []\n",
    "    frequency = defaultdict(int)\n",
    "    for idx, infilename in enumerate(infile_list):\n",
    "        print('Loading %s %i of %i' % (infilename, idx, len(infile_list)) )\n",
    "\n",
    "        df = pd.read_csv(infilename, sep='\\t', encoding='latin-1')\n",
    "        df['text'] = df['text'].str.replace('[^A-Za-z0-9]+', ' ').str.strip()\n",
    "        texts = [ t.split(\" \") for t in df['text'] ]\n",
    "\n",
    "        for text in texts:\n",
    "            for token in text:\n",
    "                frequency[token] += 1\n",
    "        texts = [ [token for token in text ]  for text in texts]\n",
    "        # Collecting all the list of tokens\n",
    "        all_tokens.append(texts)\n",
    "\n",
    "    texts = sum(all_tokens, [])\n",
    "    # Getting the dictionary with token info\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    # Mapping to numeric list\n",
    "    word2idx = dictionary.token2id\n",
    "    dictionary.id2token = {v:k for k,v in dictionary.token2id.items()}\n",
    "    idx2word = dictionary.id2token\n",
    "    \n",
    "    # Exporting the dictionaries\n",
    "    print(\"Exporting word to index and dictionary to word indices\")\n",
    "    output = open('./0-output/LSTMDQN_Dic_token2id.pkl', 'ab+')\n",
    "    pickle.dump(word2idx, output)\n",
    "    output.close()\n",
    "\n",
    "    output = open('./0-output/LSTMDQN_Dic_id2token.pkl', 'ab+')\n",
    "    pickle.dump(idx2word, output)\n",
    "    output.close()\n",
    "    \n",
    "    # Merging the dictionaries toa pandas data frame with summary info\n",
    "    odf0 = pd.DataFrame.from_dict(dictionary.dfs, orient='index').reset_index()\n",
    "    odf1 = pd.DataFrame.from_dict(word2idx, orient='index').reset_index()\n",
    "    odf0.columns = ['id', 'frequency']\n",
    "    odf1.columns = ['token', 'id']\n",
    "    odf = pd.merge(left=odf0, right=odf1, on='id')\n",
    "    odf = odf[['id','token', 'frequency']]\n",
    "    odf.to_csv('./0-output/total_corpus_smry.csv')\n",
    "    \n",
    "    return dictionary\n",
    "    \n",
    "def TokenizeData(infile_list, outfile_list, word2idx):\n",
    "    for idx, (infilename, outfilename) in enumerate(zip(infile_list, outfile_list)):\n",
    "        df = pd.read_csv(infilename, sep='\\t', encoding='latin-1')\n",
    "        df['text'] = df['text'].str.replace('[^A-Za-z0-9]+', ' ').str.strip()\n",
    "        texts = [ t.split(\" \") for t in df['text'] ]\n",
    "\n",
    "        frequency = defaultdict(int)\n",
    "        for text in texts:\n",
    "            for token in text:\n",
    "                frequency[token] += 1\n",
    "        texts = [ [token for token in text ]  for text in texts]\n",
    "        \n",
    "        text_numindex = [ [word2idx[i] for i in t] for t in texts]\n",
    "        # Exporting files\n",
    "        print('...file export to %s' % outfilename)\n",
    "        with open(outfilename+'_numtext.csv', 'wb') as csvfile:\n",
    "            data = csv.writer(csvfile, delimiter=' ', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "            data.writerow(['Text'])\n",
    "            data.writerows(text_numindex)\n",
    "\n",
    "        csvfile.close()\n",
    "    print('...Exporting of tokenized data complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir('/Users/franciscojavierarceo/GitHub/DeepNLPQLearning/DO_NOT_UPLOAD_THIS_DATA/corpus-data/')\n",
    "\n",
    "infilelist = [\n",
    "            './2012_aurora_shooting.tsv.gz', \n",
    "            './2012_pakistan_garment_factory_fires.tsv.gz',\n",
    "            './hurricane_sandy.tsv.gz',\n",
    "            './wisconsin_sikh_temple_shooting.tsv.gz'\n",
    "]\n",
    "outfilelist = [\n",
    "            './0-output/2012_aurora_shooting',\n",
    "            './0-output/2012_pakistan_garment_factory_fires',\n",
    "            './0-output/hurricane_sandy',\n",
    "            './0-output/wisconsin_sikh_temple_shooting'\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mycorpora = BuildIndexFiles(infilelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TokenizeData(infilelist, outfilelist, word2idx=mycorpora.token2id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
