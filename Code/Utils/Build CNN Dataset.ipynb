{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Each file contains four parts separated by ‘\\n\\n’. They are\n",
    "    1. url of the original article;\n",
    "    2. sentences in the article and their labels (for sentence-based extractive summarization);\n",
    "    3. extractable highlights (for word extraction-based abstractive summarization);\n",
    "    4. named entity mapping.\n",
    "\n",
    "### Sentence labels. There are three labels for the sentences: 1, 2 and 0. \n",
    "\n",
    "    - 1: sentence should extracted; \n",
    "    - 2: sentence might be extracted; \n",
    "    - 0: sentence shouldn't be extracted.\n",
    "\n",
    "### Extractable highlights\n",
    "\n",
    "The extractable highlights are created by examining if a word (or its morphological transformation) in the highlight appears in the article or a general purpose stop-word list, which together constitute the output space (i.e., the allowed vocabulary during summary generation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib2\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildEntityDictionary(input_path, filenames):\n",
    "    # Swapping in the entity names\n",
    "    entitykey, entityname = [], []\n",
    "    for filename in filenames:\n",
    "        f = open(os.path.join(input_path, filename))\n",
    "        data = f.read()\n",
    "        entities = data.split(\"\\n\\n\")[3]\n",
    "\n",
    "        for entity in entities.split(\"\\n\"):\n",
    "            entitykey.append( entity.split(\":\")[0] )\n",
    "            entityname.append( entity.split(\":\")[1] )\n",
    "\n",
    "    edictionary = dict(zip(entitykey, entityname))\n",
    "    return edictionary    \n",
    "\n",
    "def cleandata(input_path, files, edict):\n",
    "    f = open(os.path.join(input_path, files))\n",
    "    data = f.read()\n",
    "\n",
    "    url  = data.split(\"\\n\\n\")[0]\n",
    "    article = data.split(\"\\n\\n\")[1]\n",
    "    nuggets = data.split(\"\\n\\n\")[2]\n",
    "    entities = data.split(\"\\n\\n\")[3]\n",
    "\n",
    "    # Parsing the sentences and substituting\n",
    "    sentencelist, sentencelabel = [], []\n",
    "    for sentence in article.split(\"\\n\"):\n",
    "        # Swapping in the entity names\n",
    "        sentencelabel.append(int(sentence.split(\"\\t\\t\\t\")[1]))\n",
    "        sentence = sentence.split(\"\\t\\t\\t\")[0]\n",
    "        newsentence = ' '.join([edict[word] if word in edict else word for word in sentence.split(\" \")])\n",
    "        sentencelist.append(newsentence)\n",
    "\n",
    "    # Collecting the sentences in a list\n",
    "    df = pd.DataFrame(sentencelist, columns=['Sentence'])\n",
    "    df['Label'] = sentencelabel\n",
    "\n",
    "    # Extracting the nuggets\n",
    "    highlight = []\n",
    "    for nugget in nuggets.split(\"\\n\"):\n",
    "        newnugget = ' '.join([edict[word] if word in edict else word for word in nugget.split(\" \")])\n",
    "        highlight.append(newnugget)\n",
    "\n",
    "    nuggets = pd.DataFrame(highlight, columns=['Nugget'])\n",
    "    # Getting the title/query\n",
    "#     response = urllib2.urlopen(url)\n",
    "    html = requests.get(url).text\n",
    "#     html = response.read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    try:\n",
    "        title = soup.findAll(\"title\")[0].text\n",
    "    except:\n",
    "        title = 'MISSING'\n",
    "    return title, nuggets, df, df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputpath = '/Users/franciscojavierarceo/GitHub/DeepNLPQLearning/data/neuralsum/cnn/training/'\n",
    "outputpath = '/Users/franciscojavierarceo/GitHub/DeepNLPQLearning/data2/0-output'\n",
    "datafiles = os.listdir(inputpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "edict = buildEntityDictionary(inputpath, datafiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filelist = os.listdir(outputpath)\n",
    "finished = [int(x.replace(\"q\",'').replace(\"_stream.csv\", '')) for x in filelist if 'stream.csv' in x]\n",
    "finishedval = max(finished)\n",
    "# outdf = pd.DataFrame(columns=['query_id','query','streamSize','query_filename', 'outfile_name', 'nuggetfilename'])\n",
    "\n",
    "for i, datafile in enumerate(datafiles):\n",
    "    if i > finishedval:\n",
    "        query, nuggets, stream, streamSize = cleandata(inputpath, datafile, edict)\n",
    "        outfilename = 'q%i_stream.csv' % i\n",
    "        nuggetfilename = 'q%i_nuggets.csv' % i\n",
    "        tmpdf = pd.DataFrame( [i, query, streamSize, datafile, outfilename, nuggetfilename] ).T\n",
    "        tmpdf.columns = ['query_id','query','streamSize','query_filename', 'outfile_name', 'nuggetfilename']\n",
    "        outdf = pd.concat([outdf, tmpdf], axis=0)\n",
    "        stream.to_csv(os.path.join(outputpath, outfilename), index=False)\n",
    "        nuggets.to_csv(os.path.join(outputpath, nuggetfilename), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85198, 2541554)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outdf.shape[0], outdf['streamSize'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>query</th>\n",
       "      <th>streamSize</th>\n",
       "      <th>query_filename</th>\n",
       "      <th>outfile_name</th>\n",
       "      <th>nuggetfilename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Mistaken for your child's grandmother - CNN.com</td>\n",
       "      <td>61</td>\n",
       "      <td>000223f0c9a759b9cdd2f86ac8c2899747937263.summary</td>\n",
       "      <td>q0_stream.csv</td>\n",
       "      <td>q0_nuggets.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Arraignment for 8 current, former Bell, Califo...</td>\n",
       "      <td>36</td>\n",
       "      <td>00030b9744b6d8c21d3a9fcb35460b06d3a71e2e.summary</td>\n",
       "      <td>q1_stream.csv</td>\n",
       "      <td>q1_nuggets.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>U.S. and Russia can end the suffering - CNN.com</td>\n",
       "      <td>49</td>\n",
       "      <td>00036c4d44d9af8d34280086a887f3b7847cdae1.summary</td>\n",
       "      <td>q2_stream.csv</td>\n",
       "      <td>q2_nuggets.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>U.N. Security Council to hold emergency meetin...</td>\n",
       "      <td>31</td>\n",
       "      <td>0003e88107432daa7a852f9fc9f26915122f218b.summary</td>\n",
       "      <td>q3_stream.csv</td>\n",
       "      <td>q3_nuggets.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>Residents evacuated as firefighters battle bla...</td>\n",
       "      <td>30</td>\n",
       "      <td>00052899fc9012ce1798f7b70710304913befd28.summary</td>\n",
       "      <td>q4_stream.csv</td>\n",
       "      <td>q4_nuggets.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  query_id                                              query streamSize  \\\n",
       "0        0    Mistaken for your child's grandmother - CNN.com         61   \n",
       "0        1  Arraignment for 8 current, former Bell, Califo...         36   \n",
       "0        2    U.S. and Russia can end the suffering - CNN.com         49   \n",
       "0        3  U.N. Security Council to hold emergency meetin...         31   \n",
       "0        4  Residents evacuated as firefighters battle bla...         30   \n",
       "\n",
       "                                     query_filename   outfile_name  \\\n",
       "0  000223f0c9a759b9cdd2f86ac8c2899747937263.summary  q0_stream.csv   \n",
       "0  00030b9744b6d8c21d3a9fcb35460b06d3a71e2e.summary  q1_stream.csv   \n",
       "0  00036c4d44d9af8d34280086a887f3b7847cdae1.summary  q2_stream.csv   \n",
       "0  0003e88107432daa7a852f9fc9f26915122f218b.summary  q3_stream.csv   \n",
       "0  00052899fc9012ce1798f7b70710304913befd28.summary  q4_stream.csv   \n",
       "\n",
       "   nuggetfilename  \n",
       "0  q0_nuggets.csv  \n",
       "0  q1_nuggets.csv  \n",
       "0  q2_nuggets.csv  \n",
       "0  q3_nuggets.csv  \n",
       "0  q4_nuggets.csv  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outdf.to_csv(\"/Users/franciscojavierarceo/GitHub/DeepNLPQLearning/data2/1-output/cnn_trainingqueries.csv\", \n",
    "             index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "finalinputdir = '/Users/franciscojavierarceo/GitHub/DeepNLPQLearning/data2/0-output/'\n",
    "finaloutputdir = '/Users/franciscojavierarceo/GitHub/DeepNLPQLearning/data2/1-output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "0    1\n",
       "0    2\n",
       "0    3\n",
       "0    4\n",
       "Name: query_id, dtype: object"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outdf['query_id'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "finalinputfiles = os.listdir(finalinputdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "streams = [x for x in finalinputfiles if '_stream.csv' in x]\n",
    "nuggets = [x for x in finalinputfiles if '_nuggets.csv' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "need more than 1 value to unpack",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-0dda383e9b00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m  \u001b[0mfinalinputfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: need more than 1 value to unpack"
     ]
    }
   ],
   "source": [
    "finalpairs = [(q, s) for (q, s) in  finalinputfiles ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167136"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(finalinputfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
