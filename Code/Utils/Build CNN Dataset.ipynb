{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Each file contains four parts separated by ‘\\n\\n’. They are\n",
    "    1. url of the original article;\n",
    "    2. sentences in the article and their labels (for sentence-based extractive summarization);\n",
    "    3. extractable highlights (for word extraction-based abstractive summarization);\n",
    "    4. named entity mapping.\n",
    "\n",
    "### Sentence labels. There are three labels for the sentences: 1, 2 and 0. \n",
    "\n",
    "    - 1: sentence should extracted; \n",
    "    - 2: sentence might be extracted; \n",
    "    - 0: sentence shouldn't be extracted.\n",
    "\n",
    "### Extractable highlights\n",
    "\n",
    "The extractable highlights are created by examining if a word (or its morphological transformation) in the highlight appears in the article or a general purpose stop-word list, which together constitute the output space (i.e., the allowed vocabulary during summary generation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib2\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildEntityDictionary(input_path, filenames):\n",
    "    # Swapping in the entity names\n",
    "    entitykey, entityname = [], []\n",
    "    for filename in filenames:\n",
    "        f = open(os.path.join(input_path, filename))\n",
    "        data = f.read()\n",
    "        entities = data.split(\"\\n\\n\")[3]\n",
    "\n",
    "        for entity in entities.split(\"\\n\"):\n",
    "            entitykey.append( entity.split(\":\")[0] )\n",
    "            entityname.append( entity.split(\":\")[1] )\n",
    "\n",
    "    edictionary = dict(zip(entitykey, entityname))\n",
    "    return edictionary    \n",
    "\n",
    "def cleandata(input_path, files, edict):\n",
    "    f = open(os.path.join(input_path, files))\n",
    "    data = f.read()\n",
    "\n",
    "    url  = data.split(\"\\n\\n\")[0]\n",
    "    article = data.split(\"\\n\\n\")[1]\n",
    "    nuggets = data.split(\"\\n\\n\")[2]\n",
    "    entities = data.split(\"\\n\\n\")[3]\n",
    "\n",
    "    # Parsing the sentences and substituting\n",
    "    sentencelist, sentencelabel = [], []\n",
    "    for sentence in article.split(\"\\n\"):\n",
    "        # Swapping in the entity names\n",
    "        sentencelabel.append(int(sentence.split(\"\\t\\t\\t\")[1]))\n",
    "        sentence = sentence.split(\"\\t\\t\\t\")[0]\n",
    "        newsentence = ' '.join([edict[word] if word in edict else word for word in sentence.split(\" \")])\n",
    "        sentencelist.append(newsentence)\n",
    "\n",
    "    # Collecting the sentences in a list\n",
    "    df = pd.DataFrame(sentencelist, columns=['Sentence'])\n",
    "    df['Label'] = sentencelabel\n",
    "\n",
    "    # Extracting the nuggets\n",
    "    highlight = []\n",
    "    for nugget in nuggets.split(\"\\n\"):\n",
    "        newnugget = ' '.join([edict[word] if word in edict else word for word in nugget.split(\" \")])\n",
    "        highlight.append(newnugget)\n",
    "\n",
    "    nuggets = pd.DataFrame(highlight, columns=['Nugget'])\n",
    "    # Getting the title/query\n",
    "#     response = urllib2.urlopen(url)\n",
    "    html = requests.get(url).text\n",
    "#     html = response.read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    try:\n",
    "        title = soup.findAll(\"title\")[0].text\n",
    "    except:\n",
    "        title = 'MISSING'\n",
    "    return title, nuggets, df, df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputpath = '/Users/franciscojavierarceo/GitHub/DeepNLPQLearning/data/neuralsum/cnn/training/'\n",
    "outputpath = '/Users/franciscojavierarceo/GitHub/DeepNLPQLearning/data2/0-output'\n",
    "datafiles = os.listdir(inputpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "edict = buildEntityDictionary(inputpath, datafiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filelist = os.listdir(outputpath)\n",
    "finished = [int(x.replace(\"q\",'').replace(\"_stream.csv\", '')) for x in filelist if 'stream.csv' in x]\n",
    "finishedval = max(finished)\n",
    "# outdf = pd.DataFrame(columns=['query_id','query','streamSize','query_filename', 'outfile_name', 'nuggetfilename'])\n",
    "\n",
    "for i, datafile in enumerate(datafiles):\n",
    "    if i > finishedval:\n",
    "        query, nuggets, stream, streamSize = cleandata(inputpath, datafile, edict)\n",
    "        outfilename = 'q%i_stream.csv' % i\n",
    "        nuggetfilename = 'q%i_nuggets.csv' % i\n",
    "        tmpdf = pd.DataFrame( [i, query, streamSize, datafile, outfilename, nuggetfilename] ).T\n",
    "        tmpdf.columns = ['query_id','query','streamSize','query_filename', 'outfile_name', 'nuggetfilename']\n",
    "        outdf = pd.concat([outdf, tmpdf], axis=0)\n",
    "        stream.to_csv(os.path.join(outputpath, outfilename), index=False)\n",
    "        nuggets.to_csv(os.path.join(outputpath, nuggetfilename), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outdf.to_csv(\"/Users/franciscojavierarceo/GitHub/DeepNLPQLearning/data2/1-output/cnn_trainingqueries.csv\", \n",
    "             index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "finalinputdir = '/Users/franciscojavierarceo/GitHub/DeepNLPQLearning/data2/0-output/'\n",
    "finaloutputdir = '/Users/franciscojavierarceo/GitHub/DeepNLPQLearning/data2/1-output/'\n",
    "\n",
    "finalinputfiles = os.listdir(finalinputdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating the summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "streams = [(int(x.split(\"_\")[0].replace(\"q\",'')), x) for x in finalinputfiles if '_stream.csv' in x]\n",
    "nuggets = [(int(x.split(\"_\")[0].replace(\"q\",'')), x) for x in finalinputfiles if '_nuggets.csv' in x]\n",
    "\n",
    "streamsummary = pd.DataFrame(streams, columns=['query_id','streamname'])\n",
    "nuggetsummary = pd.DataFrame(nuggets, columns=['query_id','nuggetname'])\n",
    "\n",
    "fulldf = pd.merge(streamsummary, nuggetsummary, how='inner', left_on = 'query_id', right_on='query_id')\n",
    "dupes = outdf.drop_duplicates(inplace=False)['query_id'].value_counts().reset_index()\n",
    "dupes.columns = ['query_id', 'count']\n",
    "dupes = dupes[dupes['count'] > 1]\n",
    "dedupefilter = outdf['query_id'].isin(dupes['query_id'])==False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Exporting files\n",
    "outdf[dedupefilter].to_csv(\n",
    "    \"/Users/franciscojavierarceo/GitHub/DeepNLPQLearning/data2/1-output/cnn_trainingqueries.csv\", \n",
    "             index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85198, 2541426)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outdf.shape[0], outdf[dedupefilter]['streamSize'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, (index, row) in enumerate(outdf[dedupefilter].iterrows()):\n",
    "    if i == 0:\n",
    "        cleanedstreams = pd.read_csv(finalinputdir + row['outfile_name'])\n",
    "        cleanedstreams['query'] = row['query'].replace(\" - CNN.com\", \"\")\n",
    "        cleanedstreams['query_id'] = row['query_id']\n",
    "    else:\n",
    "        tmp  = pd.read_csv(finalinputdir + row['outfile_name'])\n",
    "        tmp['query'] = row['query'].replace(\" - CNN.com\", \"\")\n",
    "        tmp['query_id'] = row['query_id']\n",
    "        cleanedstreams = pd.concat([cleanedstreams, tmp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "      <th>query</th>\n",
       "      <th>query_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-- i 'm 45 , and my son is 7</td>\n",
       "      <td>1</td>\n",
       "      <td>Mistaken for your child's grandmother</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>once in a while , i still get carded when i tr...</td>\n",
       "      <td>1</td>\n",
       "      <td>Mistaken for your child's grandmother</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i was 38 when Dominican Republic Emergency Ope...</td>\n",
       "      <td>1</td>\n",
       "      <td>Mistaken for your child's grandmother</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>both incidents took place after i moved from A...</td>\n",
       "      <td>1</td>\n",
       "      <td>Mistaken for your child's grandmother</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i thought about the incidents when i read a re...</td>\n",
       "      <td>2</td>\n",
       "      <td>Mistaken for your child's grandmother</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Label  \\\n",
       "0                       -- i 'm 45 , and my son is 7      1   \n",
       "1  once in a while , i still get carded when i tr...      1   \n",
       "2  i was 38 when Dominican Republic Emergency Ope...      1   \n",
       "3  both incidents took place after i moved from A...      1   \n",
       "4  i thought about the incidents when i read a re...      2   \n",
       "\n",
       "                                   query  query_id  \n",
       "0  Mistaken for your child's grandmother         0  \n",
       "1  Mistaken for your child's grandmother         0  \n",
       "2  Mistaken for your child's grandmother         0  \n",
       "3  Mistaken for your child's grandmother         0  \n",
       "4  Mistaken for your child's grandmother         0  "
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanedstreams.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2541426, 4)"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanedstreams.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleanedstreams.to_csv(\n",
    "    \"/Users/franciscojavierarceo/GitHub/DeepNLPQLearning/data2/1-output/cnn_trainingstreams.csv\", \n",
    "             index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
