{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Each file contains four parts separated by ‘\\n\\n’. They are\n",
    "    1. url of the original article;\n",
    "    2. sentences in the article and their labels (for sentence-based extractive summarization);\n",
    "    3. extractable highlights (for word extraction-based abstractive summarization);\n",
    "    4. named entity mapping.\n",
    "\n",
    "### Sentence labels. There are three labels for the sentences: 1, 2 and 0. \n",
    "\n",
    "    - 1: sentence should extracted; \n",
    "    - 2: sentence might be extracted; \n",
    "    - 0: sentence shouldn't be extracted.\n",
    "\n",
    "### Extractable highlights\n",
    "\n",
    "The extractable highlights are created by examining if a word (or its morphological transformation) in the highlight appears in the article or a general purpose stop-word list, which together constitute the output space (i.e., the allowed vocabulary during summary generation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# import urllib2\n",
    "from urllib.request import urlopen\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildEntityDictionary(input_path, filenames):\n",
    "    # Swapping in the entity names\n",
    "    entitykey, entityname = [], []\n",
    "    for filename in filenames:\n",
    "        f = open(os.path.join(input_path, filename))\n",
    "        data = f.read()\n",
    "        entities = data.split(\"\\n\\n\")[3]\n",
    "\n",
    "        for entity in entities.split(\"\\n\"):\n",
    "            entitykey.append( entity.split(\":\")[0] )\n",
    "            entityname.append( entity.split(\":\")[1] )\n",
    "\n",
    "    edictionary = dict(zip(entitykey, entityname))\n",
    "    return edictionary    \n",
    "\n",
    "def cleandata(input_path, files, edict):\n",
    "    f = open(os.path.join(input_path, files))\n",
    "    data = f.read()\n",
    "\n",
    "    url  = data.split(\"\\n\\n\")[0]\n",
    "    article = data.split(\"\\n\\n\")[1]\n",
    "    nuggets = data.split(\"\\n\\n\")[2]\n",
    "    entities = data.split(\"\\n\\n\")[3]\n",
    "\n",
    "    # Parsing the sentences and substituting\n",
    "    sentencelist, sentencelabel = [], []\n",
    "    for sentence in article.split(\"\\n\"):\n",
    "        # Swapping in the entity names\n",
    "        sentencelabel.append(int(sentence.split(\"\\t\\t\\t\")[1]))\n",
    "        sentence = sentence.split(\"\\t\\t\\t\")[0]\n",
    "        newsentence = ' '.join([edict[word] if word in edict else word for word in sentence.split(\" \")])\n",
    "        sentencelist.append(newsentence)\n",
    "\n",
    "    # Collecting the sentences in a list\n",
    "    df = pd.DataFrame(sentencelist, columns=['Sentence'])\n",
    "    df['Label'] = sentencelabel\n",
    "\n",
    "    # Extracting the nuggets\n",
    "    highlight = []\n",
    "    for nugget in nuggets.split(\"\\n\"):\n",
    "        newnugget = ' '.join([edict[word] if word in edict else word for word in nugget.split(\" \")])\n",
    "        highlight.append(newnugget)\n",
    "\n",
    "    nuggets = pd.DataFrame(highlight, columns=['Nugget'])\n",
    "    # Getting the title/query\n",
    "#    html = requests.get(url).text\n",
    "    html = urlopen(url).read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    try:\n",
    "        title = soup.findAll(\"title\")[0].text\n",
    "    except:\n",
    "        title = 'MISSING'\n",
    "    return title, nuggets, df, df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inputpath = '/Users/franciscojavierarceo/GitHub/DeepNLPQLearning/data/neuralsum/cnn/training/'\n",
    "outputpath = '/Users/franciscojavierarceo/GitHub/DeepNLPQLearning/data2/0-output'\n",
    "# datafiles = os.listdir(inputpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "edict = buildEntityDictionary(inputpath, datafiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filelist = os.listdir(outputpath)\n",
    "finished = [int(x.replace(\"q\",'').replace(\"_stream.csv\", '')) for x in filelist if 'stream.csv' in x]\n",
    "finishedval = max(finished)\n",
    "# outdf = pd.DataFrame(columns=['query_id','query','streamSize','query_filename', 'outfile_name', 'nuggetfilename'])\n",
    "\n",
    "for i, datafile in enumerate(datafiles):\n",
    "    if i > finishedval:\n",
    "        query, nuggets, stream, streamSize = cleandata(inputpath, datafile, edict)\n",
    "        outfilename = 'q%i_stream.csv' % i\n",
    "        nuggetfilename = 'q%i_nuggets.csv' % i\n",
    "        tmpdf = pd.DataFrame( [i, query, streamSize, datafile, outfilename, nuggetfilename] ).T\n",
    "        tmpdf.columns = ['query_id','query','streamSize','query_filename', 'outfile_name', 'nuggetfilename']\n",
    "        outdf = pd.concat([outdf, tmpdf], axis=0)\n",
    "        stream.to_csv(os.path.join(outputpath, outfilename), index=False)\n",
    "        nuggets.to_csv(os.path.join(outputpath, nuggetfilename), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outdf.to_csv(\"/Users/franciscojavierarceo/GitHub/DeepNLPQLearning/data2/1-output/cnn_trainingqueries.csv\", \n",
    "             index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "finalinputdir = '/Users/franciscojavierarceo/GitHub/DeepNLPQLearning/data2/0-output/'\n",
    "finaloutputdir = '/Users/franciscojavierarceo/GitHub/DeepNLPQLearning/data2/1-output/'\n",
    "\n",
    "finalinputfiles = os.listdir(finalinputdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating the summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "streams = [(int(x.split(\"_\")[0].replace(\"q\",'')), x) for x in finalinputfiles if '_stream.csv' in x]\n",
    "nuggets = [(int(x.split(\"_\")[0].replace(\"q\",'')), x) for x in finalinputfiles if '_nuggets.csv' in x]\n",
    "\n",
    "streamsummary = pd.DataFrame(streams, columns=['query_id','streamname'])\n",
    "nuggetsummary = pd.DataFrame(nuggets, columns=['query_id','nuggetname'])\n",
    "\n",
    "fulldf = pd.merge(streamsummary, nuggetsummary, how='inner', left_on = 'query_id', right_on='query_id')\n",
    "dupes = outdf.drop_duplicates(inplace=False)['query_id'].value_counts().reset_index()\n",
    "dupes.columns = ['query_id', 'count']\n",
    "dupes = dupes[dupes['count'] > 1]\n",
    "dedupefilter = outdf['query_id'].isin(dupes['query_id'])==False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exporting files\n",
    "outdf[dedupefilter].to_csv(\n",
    "    \"/Users/franciscojavierarceo/GitHub/DeepNLPQLearning/data2/1-output/cnn_trainingqueries.csv\", \n",
    "             index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85198, 2541426)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outdf.shape[0], outdf[dedupefilter]['streamSize'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.829643888354187"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outdf[dedupefilter]['streamSize'].sum() / float(outdf.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data back in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#import urllib2\n",
    "#import requests\n",
    "import pandas as pd\n",
    "#from bs4 import BeautifulSoup\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "finalinputdir = '/home/francisco/GitHub/DQN-Event-Summarization/data/0-output/'\n",
    "finaloutputdir = '/home/francisco/GitHub/DQN-Event-Summarization/data/1-output/'\n",
    "\n",
    "finalinputfiles = os.listdir(finalinputdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outdf = pd.read_csv(\"/home/francisco/GitHub/DQN-Event-Summarization/data/1-output/cnn_trainingqueries.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_data(inputdir, odf_row):\n",
    "        cleanedstreams = pd.read_csv(inputdir + odf_row['outfile_name'])\n",
    "        cleanedstreams['query'] = odf_row['query'].replace(\" - CNN.com\", \"\")\n",
    "        cleanedstreams['query_id'] = odf_row['query_id']\n",
    "        cleanedstreams['true_summary'] = ' '.join(cleanedstreams[cleanedstreams['Label']==1].Sentence)\n",
    "        cleanedstreams['sentence_idx'] = cleanedstreams.index\n",
    "        return cleanedstreams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "streams = Parallel(n_jobs=-1)(\n",
    "    delayed(extract_data)(finalinputdir, row) for i, row in outdf.iterrows()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleanedstreams = pd.concat(streams)\n",
    "cleanedstreams = cleanedstreams[['query_id', 'sentence_idx', 'Label','query','Sentence', 'true_summary']]\n",
    "cleanedstreams.columns = [x.lower() for x in cleanedstreams.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>label</th>\n",
       "      <th>query</th>\n",
       "      <th>sentence</th>\n",
       "      <th>true_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Mistaken for your child's grandmother</td>\n",
       "      <td>-- i 'm 45 , and my son is 7</td>\n",
       "      <td>-- i 'm 45 , and my son is 7 once in a while ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Mistaken for your child's grandmother</td>\n",
       "      <td>once in a while , i still get carded when i tr...</td>\n",
       "      <td>-- i 'm 45 , and my son is 7 once in a while ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Mistaken for your child's grandmother</td>\n",
       "      <td>i was 38 when Dominican Republic Emergency Ope...</td>\n",
       "      <td>-- i 'm 45 , and my son is 7 once in a while ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Mistaken for your child's grandmother</td>\n",
       "      <td>both incidents took place after i moved from A...</td>\n",
       "      <td>-- i 'm 45 , and my son is 7 once in a while ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>Mistaken for your child's grandmother</td>\n",
       "      <td>i thought about the incidents when i read a re...</td>\n",
       "      <td>-- i 'm 45 , and my son is 7 once in a while ,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query_id  sentence_idx  label                                  query  \\\n",
       "0         0             0      1  Mistaken for your child's grandmother   \n",
       "1         0             1      1  Mistaken for your child's grandmother   \n",
       "2         0             2      1  Mistaken for your child's grandmother   \n",
       "3         0             3      1  Mistaken for your child's grandmother   \n",
       "4         0             4      2  Mistaken for your child's grandmother   \n",
       "\n",
       "                                            sentence  \\\n",
       "0                       -- i 'm 45 , and my son is 7   \n",
       "1  once in a while , i still get carded when i tr...   \n",
       "2  i was 38 when Dominican Republic Emergency Ope...   \n",
       "3  both incidents took place after i moved from A...   \n",
       "4  i thought about the incidents when i read a re...   \n",
       "\n",
       "                                        true_summary  \n",
       "0  -- i 'm 45 , and my son is 7 once in a while ,...  \n",
       "1  -- i 'm 45 , and my son is 7 once in a while ,...  \n",
       "2  -- i 'm 45 , and my son is 7 once in a while ,...  \n",
       "3  -- i 'm 45 , and my son is 7 once in a while ,...  \n",
       "4  -- i 'm 45 , and my son is 7 once in a while ,...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanedstreams.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleanedstreams.to_csv(\n",
    "    \"/Users/franciscojavierarceo/GitHub/DeepNLPQLearning/data2/1-output/cnn_trainingstreams.csv\", \n",
    "             index=False, encoding='utf-8'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>label</th>\n",
       "      <th>query</th>\n",
       "      <th>sentence</th>\n",
       "      <th>true_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Mistaken for your child's grandmother</td>\n",
       "      <td>-- i 'm 45 , and my son is 7</td>\n",
       "      <td>-- i 'm 45 , and my son is 7 once in a while ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Mistaken for your child's grandmother</td>\n",
       "      <td>once in a while , i still get carded when i tr...</td>\n",
       "      <td>-- i 'm 45 , and my son is 7 once in a while ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Mistaken for your child's grandmother</td>\n",
       "      <td>i was 38 when Dominican Republic Emergency Ope...</td>\n",
       "      <td>-- i 'm 45 , and my son is 7 once in a while ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Mistaken for your child's grandmother</td>\n",
       "      <td>both incidents took place after i moved from A...</td>\n",
       "      <td>-- i 'm 45 , and my son is 7 once in a while ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>Mistaken for your child's grandmother</td>\n",
       "      <td>i thought about the incidents when i read a re...</td>\n",
       "      <td>-- i 'm 45 , and my son is 7 once in a while ,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query_id  sentence_idx  label                                  query  \\\n",
       "0         0             0      1  Mistaken for your child's grandmother   \n",
       "1         0             1      1  Mistaken for your child's grandmother   \n",
       "2         0             2      1  Mistaken for your child's grandmother   \n",
       "3         0             3      1  Mistaken for your child's grandmother   \n",
       "4         0             4      2  Mistaken for your child's grandmother   \n",
       "\n",
       "                                            sentence  \\\n",
       "0                       -- i 'm 45 , and my son is 7   \n",
       "1  once in a while , i still get carded when i tr...   \n",
       "2  i was 38 when Dominican Republic Emergency Ope...   \n",
       "3  both incidents took place after i moved from A...   \n",
       "4  i thought about the incidents when i read a re...   \n",
       "\n",
       "                                        true_summary  \n",
       "0  -- i 'm 45 , and my son is 7 once in a while ,...  \n",
       "1  -- i 'm 45 , and my son is 7 once in a while ,...  \n",
       "2  -- i 'm 45 , and my son is 7 once in a while ,...  \n",
       "3  -- i 'm 45 , and my son is 7 once in a while ,...  \n",
       "4  -- i 'm 45 , and my son is 7 once in a while ,...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanedstreams.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import pickle\n",
    "import csv\n",
    "import gzip\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from gensim import corpora\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/francisco/GitHub/DQN-Event-Summarization/Code/Utils\r\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"/home/francisco/GitHub/DQN-Event-Summarization/data/1-output/cnn_trainingstreams.csv\")\n",
    "df = pd.read_csv(\"/Users/franciscojavierarceo/GitHub/DeepNLPQLearning/data2/1-output/cnn_trainingstreams.csv\", nrows=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>label</th>\n",
       "      <th>query</th>\n",
       "      <th>sentence</th>\n",
       "      <th>true_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Mistaken for your child's grandmother</td>\n",
       "      <td>-- i 'm 45 , and my son is 7</td>\n",
       "      <td>-- i 'm 45 , and my son is 7 once in a while ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Mistaken for your child's grandmother</td>\n",
       "      <td>once in a while , i still get carded when i tr...</td>\n",
       "      <td>-- i 'm 45 , and my son is 7 once in a while ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Mistaken for your child's grandmother</td>\n",
       "      <td>i was 38 when Dominican Republic Emergency Ope...</td>\n",
       "      <td>-- i 'm 45 , and my son is 7 once in a while ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Mistaken for your child's grandmother</td>\n",
       "      <td>both incidents took place after i moved from A...</td>\n",
       "      <td>-- i 'm 45 , and my son is 7 once in a while ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>Mistaken for your child's grandmother</td>\n",
       "      <td>i thought about the incidents when i read a re...</td>\n",
       "      <td>-- i 'm 45 , and my son is 7 once in a while ,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query_id  sentence_idx  label                                  query  \\\n",
       "0         0             0      1  Mistaken for your child's grandmother   \n",
       "1         0             1      1  Mistaken for your child's grandmother   \n",
       "2         0             2      1  Mistaken for your child's grandmother   \n",
       "3         0             3      1  Mistaken for your child's grandmother   \n",
       "4         0             4      2  Mistaken for your child's grandmother   \n",
       "\n",
       "                                            sentence  \\\n",
       "0                       -- i 'm 45 , and my son is 7   \n",
       "1  once in a while , i still get carded when i tr...   \n",
       "2  i was 38 when Dominican Republic Emergency Ope...   \n",
       "3  both incidents took place after i moved from A...   \n",
       "4  i thought about the incidents when i read a re...   \n",
       "\n",
       "                                        true_summary  \n",
       "0  -- i 'm 45 , and my son is 7 once in a while ,...  \n",
       "1  -- i 'm 45 , and my son is 7 once in a while ,...  \n",
       "2  -- i 'm 45 , and my son is 7 once in a while ,...  \n",
       "3  -- i 'm 45 , and my son is 7 once in a while ,...  \n",
       "4  -- i 'm 45 , and my son is 7 once in a while ,...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['true_summary'] = df['true_summary'].str.replace('[^A-Za-z0-9]+', ' ').str.strip().str.lower().fillna(\"\")\n",
    "df['sentence'] = df['sentence'].str.replace('[^A-Za-z0-9]+', ' ').str.strip().str.lower().fillna(\"\")\n",
    "df['query'] = df['query'].str.replace('[^A-Za-z0-9]+', ' ').str.strip().str.lower().fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = df['query'][0].split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mistaken', 'for', 'your', 'child', 's', 'grandmother']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary([texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.id2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'child': 4,\n",
       " u'for': 0,\n",
       " u'grandmother': 2,\n",
       " u'mistaken': 1,\n",
       " u's': 3,\n",
       " u'your': 5}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/10000 (10%) complete rows.\n",
      "2000/10000 (20%) complete rows.\n",
      "3000/10000 (30%) complete rows.\n",
      "4000/10000 (40%) complete rows.\n",
      "5000/10000 (50%) complete rows.\n",
      "6000/10000 (60%) complete rows.\n",
      "7000/10000 (70%) complete rows.\n",
      "8000/10000 (80%) complete rows.\n",
      "9000/10000 (90%) complete rows.\n",
      "10000/10000 (100%) complete rows.\n"
     ]
    }
   ],
   "source": [
    "frequency = defaultdict(int)\n",
    "\n",
    "n = df.shape[0]\n",
    "div = n // 10\n",
    "qtokens, stokens, tstokens = [], [], []\n",
    "for i, row in df.iterrows():\n",
    "    qtokens+= [row['query'].split(\" \")]\n",
    "    stokens+= [row['sentence'].split(\" \")]\n",
    "    tstokens+= [row['true_summary'].split(\" \")]\n",
    "    if ((i + 1) % div) == 0:\n",
    "        print(\"%i/%i (%i%%) complete rows.\" % (i+1, n, (i+1)/float(n) * 100 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Getting the dictionary with token info\n",
    "dictionary = corpora.Dictionary(stokens + qtokens + tstokens )\n",
    "\n",
    "# Mapping to numeric list -- adding plus one to tokens\n",
    "dictionary.token2id = {k: v + 1 for k,v in dictionary.token2id.items()}\n",
    "word2idx = dictionary.token2id\n",
    "\n",
    "dictionary.id2token = {v:k for k,v in dictionary.token2id.items()}\n",
    "idx2word = dictionary.id2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: u'and',\n",
       " 2: u'i',\n",
       " 3: u'is',\n",
       " 4: u'45',\n",
       " 5: u'm',\n",
       " 6: u'son',\n",
       " 7: u'7',\n",
       " 8: u'my',\n",
       " 9: u'certain',\n",
       " 10: u'an',\n",
       " 11: u'as',\n",
       " 12: u'guarded',\n",
       " 13: u'in',\n",
       " 14: u'cold',\n",
       " 15: u'still',\n",
       " 16: u'your',\n",
       " 17: u'if',\n",
       " 18: u'aback',\n",
       " 19: u'different',\n",
       " 20: u'pause',\n",
       " 21: u'hesitation',\n",
       " 22: u'awkward',\n",
       " 23: u'get',\n",
       " 24: u'when',\n",
       " 25: u'two',\n",
       " 26: u'strangers',\n",
       " 27: u'to',\n",
       " 28: u'only',\n",
       " 29: u'assumption',\n",
       " 30: u'taken',\n",
       " 31: u'was',\n",
       " 32: u'surprised',\n",
       " 33: u'buy',\n",
       " 34: u'option',\n",
       " 35: u'that',\n",
       " 36: u'grandmother',\n",
       " 37: u'most',\n",
       " 38: u'by',\n",
       " 39: u'recent',\n",
       " 40: u'a',\n",
       " 41: u'carded',\n",
       " 42: u'instant',\n",
       " 43: u'grandson',\n",
       " 44: u'medicines',\n",
       " 45: u'months',\n",
       " 46: u'well',\n",
       " 47: u'asked',\n",
       " 48: u'try',\n",
       " 49: u'while',\n",
       " 50: u'without',\n",
       " 51: u'so',\n",
       " 52: u'the',\n",
       " 53: u'or',\n",
       " 54: u'once',\n",
       " 55: u'operations',\n",
       " 56: u'be',\n",
       " 57: u'38',\n",
       " 58: u'center',\n",
       " 59: u'emergency',\n",
       " 60: u'mother',\n",
       " 61: u'age',\n",
       " 62: u'exactly',\n",
       " 63: u'young',\n",
       " 64: u'but',\n",
       " 65: u'born',\n",
       " 66: u'near',\n",
       " 67: u'republic',\n",
       " 68: u'nowhere',\n",
       " 69: u'imagine',\n",
       " 70: u'not',\n",
       " 71: u'dominican',\n",
       " 72: u'moved',\n",
       " 73: u'fellow',\n",
       " 74: u'city',\n",
       " 75: u'from',\n",
       " 76: u'21',\n",
       " 77: u'los',\n",
       " 78: u'black',\n",
       " 79: u'international',\n",
       " 80: u'berry',\n",
       " 81: u'mike',\n",
       " 82: u'who',\n",
       " 83: u'aubrey',\n",
       " 84: u'after',\n",
       " 85: u'took',\n",
       " 86: u'one',\n",
       " 87: u'women',\n",
       " 88: u'both',\n",
       " 89: u'about',\n",
       " 90: u'louis',\n",
       " 91: u'times',\n",
       " 92: u'angeles',\n",
       " 93: u'airport',\n",
       " 94: u'incidents',\n",
       " 95: u'place',\n",
       " 96: u'senior',\n",
       " 97: u'krzyzewski',\n",
       " 98: u'essay',\n",
       " 99: u'on',\n",
       " 100: u'b',\n",
       " 101: u'daughter',\n",
       " 102: u'her',\n",
       " 103: u'for',\n",
       " 104: u'being',\n",
       " 105: u'mistaken',\n",
       " 106: u'thought',\n",
       " 107: u'read',\n",
       " 108: u's',\n",
       " 109: u'r',\n",
       " 110: u'nanny',\n",
       " 111: u'case',\n",
       " 112: u'grubb',\n",
       " 113: u'responses',\n",
       " 114: u'post',\n",
       " 115: u'of',\n",
       " 116: u'inspired',\n",
       " 117: u'kidding',\n",
       " 118: u'makeover',\n",
       " 119: u'me',\n",
       " 120: u'lively',\n",
       " 121: u'tips',\n",
       " 122: u'got',\n",
       " 123: u'you',\n",
       " 124: u'where',\n",
       " 125: u'ranging',\n",
       " 126: u'identity',\n",
       " 127: u'are',\n",
       " 128: u'patrol',\n",
       " 129: u'had',\n",
       " 130: u'grandmothers',\n",
       " 131: u'high',\n",
       " 132: u'have',\n",
       " 133: u'escape',\n",
       " 134: u'given',\n",
       " 135: u'question',\n",
       " 136: u'should',\n",
       " 137: u'rates',\n",
       " 138: u'too',\n",
       " 139: u'historically',\n",
       " 140: u'teen',\n",
       " 141: u'hairs',\n",
       " 142: u'let',\n",
       " 143: u'community',\n",
       " 144: u'birth',\n",
       " 145: u'gray',\n",
       " 146: u'especially',\n",
       " 147: u'maybe',\n",
       " 148: u'n',\n",
       " 149: u't',\n",
       " 150: u'many',\n",
       " 151: u'guess',\n",
       " 152: u'like',\n",
       " 153: u'whether',\n",
       " 154: u'caretaker',\n",
       " 155: u'over',\n",
       " 156: u'since',\n",
       " 157: u'harder',\n",
       " 158: u'it',\n",
       " 159: u'40',\n",
       " 160: u'rearing',\n",
       " 161: u'trends',\n",
       " 162: u'mature',\n",
       " 163: u'child',\n",
       " 164: u'convergence',\n",
       " 165: u'grandparent',\n",
       " 166: u'learned',\n",
       " 167: u'more',\n",
       " 168: u'make',\n",
       " 169: u'er',\n",
       " 170: u'teenagers',\n",
       " 171: u'fewer',\n",
       " 172: u'say',\n",
       " 173: u'experts',\n",
       " 174: u'babies',\n",
       " 175: u'having',\n",
       " 176: u'first',\n",
       " 177: u'we',\n",
       " 178: u'based',\n",
       " 179: u'says',\n",
       " 180: u'programs',\n",
       " 181: u'ceo',\n",
       " 182: u'work',\n",
       " 183: u'independiente',\n",
       " 184: u'best',\n",
       " 185: u'evidence',\n",
       " 186: u'founded',\n",
       " 187: u'rico',\n",
       " 188: u'carlos',\n",
       " 189: u'pregnancy',\n",
       " 190: u'puerto',\n",
       " 191: u'believe',\n",
       " 192: u'arroyo',\n",
       " 193: u'giving',\n",
       " 194: u'also',\n",
       " 195: u'kids',\n",
       " 196: u'hope',\n",
       " 197: u'seeing',\n",
       " 198: u'all',\n",
       " 199: u'rate',\n",
       " 200: u'world',\n",
       " 201: u'education',\n",
       " 202: u'even',\n",
       " 203: u'what',\n",
       " 204: u'vannasing',\n",
       " 205: u'their',\n",
       " 206: u'currently',\n",
       " 207: u'delay',\n",
       " 208: u'brighter',\n",
       " 209: u'do',\n",
       " 210: u'sue',\n",
       " 211: u'nation',\n",
       " 212: u'reason',\n",
       " 213: u'they',\n",
       " 214: u're',\n",
       " 215: u'highest',\n",
       " 216: u'with',\n",
       " 217: u'than',\n",
       " 218: u'circumstances',\n",
       " 219: u'13',\n",
       " 220: u'1995',\n",
       " 221: u'ranks',\n",
       " 222: u'no',\n",
       " 223: u'according',\n",
       " 224: u'future',\n",
       " 225: u'elbit',\n",
       " 226: u'states',\n",
       " 227: u'including',\n",
       " 228: u'year',\n",
       " 229: u'olds',\n",
       " 230: u'decreases',\n",
       " 231: u'between',\n",
       " 232: u'war',\n",
       " 233: u'shows',\n",
       " 234: u'births',\n",
       " 235: u'israeli',\n",
       " 236: u'dramatic',\n",
       " 237: u'iraq',\n",
       " 238: u'data',\n",
       " 239: u'cochin',\n",
       " 240: u'10',\n",
       " 241: u'15',\n",
       " 242: u'17',\n",
       " 243: u'island',\n",
       " 244: u'2007',\n",
       " 245: u'ltd',\n",
       " 246: u'2009',\n",
       " 247: u'trend',\n",
       " 248: u'national',\n",
       " 249: u'brazil',\n",
       " 250: u'lowest',\n",
       " 251: u'gaviria',\n",
       " 252: u'has',\n",
       " 253: u'tracked',\n",
       " 254: u'point',\n",
       " 255: u'19',\n",
       " 256: u'preliminary',\n",
       " 257: u'ages',\n",
       " 258: u'been',\n",
       " 259: u'years',\n",
       " 260: u'this',\n",
       " 261: u'nearly',\n",
       " 262: u'70',\n",
       " 263: u'reached',\n",
       " 264: u'teens',\n",
       " 265: u'cesar',\n",
       " 266: u'its',\n",
       " 267: u'published',\n",
       " 268: u'1991',\n",
       " 269: u'per',\n",
       " 270: u'1',\n",
       " 271: u'000',\n",
       " 272: u'2',\n",
       " 273: u'london',\n",
       " 274: u'plummeted',\n",
       " 275: u'69',\n",
       " 276: u'86',\n",
       " 277: u'example',\n",
       " 278: u'whites',\n",
       " 279: u'hispanic',\n",
       " 280: u'non',\n",
       " 281: u'much',\n",
       " 282: u'higher',\n",
       " 283: u'11',\n",
       " 284: u'february',\n",
       " 285: u'compared',\n",
       " 286: u'just',\n",
       " 287: u'32',\n",
       " 288: u'41',\n",
       " 289: u'at',\n",
       " 290: u'report',\n",
       " 291: u'life',\n",
       " 292: u'right',\n",
       " 293: u'weighing',\n",
       " 294: u'grandchildren',\n",
       " 295: u'people',\n",
       " 296: u'grandparents',\n",
       " 297: u'wrong',\n",
       " 298: u'options',\n",
       " 299: u'raising',\n",
       " 300: u'raised',\n",
       " 301: u'children',\n",
       " 302: u'theirs',\n",
       " 303: u'percentage',\n",
       " 304: u'reports',\n",
       " 305: u'doubled',\n",
       " 306: u'white',\n",
       " 307: u'july',\n",
       " 308: u'constant',\n",
       " 309: u'5',\n",
       " 310: u'figure',\n",
       " 311: u'remained',\n",
       " 312: u'impact',\n",
       " 313: u'harsh',\n",
       " 314: u'million',\n",
       " 315: u'citing',\n",
       " 316: u'up',\n",
       " 317: u'indicates',\n",
       " 318: u'2000',\n",
       " 319: u'economic',\n",
       " 320: u'primarily',\n",
       " 321: u'were',\n",
       " 322: u'9',\n",
       " 323: u'8',\n",
       " 324: u'2008',\n",
       " 325: u'latest',\n",
       " 326: u'gamal',\n",
       " 327: u'12',\n",
       " 328: u'primary',\n",
       " 329: u'number',\n",
       " 330: u'caregivers',\n",
       " 331: u'serving',\n",
       " 332: u'spiked',\n",
       " 333: u'fell',\n",
       " 334: u'14',\n",
       " 335: u'rose',\n",
       " 336: u'becoming',\n",
       " 337: u'period',\n",
       " 338: u'eight',\n",
       " 339: u'during',\n",
       " 340: u'24',\n",
       " 341: u'18',\n",
       " 342: u'3',\n",
       " 343: u'walken',\n",
       " 344: u'53',\n",
       " 345: u'project',\n",
       " 346: u'because',\n",
       " 347: u'substance',\n",
       " 348: u'associate',\n",
       " 349: u'these',\n",
       " 350: u'come',\n",
       " 351: u'maltreatment',\n",
       " 352: u'jim',\n",
       " 353: u'abandonment',\n",
       " 354: u'outreach',\n",
       " 355: u'abuse',\n",
       " 356: u'christianity',\n",
       " 357: u'director',\n",
       " 358: u'live',\n",
       " 359: u'said',\n",
       " 360: u'launched',\n",
       " 361: u'turkish',\n",
       " 362: u'daly',\n",
       " 363: u'research',\n",
       " 364: u'dr',\n",
       " 365: u'today',\n",
       " 366: u'settling',\n",
       " 367: u'again',\n",
       " 368: u'raise',\n",
       " 369: u'family',\n",
       " 370: u'thinking',\n",
       " 371: u'gearing',\n",
       " 372: u'retiring',\n",
       " 373: u'sixties',\n",
       " 374: u'fifties',\n",
       " 375: u'entering',\n",
       " 376: u'late',\n",
       " 377: u'down',\n",
       " 378: u'opposed',\n",
       " 379: u'referrals',\n",
       " 380: u'financial',\n",
       " 381: u'eligible',\n",
       " 382: u'assistance',\n",
       " 383: u'health',\n",
       " 384: u'services',\n",
       " 385: u'caretakers',\n",
       " 386: u'gives',\n",
       " 387: u'housing',\n",
       " 388: u'counties',\n",
       " 389: u'care',\n",
       " 390: u'families',\n",
       " 391: u'average',\n",
       " 392: u'60',\n",
       " 393: u'75',\n",
       " 394: u'participate',\n",
       " 395: u'led',\n",
       " 396: u'waiting',\n",
       " 397: u'there',\n",
       " 398: u'list',\n",
       " 399: u'central',\n",
       " 400: u'nic',\n",
       " 401: u'rest',\n",
       " 402: u'psychologist',\n",
       " 403: u'assistant',\n",
       " 404: u'robertson',\n",
       " 405: u'avenue',\n",
       " 406: u'25',\n",
       " 407: u'30',\n",
       " 408: u'practice',\n",
       " 409: u'private',\n",
       " 410: u'patients',\n",
       " 411: u'she',\n",
       " 412: u'doing',\n",
       " 413: u've',\n",
       " 414: u'andi',\n",
       " 415: u'1997',\n",
       " 416: u'incarcerated',\n",
       " 417: u'57',\n",
       " 418: u'sometimes',\n",
       " 419: u'problems',\n",
       " 420: u'three',\n",
       " 421: u'outside',\n",
       " 422: u'lives',\n",
       " 423: u'davis',\n",
       " 424: u'responsibility',\n",
       " 425: u'living',\n",
       " 426: u'office',\n",
       " 427: u'day',\n",
       " 428: u'part',\n",
       " 429: u'time',\n",
       " 430: u'switzerland',\n",
       " 431: u'law',\n",
       " 432: u'until',\n",
       " 433: u'left',\n",
       " 434: u'challenges',\n",
       " 435: u'disorder',\n",
       " 436: u'medical',\n",
       " 437: u'rare',\n",
       " 438: u'blood',\n",
       " 439: u'youngest',\n",
       " 440: u'start',\n",
       " 441: u'now',\n",
       " 442: u'college',\n",
       " 443: u'23',\n",
       " 444: u'grandkids',\n",
       " 445: u'friends',\n",
       " 446: u'married',\n",
       " 447: u'some',\n",
       " 448: u'16',\n",
       " 449: u'later',\n",
       " 450: u'alone',\n",
       " 451: u'increase',\n",
       " 452: u'44',\n",
       " 453: u'group',\n",
       " 454: u'longer',\n",
       " 455: u'jumped',\n",
       " 456: u'1970',\n",
       " 457: u'2006',\n",
       " 458: u'4',\n",
       " 459: u'6',\n",
       " 460: u'22',\n",
       " 461: u'mothers',\n",
       " 462: u'observers',\n",
       " 463: u'confused',\n",
       " 464: u'wonder',\n",
       " 465: u'experienced',\n",
       " 466: u'christian',\n",
       " 467: u'writer',\n",
       " 468: u'seventh',\n",
       " 469: u'stay',\n",
       " 470: u'oscars',\n",
       " 471: u'whiplash',\n",
       " 472: u'coptic',\n",
       " 473: u'home',\n",
       " 474: u'meant',\n",
       " 475: u'then',\n",
       " 476: u'constantly',\n",
       " 477: u'youthful',\n",
       " 478: u'face',\n",
       " 479: u'cute',\n",
       " 480: u'realized',\n",
       " 481: u'woman',\n",
       " 482: u'restaurant',\n",
       " 483: u'seconds',\n",
       " 484: u'shock',\n",
       " 485: u'grandbaby',\n",
       " 486: u'looked',\n",
       " 487: u'behind',\n",
       " 488: u'mouth',\n",
       " 489: u'came',\n",
       " 490: u'hanging',\n",
       " 491: u'baby',\n",
       " 492: u'weeks',\n",
       " 493: u'speechless',\n",
       " 494: u'open',\n",
       " 495: u'bigger',\n",
       " 496: u'closed',\n",
       " 497: u'girls',\n",
       " 498: u'enough',\n",
       " 499: u'oldest',\n",
       " 500: u'old',\n",
       " 501: u'possible',\n",
       " 502: u'think',\n",
       " 503: u'status',\n",
       " 504: u'blurted',\n",
       " 505: u'previously',\n",
       " 506: u'explain',\n",
       " 507: u'regional',\n",
       " 508: u'customs',\n",
       " 509: u'mom',\n",
       " 510: u'new',\n",
       " 511: u'surprise',\n",
       " 512: u'out',\n",
       " 513: u'might',\n",
       " 514: u'why',\n",
       " 515: u'before',\n",
       " 516: u'atlanta',\n",
       " 517: u'lived',\n",
       " 518: u'recently',\n",
       " 519: u'school',\n",
       " 520: u'room',\n",
       " 521: u'harvard',\n",
       " 522: u'author',\n",
       " 523: u'42',\n",
       " 524: u'ngos',\n",
       " 525: u'duchess',\n",
       " 526: u'state',\n",
       " 527: u'other',\n",
       " 528: u'treat',\n",
       " 529: u'department',\n",
       " 530: u'situation',\n",
       " 531: u'37',\n",
       " 532: u'last',\n",
       " 533: u'husband',\n",
       " 534: u'marcus',\n",
       " 535: u'brown',\n",
       " 536: u'professor',\n",
       " 537: u'susan',\n",
       " 538: u'neiman',\n",
       " 539: u'biden',\n",
       " 540: u'mall',\n",
       " 541: u'joe',\n",
       " 542: u'rice',\n",
       " 543: u'dan',\n",
       " 544: u'southpark',\n",
       " 545: u'eventually',\n",
       " 546: u'his',\n",
       " 547: u'pregnant',\n",
       " 548: u'ca',\n",
       " 549: u'weighed',\n",
       " 550: u'will',\n",
       " 551: u'carefully',\n",
       " 552: u'glance',\n",
       " 553: u'tell',\n",
       " 554: u'wait',\n",
       " 555: u'earth',\n",
       " 556: u'website',\n",
       " 557: u'com',\n",
       " 558: u'www',\n",
       " 559: u'tananarivedue',\n",
       " 560: u'accused',\n",
       " 561: u'misappropriation',\n",
       " 562: u'won',\n",
       " 563: u'funds',\n",
       " 564: u'wednesday',\n",
       " 565: u'public',\n",
       " 566: u'current',\n",
       " 567: u'reduction',\n",
       " 568: u'officials',\n",
       " 569: u'california',\n",
       " 570: u'alaska',\n",
       " 571: u'bail',\n",
       " 572: u'former',\n",
       " 573: u'attorney',\n",
       " 574: u'october',\n",
       " 575: u'arraignment',\n",
       " 576: u'arrested',\n",
       " 577: u'them',\n",
       " 578: u'defense',\n",
       " 579: u'five',\n",
       " 580: u'others',\n",
       " 581: u'postponed',\n",
       " 582: u'reset',\n",
       " 583: u'manager',\n",
       " 584: u'police',\n",
       " 585: u'metropolitan',\n",
       " 586: u'275',\n",
       " 587: u'human',\n",
       " 588: u'350',\n",
       " 589: u'beverly',\n",
       " 590: u'went',\n",
       " 591: u'mayor',\n",
       " 592: u'377',\n",
       " 593: u'showed',\n",
       " 594: u'vice',\n",
       " 595: u'cybersecurity',\n",
       " 596: u'can',\n",
       " 597: u'anytime',\n",
       " 598: u'member',\n",
       " 599: u'sources',\n",
       " 600: u'legitimate',\n",
       " 601: u'council',\n",
       " 602: u'communications',\n",
       " 603: u'judge',\n",
       " 604: u'ruled',\n",
       " 605: u'phantom',\n",
       " 606: u'misappropriated',\n",
       " 607: u'district',\n",
       " 608: u'charges',\n",
       " 609: u'meetings',\n",
       " 610: u'paid',\n",
       " 611: u'allege',\n",
       " 612: u'tuesday',\n",
       " 613: u'bank',\n",
       " 614: u'european',\n",
       " 615: u'olympic',\n",
       " 616: u'several',\n",
       " 617: u'media',\n",
       " 618: u'thousand',\n",
       " 619: u'chief',\n",
       " 620: u'each',\n",
       " 621: u'making',\n",
       " 622: u'hundred',\n",
       " 623: u'resigned',\n",
       " 624: u'dollars',\n",
       " 625: u'myself',\n",
       " 626: u'charged',\n",
       " 627: u'feelin',\n",
       " 628: u'complaint',\n",
       " 629: u'own',\n",
       " 630: u'working',\n",
       " 631: u'citizens',\n",
       " 632: u'hard',\n",
       " 633: u'tax',\n",
       " 634: u'used',\n",
       " 635: u'piggy',\n",
       " 636: u'which',\n",
       " 637: u'collected',\n",
       " 638: u'alleges',\n",
       " 639: u'looted',\n",
       " 640: u'elisabetta',\n",
       " 641: u'defendants',\n",
       " 642: u'grillo',\n",
       " 643: u'councilman',\n",
       " 644: u'illinois',\n",
       " 645: u'annually',\n",
       " 646: u'counts',\n",
       " 647: u'800',\n",
       " 648: u'conflict',\n",
       " 649: u'interest',\n",
       " 650: u'among',\n",
       " 651: u'prosecutors',\n",
       " 652: u'contracts',\n",
       " 653: u'never',\n",
       " 654: u'allegations',\n",
       " 655: u'employment',\n",
       " 656: u'approved',\n",
       " 657: u'wrote',\n",
       " 658: u'he',\n",
       " 659: u'himself',\n",
       " 660: u'1994',\n",
       " 661: u'unauthorized',\n",
       " 662: u'september',\n",
       " 663: u'loans',\n",
       " 664: u'officer',\n",
       " 665: u'saatchi',\n",
       " 666: u'hired',\n",
       " 667: u'democrats',\n",
       " 668: u'dozens',\n",
       " 669: u'administrative',\n",
       " 670: u'gave',\n",
       " 671: u'responsible',\n",
       " 672: u'losses',\n",
       " 673: u'witnessed',\n",
       " 674: u'unprofessional',\n",
       " 675: u'asia',\n",
       " 676: u'arrest',\n",
       " 677: u'client',\n",
       " 678: u'ensured',\n",
       " 679: u'reporters',\n",
       " 680: u'called',\n",
       " 681: u'grossly',\n",
       " 682: u'wants',\n",
       " 683: u'good',\n",
       " 684: u'campaign',\n",
       " 685: u'candidate',\n",
       " 686: u'material',\n",
       " 687: u'general',\n",
       " 688: u'candidacy',\n",
       " 689: u'reference',\n",
       " 690: u'salary',\n",
       " 691: u'adding',\n",
       " 692: u'openly',\n",
       " 693: u'transparent',\n",
       " 694: u'fight',\n",
       " 695: u'followed',\n",
       " 696: u'did',\n",
       " 697: u'loan',\n",
       " 698: u'witnesses',\n",
       " 699: u'program',\n",
       " 700: u'interview',\n",
       " 701: u'pertinent',\n",
       " 702: u'procedures',\n",
       " 703: u'geological',\n",
       " 704: u'misappropriating',\n",
       " 705: u'survey',\n",
       " 706: u'approximately',\n",
       " 707: u'u',\n",
       " 708: u'dozen',\n",
       " 709: u'held',\n",
       " 710: u'bogus',\n",
       " 711: u'minutes',\n",
       " 712: u'lasted',\n",
       " 713: u'payment',\n",
       " 714: u'provided',\n",
       " 715: u'checks',\n",
       " 716: u'balances',\n",
       " 717: u'collecting',\n",
       " 718: u'busy',\n",
       " 719: u'enriching',\n",
       " 720: u'money',\n",
       " 721: u'occurred',\n",
       " 722: u'members',\n",
       " 723: u'themselves',\n",
       " 724: u'sued',\n",
       " 725: u'muslim',\n",
       " 726: u'salaries',\n",
       " 727: u'brotherhood',\n",
       " 728: u'excess',\n",
       " 729: u'recover',\n",
       " 730: u'benefits',\n",
       " 731: u'pension',\n",
       " 732: u'pursuing',\n",
       " 733: u'duty',\n",
       " 734: u'enormous',\n",
       " 735: u'very',\n",
       " 736: u'enrich',\n",
       " 737: u'magnitude',\n",
       " 738: u'pensions',\n",
       " 739: u'obscene',\n",
       " 740: u'matter',\n",
       " 741: u'trigger',\n",
       " 742: u'serious',\n",
       " 743: u'breach',\n",
       " 744: u'similar',\n",
       " 745: u'fraud',\n",
       " 746: u'accuses',\n",
       " 747: u'civil',\n",
       " 748: u'conspiracy',\n",
       " 749: u'suit',\n",
       " 750: u'fiduciary',\n",
       " 751: u'waste',\n",
       " 752: u'pay',\n",
       " 753: u'amount',\n",
       " 754: u'misled',\n",
       " 755: u'deliberately',\n",
       " 756: u'accurate',\n",
       " 757: u'australia',\n",
       " 758: u'business',\n",
       " 759: u'624',\n",
       " 760: u'india',\n",
       " 761: u'36',\n",
       " 762: u'southeast',\n",
       " 763: u'census',\n",
       " 764: u'residents',\n",
       " 765: u'annual',\n",
       " 766: u'median',\n",
       " 767: u'income',\n",
       " 768: u'35',\n",
       " 769: u'less',\n",
       " 770: u'received',\n",
       " 771: u'787',\n",
       " 772: u'earned',\n",
       " 773: u'638',\n",
       " 774: u'336',\n",
       " 775: u'457',\n",
       " 776: u'base',\n",
       " 777: u'sized',\n",
       " 778: u'house',\n",
       " 779: u'cutting',\n",
       " 780: u'cities',\n",
       " 781: u'96',\n",
       " 782: u'yearly',\n",
       " 783: u'jump',\n",
       " 784: u'1993',\n",
       " 785: u'percent',\n",
       " 786: u'47',\n",
       " 787: u'2005',\n",
       " 788: u'2001',\n",
       " 789: u'raises',\n",
       " 790: u'awarded',\n",
       " 791: u'saying',\n",
       " 792: u'would',\n",
       " 793: u'respectively',\n",
       " 794: u'memo',\n",
       " 795: u'governor',\n",
       " 796: u'prepared',\n",
       " 797: u'running',\n",
       " 798: u'185',\n",
       " 799: u'673',\n",
       " 800: u'month',\n",
       " 801: u'478',\n",
       " 802: u'cnn',\n",
       " 803: u'john',\n",
       " 804: u'contributed',\n",
       " 805: u'witherow',\n",
       " 806: u'air',\n",
       " 807: u'operates',\n",
       " 808: u'riel',\n",
       " 809: u'moving',\n",
       " 810: u'awards',\n",
       " 811: u'troops',\n",
       " 812: u'disruption',\n",
       " 813: u'steady',\n",
       " 814: u'into',\n",
       " 815: u'rights',\n",
       " 816: u'watch',\n",
       " 817: u'soul',\n",
       " 818: u'notwithstanding',\n",
       " 819: u'lawson',\n",
       " 820: u'train',\n",
       " 821: u'music',\n",
       " 822: u'keeps',\n",
       " 823: u'roads',\n",
       " 824: u'keystone',\n",
       " 825: u'haiti',\n",
       " 826: u'essential',\n",
       " 827: u'mat',\n",
       " 828: u'anthony',\n",
       " 829: u'protect',\n",
       " 830: u'sphere',\n",
       " 831: u'insists',\n",
       " 832: u'away',\n",
       " 833: u'influence',\n",
       " 834: u'ii',\n",
       " 835: u'few',\n",
       " 836: u'miles',\n",
       " 837: u'roderick',\n",
       " 838: u'ethnic',\n",
       " 839: u'burton',\n",
       " 840: u'claims',\n",
       " 841: u'special',\n",
       " 842: u'interests',\n",
       " 843: u'great',\n",
       " 844: u'allies',\n",
       " 845: u'theory',\n",
       " 846: u'installations',\n",
       " 847: u'serve',\n",
       " 848: u'mutual',\n",
       " 849: u'power',\n",
       " 850: u'security',\n",
       " 851: u'demonstrate',\n",
       " 852: u'bases',\n",
       " 853: u'shared',\n",
       " 854: u'help',\n",
       " 855: u'act',\n",
       " 856: u'together',\n",
       " 857: u'severe',\n",
       " 858: u'alleviate',\n",
       " 859: u'faces',\n",
       " 860: u'crisis',\n",
       " 861: u'history',\n",
       " 862: u'terrorist',\n",
       " 863: u'violence',\n",
       " 864: u'perhaps',\n",
       " 865: u'acts',\n",
       " 866: u'perished',\n",
       " 867: u'depravity',\n",
       " 868: u'unspeakable',\n",
       " 869: u'suddenly',\n",
       " 870: u'homes',\n",
       " 871: u'lost',\n",
       " 872: u'possessions',\n",
       " 873: u'half',\n",
       " 874: u'lit',\n",
       " 875: u'urgent',\n",
       " 876: u'beacon',\n",
       " 877: u'calling',\n",
       " 878: u'government',\n",
       " 879: u'expectations',\n",
       " 880: u'fair',\n",
       " 881: u'getting',\n",
       " 882: u'respond',\n",
       " 883: u'little',\n",
       " 884: u'beyond',\n",
       " 885: u'door',\n",
       " 886: u'burning',\n",
       " 887: u'fire',\n",
       " 888: u'offered',\n",
       " 889: u'occupies',\n",
       " 890: u'reminded',\n",
       " 891: u'next',\n",
       " 892: u'gestures',\n",
       " 893: u'how',\n",
       " 894: u'token',\n",
       " 895: u'neighbor',\n",
       " 896: u'far',\n",
       " 897: u'actually',\n",
       " 898: u'rushing',\n",
       " 899: u'comes',\n",
       " 900: u'week',\n",
       " 901: u'certainly',\n",
       " 902: u'abdullah',\n",
       " 903: u'al',\n",
       " 904: u'hamid',\n",
       " 905: u'summit',\n",
       " 906: u'agenda',\n",
       " 907: u'president',\n",
       " 908: u'hold',\n",
       " 909: u'around',\n",
       " 910: u'offer',\n",
       " 911: u'southern',\n",
       " 912: u'resolving',\n",
       " 913: u'leadership',\n",
       " 914: u'horrendous',\n",
       " 915: u'bolivia',\n",
       " 916: u'planned',\n",
       " 917: u'methodical',\n",
       " 918: u'clearly',\n",
       " 919: u'wearing',\n",
       " 920: u'equipment',\n",
       " 921: u'rifles',\n",
       " 922: u'armored',\n",
       " 923: u'carriers',\n",
       " 924: u'military',\n",
       " 925: u'personnel',\n",
       " 926: u'assault',\n",
       " 927: u'attackers',\n",
       " 928: u'clubs',\n",
       " 929: u'uniforms',\n",
       " 930: u'operating',\n",
       " 931: u'toughs',\n",
       " 932: u'waves',\n",
       " 933: u'knives',\n",
       " 934: u'involved',\n",
       " 935: u'armed',\n",
       " 936: u'gangs',\n",
       " 937: u'entire',\n",
       " 938: u'shot',\n",
       " 939: u'neighborhoods',\n",
       " 940: u'could',\n",
       " 941: u'off',\n",
       " 942: u'torched',\n",
       " 943: u'carted',\n",
       " 944: u'remorse',\n",
       " 945: u'civilians',\n",
       " 946: u'video',\n",
       " 947: u'refugees',\n",
       " 948: u'killed',\n",
       " 949: u'victims',\n",
       " 950: u'appeared',\n",
       " 951: u'long',\n",
       " 952: u'characterize',\n",
       " 953: u'groups',\n",
       " 954: u'quick',\n",
       " 955: u'animosity',\n",
       " 956: u'cases',\n",
       " 957: u'events',\n",
       " 958: u'characterizations',\n",
       " 959: u'warned',\n",
       " 960: u'region',\n",
       " 961: u'against',\n",
       " 962: u'depth',\n",
       " 963: u'scholars',\n",
       " 964: u'league',\n",
       " 965: u'hbo',\n",
       " 966: u'tension',\n",
       " 967: u'major',\n",
       " 968: u'simmering',\n",
       " 969: u'narrative',\n",
       " 970: u'go',\n",
       " 971: u'baseball',\n",
       " 972: u'made',\n",
       " 973: u'3s',\n",
       " 974: u'component',\n",
       " 975: u'mccray',\n",
       " 976: u'political',\n",
       " 977: u'noting',\n",
       " 978: u'cautioning',\n",
       " 979: u'appearance',\n",
       " 980: u'strong',\n",
       " 981: u'omega',\n",
       " 982: u'road',\n",
       " 983: u'really',\n",
       " 984: u'inc',\n",
       " 985: u'correct',\n",
       " 986: u'lunarline',\n",
       " 987: u'1990',\n",
       " 988: u'waning',\n",
       " 989: u'suspicious',\n",
       " 990: u'interethnic',\n",
       " 991: u'days',\n",
       " 992: u'clashes',\n",
       " 993: u'roger',\n",
       " 994: u'williams',\n",
       " 995: u'pogroms',\n",
       " 996: u'ago',\n",
       " 997: u'began',\n",
       " 998: u'reflect',\n",
       " 999: u'organization',\n",
       " 1000: u'careful',\n",
       " ...}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.id2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_tokens = []\n",
    "ntexts, qtexts = [], []\n",
    "frequency = defaultdict(int)\n",
    "\n",
    "df['true_summary'] = df['true_summary'].str.replace('[^A-Za-z0-9]+', ' ').str.strip().str.lower()\n",
    "texts = [t.split(\" \") for t in df['true_summary'] ]\n",
    "\n",
    "if 'nuggets' in infilename:\n",
    "    df = pd.read_csv(infilename)\n",
    "    df['nugget_text'] = df['nugget_text'].str.replace('[^A-Za-z0-9]+', ' ').str.strip().str.lower()\n",
    "    texts = [t.split(\" \") for t in df['nugget_text'] ]\n",
    "    ntexts.append(texts)\n",
    "\n",
    "if infilename in qfilenames:\n",
    "    texts = loadQuery(infilename)\n",
    "    qtexts.append(texts)\n",
    "\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "texts = [ [token for token in text] for text in texts]\n",
    "# Collecting all the list of tokens\n",
    "all_tokens.append(texts)\n",
    "\n",
    "texts  = sum(all_tokens, [])\n",
    "qtexts = sum(qtexts, [])\n",
    "ntexts = sum(ntexts, [])\n",
    "\n",
    "# Getting the dictionary with token info\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# Mapping to numeric list -- adding plus one to tokens\n",
    "dictionary.token2id = {k: v+1 for k,v in dictionary.token2id.items()}\n",
    "word2idx = dictionary.token2id\n",
    "\n",
    "dictionary.id2token = {v:k for k,v in dictionary.token2id.items()}\n",
    "idx2word = dictionary.id2token\n",
    "\n",
    "# Exporting the dictionaries\n",
    "print(\"Exporting word to index and dictionary to word indices\")\n",
    "output = open(os.path.join(inputdir,'0-output/LSTMDQN_Dic_token2id.pkl'), 'ab+')\n",
    "pickle.dump(word2idx, output)\n",
    "output.close()\n",
    "\n",
    "output = open(os.path.join(inputdir,'0-output/LSTMDQN_Dic_id2token.pkl'), 'ab+')\n",
    "pickle.dump(idx2word, output)\n",
    "output.close()\n",
    "\n",
    "# Merging the dictionaries toa pandas data frame with summary info\n",
    "odf0 = pd.DataFrame.from_dict(dictionary.dfs, orient='index').reset_index()\n",
    "odf1 = pd.DataFrame.from_dict(word2idx, orient='index').reset_index()\n",
    "\n",
    "odf0.columns = ['id', 'frequency']\n",
    "odf1.columns = ['token', 'id']\n",
    "# Merge by token id\n",
    "odf = pd.merge(left=odf0, right=odf1, on='id')\n",
    "odf = odf[['id','token', 'frequency']]\n",
    "# Exporting data\n",
    "odf.to_csv(os.path.join(inputdir, '0-output/total_corpus_smry.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>label</th>\n",
       "      <th>query</th>\n",
       "      <th>sentence</th>\n",
       "      <th>true_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Mistaken for your child's grandmother</td>\n",
       "      <td>-- i 'm 45 , and my son is 7</td>\n",
       "      <td>-- i 'm 45 , and my son is 7 once in a while ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Mistaken for your child's grandmother</td>\n",
       "      <td>once in a while , i still get carded when i tr...</td>\n",
       "      <td>-- i 'm 45 , and my son is 7 once in a while ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Mistaken for your child's grandmother</td>\n",
       "      <td>i was 38 when Dominican Republic Emergency Ope...</td>\n",
       "      <td>-- i 'm 45 , and my son is 7 once in a while ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Mistaken for your child's grandmother</td>\n",
       "      <td>both incidents took place after i moved from A...</td>\n",
       "      <td>-- i 'm 45 , and my son is 7 once in a while ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>Mistaken for your child's grandmother</td>\n",
       "      <td>i thought about the incidents when i read a re...</td>\n",
       "      <td>-- i 'm 45 , and my son is 7 once in a while ,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query_id  sentence_idx  label                                  query  \\\n",
       "0         0             0      1  Mistaken for your child's grandmother   \n",
       "1         0             1      1  Mistaken for your child's grandmother   \n",
       "2         0             2      1  Mistaken for your child's grandmother   \n",
       "3         0             3      1  Mistaken for your child's grandmother   \n",
       "4         0             4      2  Mistaken for your child's grandmother   \n",
       "\n",
       "                                            sentence  \\\n",
       "0                       -- i 'm 45 , and my son is 7   \n",
       "1  once in a while , i still get carded when i tr...   \n",
       "2  i was 38 when Dominican Republic Emergency Ope...   \n",
       "3  both incidents took place after i moved from A...   \n",
       "4  i thought about the incidents when i read a re...   \n",
       "\n",
       "                                        true_summary  \n",
       "0  -- i 'm 45 , and my son is 7 once in a while ,...  \n",
       "1  -- i 'm 45 , and my son is 7 once in a while ,...  \n",
       "2  -- i 'm 45 , and my son is 7 once in a while ,...  \n",
       "3  -- i 'm 45 , and my son is 7 once in a while ,...  \n",
       "4  -- i 'm 45 , and my son is 7 once in a while ,...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanedstreams.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "streams = Parallel(n_jobs=-1)(\n",
    "    delayed(extract_data)(finalinputdir, row) for i, row in outdf.iterrows()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
