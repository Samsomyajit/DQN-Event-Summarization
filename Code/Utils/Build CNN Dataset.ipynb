{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Each file contains four parts separated by ‘\\n\\n’. They are\n",
    "    1. url of the original article;\n",
    "    2. sentences in the article and their labels (for sentence-based extractive summarization);\n",
    "    3. extractable highlights (for word extraction-based abstractive summarization);\n",
    "    4. named entity mapping.\n",
    "\n",
    "### Sentence labels. There are three labels for the sentences: 1, 2 and 0. \n",
    "\n",
    "    - 1: sentence should extracted; \n",
    "    - 2: sentence might be extracted; \n",
    "    - 0: sentence shouldn't be extracted.\n",
    "\n",
    "### Extractable highlights\n",
    "\n",
    "The extractable highlights are created by examining if a word (or its morphological transformation) in the highlight appears in the article or a general purpose stop-word list, which together constitute the output space (i.e., the allowed vocabulary during summary generation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib2\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildEntityDictionary(input_path, filenames):\n",
    "    # Swapping in the entity names\n",
    "    entitykey, entityname = [], []\n",
    "    for filename in filenames:\n",
    "        f = open(os.path.join(input_path, filename))\n",
    "        data = f.read()\n",
    "        entities = data.split(\"\\n\\n\")[3]\n",
    "\n",
    "        for entity in entities.split(\"\\n\"):\n",
    "            entitykey.append( entity.split(\":\")[0] )\n",
    "            entityname.append( entity.split(\":\")[1] )\n",
    "\n",
    "    edictionary = dict(zip(entitykey, entityname))\n",
    "    return edictionary    \n",
    "\n",
    "def cleandata(input_path, files, edict):\n",
    "    f = open(os.path.join(input_path, files))\n",
    "    data = f.read()\n",
    "\n",
    "    url  = data.split(\"\\n\\n\")[0]\n",
    "    article = data.split(\"\\n\\n\")[1]\n",
    "    nuggets = data.split(\"\\n\\n\")[2]\n",
    "    entities = data.split(\"\\n\\n\")[3]\n",
    "\n",
    "    # Parsing the sentences and substituting\n",
    "    sentencelist, sentencelabel = [], []\n",
    "    for sentence in article.split(\"\\n\"):\n",
    "        # Swapping in the entity names\n",
    "        sentencelabel.append(int(sentence.split(\"\\t\\t\\t\")[1]))\n",
    "        sentence = sentence.split(\"\\t\\t\\t\")[0]\n",
    "        newsentence = ' '.join([edict[word] if word in edict else word for word in sentence.split(\" \")])\n",
    "        sentencelist.append(newsentence)\n",
    "\n",
    "    # Collecting the sentences in a list\n",
    "    df = pd.DataFrame(sentencelist, columns=['Sentence'])\n",
    "    df['Label'] = sentencelabel\n",
    "\n",
    "    # Extracting the nuggets\n",
    "    highlight = []\n",
    "    for nugget in nuggets.split(\"\\n\"):\n",
    "        newnugget = ' '.join([edict[word] if word in edict else word for word in nugget.split(\" \")])\n",
    "        highlight.append(newnugget)\n",
    "\n",
    "    nuggets = pd.DataFrame(highlight, columns=['Nugget'])\n",
    "    # Getting the title/query\n",
    "#     response = urllib2.urlopen(url)\n",
    "    html = requests.get(url).text\n",
    "#     html = response.read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    title = soup.findAll(\"title\")[0].text\n",
    "    return title, nuggets, df, df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputpath = '/Users/franciscojavierarceo/GitHub/DeepNLPQLearning/data/neuralsum/cnn/training/'\n",
    "outputpath = '/Users/franciscojavierarceo/GitHub/DeepNLPQLearning/data2/0-output'\n",
    "datafiles = os.listdir(inputpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "edict = buildEntityDictionary(inputpath, datafiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-9d58143f62fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatafile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatafiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mfinishedval\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnuggets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstreamSize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcleandata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatafile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0moutfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'q%i_stream.csv'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mnuggetfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'q%i_nuggets.csv'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-f24b52a8fa70>\u001b[0m in \u001b[0;36mcleandata\u001b[0;34m(input_path, files, edict)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m#     html = response.read()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"title\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnuggets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "filelist = os.listdir(outputpath)\n",
    "finished = [int(x.replace(\"q\",'').replace(\"_stream.csv\", '')) for x in filelist if 'stream.csv' in x]\n",
    "finishedval = max(finished)\n",
    "# outdf = pd.DataFrame(columns=['query_id','query','streamSize','query_filename', 'outfile_name', 'nuggetfilename'])\n",
    "\n",
    "for i, datafile in enumerate(datafiles):\n",
    "    if i > finishedval:\n",
    "        query, nuggets, stream, streamSize = cleandata(inputpath, datafile, edict)\n",
    "        outfilename = 'q%i_stream.csv' % i\n",
    "        nuggetfilename = 'q%i_nuggets.csv' % i\n",
    "        tmpdf = pd.DataFrame( [i, query, streamSize, datafile, outfilename, nuggetfilename] ).T\n",
    "        tmpdf.columns = ['query_id','query','streamSize','query_filename', 'outfile_name', 'nuggetfilename']\n",
    "        outdf = pd.concat([outdf, tmpdf], axis=0)\n",
    "        stream.to_csv(os.path.join(outputpath, outfilename), index=False)\n",
    "        nuggets.to_csv(os.path.join(outputpath, nuggetfilename), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66525"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outdf.to_csv(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
