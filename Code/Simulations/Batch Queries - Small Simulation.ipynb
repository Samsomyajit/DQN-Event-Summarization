{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "require 'nn'\n",
    "require 'rnn'\n",
    "require 'image'\n",
    "require 'optim'\n",
    "require 'parallel'\n",
    "dl = require 'dataload'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Some useful functions\n",
    "function genNbyK(n, k, a, b)\n",
    "    out = torch.LongTensor(n, k)\n",
    "    for i=1, n do\n",
    "        for j = 1, k do\n",
    "            out[i][j] = torch.random(a, b)\n",
    "        end\n",
    "    end\n",
    "    return out\n",
    "end\n",
    "\n",
    "function buildModel(model, vocabSize, embeddingSize, metric, adapt, use_cuda)\n",
    "    -- Small experiments seem to show that the Tanh activations performed better\\\n",
    "    --      than the ReLU for the bow model\n",
    "    if model == 'bow' then\n",
    "        print(string.format(\"Running bag-of-words model to learn %s\", metric))\n",
    "        sentenceLookup = nn.Sequential()\n",
    "                    :add(nn.LookupTableMaskZero(vocabSize, embeddingSize))\n",
    "                    :add(nn.Sum(2, 3, true)) -- Not averaging blows up model so keep this true\n",
    "                    :add(nn.Tanh())\n",
    "    else\n",
    "        print(string.format(\"Running LSTM model to learn %s\", metric))\n",
    "        sentenceLookup = nn.Sequential()\n",
    "                    :add(nn.LookupTableMaskZero(vocabSize, embeddingSize))\n",
    "                    :add(nn.SplitTable(2))\n",
    "                    :add(nn.Sequencer(nn.LSTM(embeddingSize, embeddingSize)))\n",
    "                    :add(nn.SelectTable(-1))            -- selects last state of the LSTM\n",
    "                    :add(nn.Linear(embeddingSize, embeddingSize))\n",
    "                    :add(nn.ReLU())\n",
    "    end\n",
    "    local queryLookup = sentenceLookup:clone(\"weight\", \"gradWeight\") \n",
    "    local summaryLookup = sentenceLookup:clone(\"weight\", \"gradWeight\")\n",
    "    local pmodule = nn.ParallelTable()\n",
    "                :add(sentenceLookup)\n",
    "                :add(queryLookup)\n",
    "                :add(summaryLookup)\n",
    "\n",
    "    if model == 'bow' then\n",
    "        nnmodel = nn.Sequential()\n",
    "            :add(pmodule)\n",
    "            :add(nn.JoinTable(2))\n",
    "            :add(nn.Tanh())\n",
    "            :add(nn.Linear(embeddingSize * 3, 2))\n",
    "    else\n",
    "        nnmodel = nn.Sequential()\n",
    "            :add(pmodule)\n",
    "            :add(nn.JoinTable(2))\n",
    "            :add(nn.ReLU())\n",
    "            :add(nn.Linear(embeddingSize * 3, 2))\n",
    "    end\n",
    "\n",
    "    if adapt then \n",
    "        print(\"Adaptive regularization\")\n",
    "        local logmod = nn.Sequential()\n",
    "            :add(nn.Linear(embeddingSize * 3, 1))\n",
    "            :add(nn.LogSigmoid())\n",
    "            :add(nn.SoftMax())\n",
    "\n",
    "        local regmod = nn.Sequential()\n",
    "            :add(nn.Linear(embeddingSize * 3, 2))\n",
    "\n",
    "        local fullmod = nn.ConcatTable()\n",
    "            :add(regmod)\n",
    "            :add(logmod)\n",
    "\n",
    "        local final = nn.Sequential()\n",
    "            :add(pmodule)\n",
    "            :add(nn.JoinTable(2))\n",
    "            :add(fullmod)\n",
    "\n",
    "        nnmodel = final\n",
    "    end\n",
    "\n",
    "    if use_cuda then\n",
    "        return nnmodel:cuda()\n",
    "    end\n",
    "    return nnmodel\n",
    "end\n",
    "\n",
    "function Tokenize(inputdic)\n",
    "    --- This function tokenizes the words into a unigram dictionary\n",
    "    local out = {}\n",
    "    for k, v in pairs(inputdic) do\n",
    "        if v ~= 0 then \n",
    "            if out[v] == nil then\n",
    "                out[v] = 1\n",
    "            else \n",
    "                out[v] = 1 + out[v]\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return out\n",
    "end\n",
    "\n",
    "function rougeScores(genSummary, refSummary)\n",
    "    local genTotal = 0\n",
    "    local refTotal = 0\n",
    "    local intersection = 0\n",
    "    -- Inserting the missing keys\n",
    "    for k, genCount in pairs(genSummary) do\n",
    "        if refSummary[k] == nil then\n",
    "            refSummary[k] = 0\n",
    "        end\n",
    "    end\n",
    "    for k, refCount in pairs(refSummary) do\n",
    "        local genCount = genSummary[k]\n",
    "        if genCount == nil then \n",
    "            genCount = 0 \n",
    "        end\n",
    "        intersection = intersection + math.min(refCount, genCount)\n",
    "        refTotal = refTotal + refCount\n",
    "        genTotal = genTotal + genCount\n",
    "    end\n",
    "\n",
    "    recall = intersection / refTotal\n",
    "    prec = intersection / genTotal\n",
    "    if refTotal == 0 then\n",
    "        recall = 0\n",
    "    end \n",
    "    if genTotal == 0 then\n",
    "        prec = 0\n",
    "    end\n",
    "    -- tmp = {intersection, refTotal, genTotal}\n",
    "    if recall > 0 or prec > 0 then\n",
    "        f1 = (2 * recall * prec) / (recall + prec)\n",
    "    else \n",
    "        f1 = 0\n",
    "    end\n",
    "    return recall, prec, f1\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function buildPredsummary(chosenactions, inputsentences, select_index)\n",
    "    local summary = torch.zeros(inputsentences:size())\n",
    "    for i=1, chosenactions:size(1) do\n",
    "        -- the 2 is for the SELECT index, will have to make this more general later\n",
    "        if chosenactions[i][select_index] == 1 then\n",
    "            summary[i]:copy(inputsentences[i])\n",
    "        end\n",
    "    end    \n",
    "    return summary\n",
    "end\n",
    "\n",
    "function buildPredsummaryFast(chosenactions, inputsentences, select_index)\n",
    "    local n = inputsentences:size(1)\n",
    "    local k = inputsentences:size(2)\n",
    "    local summary = torch.zeros(inputsentences:size())\n",
    "    actionmatrix = chosenactions:select(2, select_index):clone():resize(n, 1):view(n, 1):expand(n, k):clone()\n",
    "    --     This line didn't work for whatever reason...gives weird indexing...\n",
    "    --     actionmatrix = chosenactions:select(2, select_index):resize(1, n):view(n, 1):expand(n, k):clone()\n",
    "    return actionmatrix:cmul(inputsentences:double())\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function buildTotalSummary(predsummary, totalPredsummary)\n",
    "    nps = predsummary:size(1)\n",
    "    n_l = totalPredsummary:size(2)\n",
    "    indices = torch.linspace(1, n_l, n_l):long() \n",
    "    for i=1, predsummary:size(1) do\n",
    "        if predsummary[i]:sum() > 0 then \n",
    "            -- maxindex = 0\n",
    "            -- for j = 1, totalPredsummary[i]:size(1) do \n",
    "            --     if totalPredsummary[i][j] == 0 then\n",
    "            --         maxindex = maxindex + 1\n",
    "            --     end\n",
    "            -- end\n",
    "            -- lenx = predsummary[i]:size(1)\n",
    "            -- totalPredsummary[i][{{maxindex - lenx + 1, maxindex}}]:copy(predsummary[i])\n",
    "\n",
    "            minindex = 1\n",
    "            for j = 1, totalPredsummary[i]:size(1) do \n",
    "                if totalPredsummary[i][j] > 0 then\n",
    "                    minindex = minindex + 1\n",
    "                end\n",
    "            end\n",
    "            lenx = predsummary[i]:size(1)\n",
    "            totalPredsummary[i][{{minindex, minindex + lenx - 1}}]:copy(predsummary[i])\n",
    "\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "-- function buildTotalSummaryFast(predsummary, inputTotalSummary)\n",
    "--     totalPredsummary = inputTotalSummary:clone()\n",
    "--     nps = predsummary:size(1)\n",
    "--     n_l = inputTotalSummary:size(2)\n",
    "--     indices = torch.linspace(1, n_l, n_l):long() \n",
    "--     for i=1, predsummary:size(1) do\n",
    "--         if predsummary[i]:sum() > 0 then \n",
    "--             -- Finding the largest index with a zero\n",
    "--             -- maxindex = torch.max(indices[torch.eq(totalPredsummary[i], 0)])\n",
    "--             -- totalPredsummary[i][{{maxindex - lenx + 1, maxindex}}]:copy(predsummary[i])\n",
    "--             -- Finding the smallest index with a zero\n",
    "--             minindex = torch.min(indices[torch.eq(inputTotalSummary[i], 0)])\n",
    "--             lenx = predsummary[i]:size(1)\n",
    "--             totalPredsummary[i][{{minindex, minindex + lenx - 1}}]:copy(predsummary[i])\n",
    "--         end\n",
    "--     end\n",
    "--     return totalPredsummary\n",
    "-- end\n",
    "\n",
    "function buildTotalSummaryFast(predsummary, inputTotalSummary)\n",
    "    tmpSummary = inputTotalSummary:clone()\n",
    "    nps = predsummary:size(1)\n",
    "    n_l = inputTotalSummary:size(2)\n",
    "    indices = torch.linspace(1, n_l, n_l):long() \n",
    "    for i=1, predsummary:size(1) do\n",
    "        if predsummary[i]:sum() > 0 then \n",
    "            -- Finding the largest index with a zero\n",
    "            -- maxindex = torch.max(indices[torch.eq(totalPredsummary[i], 0)])\n",
    "            -- totalPredsummary[i][{{maxindex - lenx + 1, maxindex}}]:copy(predsummary[i])\n",
    "            -- Finding the smallest index with a zero\n",
    "            minindex = torch.min(indices[torch.eq(tmpSummary[i], 0)])\n",
    "            lenx = predsummary[i]:size(1)\n",
    "            tmpSummary[i][{{minindex, minindex + lenx - 1}}]:copy(predsummary[i])\n",
    "        end\n",
    "    end\n",
    "    return tmpSummary\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Setting parameters\n",
    "-- n = 10\n",
    "n = 1\n",
    "n_s = 10\n",
    "k = 8\n",
    "q = 5\n",
    "a = 1\n",
    "b = 1000\n",
    "embDim = 50\n",
    "gamma = 0.3\n",
    "SKIP = 1\n",
    "SELECT = 2\n",
    "epsilon = 1\n",
    "nepochs = 1000\n",
    "fast = true\n",
    "\n",
    "maskLayer = nn.MaskedSelect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = torch.zeros(10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "require 'cutorch'\n",
    "require 'cunn'\n",
    "require 'cunnx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0  0  0  0  0\n",
       " 0  0  0  0  0\n",
       " 0  0  0  0  0\n",
       " 0  0  0  0  0\n",
       " 0  0  0  0  0\n",
       " 0  0  0  0  0\n",
       " 0  0  0  0  0\n",
       " 0  0  0  0  0\n",
       " 0  0  0  0  0\n",
       " 0  0  0  0  0\n",
       "[torch.CudaTensor of size 10x5]\n",
       "\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x:cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulating the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Simulating streams and queries\n",
    "queries = genNbyK(n, q, a, b)\n",
    "\n",
    "-- Note that the sentences are batched by sentence index so sentences[1] is the first sentence of each article\n",
    "sentences = {}\n",
    "for i=1, n_s do\n",
    "    sentences[i] = genNbyK(n, k, a, b)\n",
    "end\n",
    "-- Optimal predicted summary\n",
    "trueSummary = torch.zeros(n, k * n_s)\n",
    "-- Using this to generate the optimal actions\n",
    "true_actions = {}\n",
    "\n",
    "for i=1, n_s do \n",
    "    ---- Simulating the data\n",
    "    trueqValues = torch.rand(n, 2)\n",
    "    \n",
    "     ---- Generating the max values and getting the indices\n",
    "    qMaxtrue, qindxtrue = torch.max(trueqValues, 2)\n",
    "    \n",
    "    --- I want to select the qindx elements for each row\n",
    "    true_actions[i] = torch.zeros(n, 2):scatter(2, qindxtrue, torch.ones(trueqValues:size()))\n",
    "    best_sentences = buildPredsummaryFast(true_actions[i], sentences[i], SELECT)\n",
    "    trueSummary = buildTotalSummaryFast(best_sentences, trueSummary)\n",
    "end\n",
    "\n",
    "qTokens = {}\n",
    "for i=1, n do\n",
    "    qTokens[i] = Tokenize(trueSummary[i]:totable())\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring the rougue metrics on the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(rougeScores(Tokenize(trueSummary[1]:totable()), Tokenize(trueSummary[1]:totable())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- model = buildModel('lstm', b, embDim, 'f1', false, false)\n",
    "model = buildModel('bow', b, embDim, 'f1', false, false)\n",
    "\n",
    "params, gradParams = model:getParameters()\n",
    "criterion = nn.MSECriterion()\n",
    "maskLayer = nn.MaskedSelect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring the model on the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimParams = { learningRate = 1e-4}\n",
    "\n",
    "totalPredsummary = {}\n",
    "qValues = {}\n",
    "qActions = {}\n",
    "qPreds = {}\n",
    "rewards = {}\n",
    "lossfull = {}\n",
    "rouguef1 = {}\n",
    "\n",
    "totalPredsummary = torch.LongTensor(n, n_s * k):fill(0)\n",
    "\n",
    "memsize = n * n_s\n",
    "queryMemory = torch.zeros(memsize, q)\n",
    "qActionMemory = torch.zeros(memsize, 2)\n",
    "predSummaryMemory = torch.zeros(memsize, n_s * k)\n",
    "sentenceMemory = torch.zeros(memsize, k)\n",
    "qPredsMemory = torch.zeros(memsize, 2)\n",
    "qValuesMemory = torch.zeros(memsize, 1)\n",
    "rewardMemory = torch.zeros(memsize, 1)\n",
    "\n",
    "--- Initializing thingss\n",
    "\n",
    "for i = 1, n_s do\n",
    "    qPreds[i] = torch.zeros(n, 2)\n",
    "    qValues[i] = torch.zeros(n, 1) \n",
    "    qActions[i] = torch.zeros(n, 2)\n",
    "    rewards[i] = torch.zeros(n, 1)\n",
    "end \n",
    "\n",
    "for epoch=1, nepochs do\n",
    "    --- Reset things at the start of each epoch\n",
    "    for i=1, n_s do\n",
    "        qPreds[i]:fill(0)\n",
    "        qValues[i]:fill(0)\n",
    "        qActions[i]:fill(0)\n",
    "        rewards[i]:fill(0)\n",
    "        totalPredsummary:fill(0)\n",
    "    end\n",
    "\n",
    "    for i=1, n_s do\n",
    "        if torch.uniform(0, 1) <= epsilon then \n",
    "            qPreds[i]:copy(torch.rand(n, 2))\n",
    "            -- Need to run a forward pass for the backward to work...wonky\n",
    "            ignore = model:forward({sentences[i], queries, totalPredsummary})\n",
    "        else \n",
    "            qPreds[i]:copy(model:forward({sentences[i], queries, totalPredsummary}) )\n",
    "        end \n",
    "        if fast then \n",
    "            qMax, qindx = torch.max(qPreds[i], 2)  -- Pulling the best actions\n",
    "            -- Here's the fast way to select the optimal action for each query\n",
    "            qActions[i]:copy(qActions[i]:scatter(2, qindx, torch.ones(qPreds[i]:size())):clone())\n",
    "            qValues[i]:copy(qMax)\n",
    "            predsummary = buildPredsummaryFast(qActions[i], sentences[i], SELECT)\n",
    "            totalPredsummary = buildTotalSummaryFast(predsummary, totalPredsummary)\n",
    "        else \n",
    "            for j=1, n do\n",
    "                if qPreds[i][j][SELECT] > qPreds[i][j][SKIP] then\n",
    "                    qActions[i][j][SELECT] = 1\n",
    "                    qValues[i][j]:fill(qPreds[i][j][SELECT])\n",
    "                else\n",
    "                    qActions[i][j][SKIP] = 1\n",
    "                    qValues[i][j]:fill(qPreds[i][j][SKIP])\n",
    "                end\n",
    "            end\n",
    "            predsummary = buildPredsummary(qActions[i], sentences[i], SELECT)\n",
    "            buildTotalSummary(predsummary, totalPredsummary)\n",
    "        end\n",
    "        for j = 1, n do\n",
    "            recall, prec, f1 = rougeScores( qTokens[j],\n",
    "                                            Tokenize(totalPredsummary[j]:totable()))\n",
    "            rewards[i][j]:fill(f1)\n",
    "        end\n",
    "        if i > 1 then\n",
    "            -- Calculating change in rougue f1\n",
    "            rewards[i]:copy(rewards[i] - rewards[i-1])\n",
    "        end\n",
    "        -- Update memory sequentially until it's full \n",
    "        qActionMemory[{{n * (i-1) + 1, n * i}}]:copy(qActions[i])\n",
    "        predSummaryMemory[{{n * (i-1) + 1, n * i}}]:copy(totalPredsummary)\n",
    "        sentenceMemory[{{n * (i-1) + 1, n * i}}]:copy(sentences[i])\n",
    "        qPredsMemory[{{n * (i-1) + 1, n * i}}]:copy(qPreds[i])\n",
    "        qValuesMemory[{{n * (i-1) + 1, n * i}}]:copy(qValues[i])\n",
    "        queryMemory[{{n * (i-1) + 1, n * i}}]:copy(queries)\n",
    "        if i  < n_s then\n",
    "            rewardMemory[{{n * (i-1) + 1, n * i}}]:copy(rewards[i] + gamma * rewards[i + 1] )\n",
    "        else\n",
    "            rewardMemory[{{n * (i-1) + 1, n * i}}]:copy(rewards[i] )\n",
    "        end\n",
    "    end\n",
    "    -- Adding back the delta for the last one\n",
    "    rouguef1[epoch] = (rewards[n_s] + rewards[ n_s - 1] ):mean()\n",
    "\n",
    "    loss = {}\n",
    "    local dataloader = dl.TensorLoader({queryMemory, sentenceMemory, predSummaryMemory, \n",
    "                            qPredsMemory, qActionMemory, qValuesMemory}, rewardMemory)\n",
    "    c = 1\n",
    "    for k, xin, reward in dataloader:sampleiter(batch_size, memsize) do\n",
    "        local function feval(params)\n",
    "            gradParams:zero()\n",
    "            if adapt then\n",
    "                local ignore = model:forward({xin[1], xin[2], xin[3]})\n",
    "                local predQOnActions = maskLayer:forward({qPredsMemory, actions_in}) \n",
    "                local ones = torch.ones(predQ:size(1)):resize(predQ:size(1))\n",
    "                lossf = criterion:forward({qValuesMemory, predReg}, {reward, ones})\n",
    "                local gradOutput = criterion:backward({qActionMemory, predReg}, {reward, ones})\n",
    "                local gradMaskLayer = maskLayer:backward({qPredsMemory, qActionMemory}, gradOutput[1])\n",
    "                model:backward({xin[1], xin[2], xin[3]}, {gradMaskLayer[1], gradOutput[2]})\n",
    "            else \n",
    "                local ignore = model:forward({xin[1], xin[2], xin[3]})\n",
    "                local predQOnActions = maskLayer:forward({xin[4], xin[5]:byte()}) \n",
    "                lossf = criterion:forward(predQOnActions, reward)\n",
    "                local gradOutput = criterion:backward(predQOnActions, reward)\n",
    "                local gradMaskLayer = maskLayer:backward({xin[4], xin[5]:byte()}, gradOutput)\n",
    "                model:backward({xin[1], xin[2], xin[3]}, gradMaskLayer[1])\n",
    "            end \n",
    "            return lossf, gradParams\n",
    "        end\n",
    "        --- optim.rmsprop returns \\theta, f(\\theta):= loss function\n",
    "         _, lossv  = optim.rmsprop(feval, params, optimParams)\n",
    "        loss[c] = lossv[1]\n",
    "        c = c + 1\n",
    "    end\n",
    "\n",
    "    lossfull[epoch] = torch.Tensor(loss):sum() / #lossv\n",
    "    if print_perf then\n",
    "        print(\n",
    "            string.format('epoch = %i; rougue = %.6f; epsilon = %.6f; loss = %.6f' , \n",
    "                epoch, rouguef1[epoch], epsilon, lossfull[epoch])\n",
    "            )\n",
    "    end\n",
    "    epsilon = epsilon - (1/10.)\n",
    "    if epsilon < 0 then\n",
    "        epsilon = 0\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Plot = require 'itorch.Plot'\n",
    "\n",
    "loss = torch.Tensor(lossfull)\n",
    "rougue = torch.Tensor(rouguef1)\n",
    "indices = torch.linspace(1, loss:size(1), loss:size(1)):long() \n",
    "plot = Plot():line(indices, loss, 'red', 'hi'):title('Plot of loss'):draw()\n",
    "plot = Plot():line(indices, rougue, 'blue', 'hi'):title('Plot of Rougue-F1'):draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rougeScores( Tokenize(trueSummary:resize(40):totable()), \n",
    "                Tokenize(totalPredsummary:totable()[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
