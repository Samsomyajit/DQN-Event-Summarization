{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "require 'nn'\n",
    "require 'rnn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Some useful functions\n",
    "function genNbyK(n, k, a, b)\n",
    "    out = torch.LongTensor(n, k)\n",
    "    for i=1, n do\n",
    "        for j = 1, k do\n",
    "            out[i][j] = torch.random(a, b)\n",
    "        end\n",
    "    end\n",
    "    return out\n",
    "end\n",
    "\n",
    "function buildModel(model, vocabSize, embeddingSize, metric, adapt, use_cuda)\n",
    "    -- Small experiments seem to show that the Tanh activations performed better\\\n",
    "    --      than the ReLU for the bow model\n",
    "    if model == 'bow' then\n",
    "        print(string.format(\"Running bag-of-words model to learn %s\", metric))\n",
    "        sentenceLookup = nn.Sequential()\n",
    "                    :add(nn.LookupTableMaskZero(vocabSize, embeddingSize))\n",
    "                    :add(nn.Sum(2, 3, true)) -- Not averaging blows up model so keep this true\n",
    "                    :add(nn.Tanh())\n",
    "    else\n",
    "        print(string.format(\"Running LSTM model to learn %s\", metric))\n",
    "        sentenceLookup = nn.Sequential()\n",
    "                    :add(nn.LookupTableMaskZero(vocabSize, embeddingSize))\n",
    "                    :add(nn.SplitTable(2))\n",
    "                    :add(nn.Sequencer(nn.LSTM(embeddingSize, embeddingSize)))\n",
    "                    :add(nn.SelectTable(-1))            -- selects last state of the LSTM\n",
    "                    :add(nn.Linear(embeddingSize, embeddingSize))\n",
    "                    :add(nn.ReLU())\n",
    "    end\n",
    "    local queryLookup = sentenceLookup:clone(\"weight\", \"gradWeight\") \n",
    "    local summaryLookup = sentenceLookup:clone(\"weight\", \"gradWeight\")\n",
    "    local pmodule = nn.ParallelTable()\n",
    "                :add(sentenceLookup)\n",
    "                :add(queryLookup)\n",
    "                :add(summaryLookup)\n",
    "\n",
    "    if model == 'bow' then\n",
    "        nnmodel = nn.Sequential()\n",
    "            :add(pmodule)\n",
    "            :add(nn.JoinTable(2))\n",
    "            :add(nn.Tanh())\n",
    "            :add(nn.Linear(embeddingSize * 3, 2))\n",
    "    else\n",
    "        nnmodel = nn.Sequential()\n",
    "            :add(pmodule)\n",
    "            :add(nn.JoinTable(2))\n",
    "            :add(nn.ReLU())\n",
    "            :add(nn.Linear(embeddingSize * 3, 2))\n",
    "    end\n",
    "\n",
    "    if adapt then \n",
    "        print(\"Adaptive regularization\")\n",
    "        local logmod = nn.Sequential()\n",
    "            :add(nn.Linear(embeddingSize * 3, 1))\n",
    "            :add(nn.LogSigmoid())\n",
    "            :add(nn.SoftMax())\n",
    "\n",
    "        local regmod = nn.Sequential()\n",
    "            :add(nn.Linear(embeddingSize * 3, 2))\n",
    "\n",
    "        local fullmod = nn.ConcatTable()\n",
    "            :add(regmod)\n",
    "            :add(logmod)\n",
    "\n",
    "        local final = nn.Sequential()\n",
    "            :add(pmodule)\n",
    "            :add(nn.JoinTable(2))\n",
    "            :add(fullmod)\n",
    "\n",
    "        nnmodel = final\n",
    "    end\n",
    "\n",
    "    if use_cuda then\n",
    "        return nnmodel:cuda()\n",
    "    end\n",
    "    return nnmodel\n",
    "end\n",
    "\n",
    "function Tokenize(inputdic)\n",
    "    --- This function tokenizes the words into a unigram dictionary\n",
    "    local out = {}\n",
    "\n",
    "    for k, v in pairs(inputdic) do\n",
    "        if out[v] == nil then\n",
    "            out[v] = 1\n",
    "        else \n",
    "            out[v] = 1 + out[v]\n",
    "        end\n",
    "    end\n",
    "    return out\n",
    "end\n",
    "\n",
    "function rougeScores(genSummary, refSummary)\n",
    "    local genTotal = 0\n",
    "    local refTotal = 0\n",
    "    local intersection = 0\n",
    "    -- Inserting the missing keys\n",
    "    for k, genCount in pairs(genSummary) do\n",
    "        if refSummary[k] == nil then\n",
    "            refSummary[k] = 0\n",
    "        end\n",
    "    end\n",
    "    for k, refCount in pairs(refSummary) do\n",
    "        local genCount = genSummary[k]\n",
    "        if genCount == nil then \n",
    "            genCount = 0 \n",
    "        end\n",
    "        intersection = intersection + math.min(refCount, genCount)\n",
    "        refTotal = refTotal + refCount\n",
    "        genTotal = genTotal + genCount\n",
    "    end\n",
    "\n",
    "    recall = intersection / refTotal\n",
    "    prec = intersection / genTotal\n",
    "    if refTotal == 0 then\n",
    "        recall = 0\n",
    "    end \n",
    "    if genTotal == 0 then\n",
    "        prec = 0\n",
    "    end\n",
    "    -- tmp = {intersection, refTotal, genTotal}\n",
    "    if recall > 0 or prec > 0 then\n",
    "        f1 = (2 * recall * prec) / (recall + prec)\n",
    "    else \n",
    "        f1 = 0\n",
    "    end\n",
    "    return recall, prec, f1\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function buildPredsummary(summary, chosenactions, inputsentences, select_index)\n",
    "    if summary == nil then\n",
    "        summary = torch.zeros(inputsentences:size())\n",
    "    end\n",
    "    for i=1, chosenactions:size(1) do\n",
    "        -- the 2 is for the SELECT index, will have to make this more general later\n",
    "        if chosenactions[i][select_index] == 1 then\n",
    "            summary[i]:copy(inputsentences[i])\n",
    "        end\n",
    "    end    \n",
    "    return summary\n",
    "end\n",
    "\n",
    "function buildPredsummaryFast(summary, chosenactions, inputsentences, select_index)\n",
    "    n = inputsentences:size(1)\n",
    "    k = inputsentences:size(2)\n",
    "    if summary == nil then\n",
    "        summary = torch.zeros(inputsentences:size())\n",
    "    end\n",
    "    actionmatrix = chosenactions:select(2, select_index):clone():resize(n, 1):view(n, 1):expand(n, k):clone()\n",
    "    --     This line didn't work for whatever reason...gives weird indexing...\n",
    "    --     actionmatrix = chosenactions:select(2, select_index):resize(1, n):view(n, 1):expand(n, k):clone()\n",
    "    return actionmatrix:cmul(inputsentences:double())\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function buildTotalSummary(predsummary, totalPredsummary)\n",
    "    nps = predsummary:size(1)\n",
    "    n_l = totalPredsummary:size(2)\n",
    "    indices = torch.linspace(1, n_l, n_l):long() \n",
    "    for i=1, predsummary:size(1) do\n",
    "        if predsummary[i]:sum() > 0 then \n",
    "            maxindex = 0\n",
    "            for j = 1, totalPredsummary[i]:size(1) do \n",
    "                if totalPredsummary[i][j] == 0 then\n",
    "                    maxindex = maxindex + 1\n",
    "                end\n",
    "            end\n",
    "            lenx = predsummary[i]:size(1)\n",
    "            totalPredsummary[i][{{maxindex - lenx + 1, maxindex}}]:copy(predsummary[i])\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "function buildTotalSummaryFast(predsummary, totalPredsummary)\n",
    "    nps = predsummary:size(1)\n",
    "    n_l = totalPredsummary:size(2)\n",
    "    indices = torch.linspace(1, n_l, n_l):long() \n",
    "    for i=1, predsummary:size(1) do\n",
    "        if predsummary[i]:sum() > 0 then \n",
    "            -- Finding the largest index with a zero\n",
    "            maxindex = torch.max(indices[torch.eq(totalPredsummary[i], 0)])\n",
    "            lenx = predsummary[i]:size(1)\n",
    "            totalPredsummary[i][{{maxindex - lenx + 1, maxindex}}]:copy(predsummary[i])\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Setting parameters\n",
    "n = 10\n",
    "n_s = 5\n",
    "k = 7\n",
    "q = 5\n",
    "a = 1\n",
    "b = 100\n",
    "embDim = 50\n",
    "SKIP = 1\n",
    "SELECT = 2\n",
    "fast = true\n",
    "\n",
    "maskLayer = nn.MaskedSelect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulating the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Simulating streams and queries\n",
    "queries = genNbyK(n, q, a, b)\n",
    "\n",
    "-- Note that the sentences are batched by sentence index so sentences[1] is the first sentence of each article\n",
    "sentences = {}\n",
    "for i=1, n_s do\n",
    "    sentences[i] = genNbyK(n, k, a, b)\n",
    "end\n",
    "\n",
    "-- Optimal predicted summary\n",
    "trueSummary = torch.zeros(n, k * n_s)\n",
    "-- Using this to generate the optimal actions\n",
    "true_actions = {}\n",
    "for i=1, n_s do \n",
    "    ---- Simulating the data\n",
    "    trueqValues = torch.rand(n, 2)\n",
    "    \n",
    "     ---- Generating the max values and getting the indices\n",
    "    qMaxtrue, qindxtrue = torch.max(trueqValues, 2)\n",
    "    \n",
    "    --- I want to select the qindx elements for each row\n",
    "    true_actions[i] = torch.zeros(n, 2):scatter(2, qindxtrue, torch.ones(trueqValues:size()))\n",
    "    best_sentences = buildPredsummaryFast(best_sentences, true_actions[i], sentences[i], SELECT)\n",
    "    buildTotalSummaryFast(best_sentences, trueSummary)\n",
    "end\n",
    "\n",
    "qTokens = {}\n",
    "for i=1, n do\n",
    "    qTokens[i] = Tokenize(trueSummary[i]:totable())\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring the rougue metrics on the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1\t1\t1\t\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(rougeScores(Tokenize(trueSummary[1]:totable()), Tokenize(trueSummary[1]:totable())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Running bag-of-words model to learn f1\t\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = buildModel('bow', b, embDim, 'f1', false, false)\n",
    "params, gradParams = model:getParameters()\n",
    "criterion = nn.MSECriterion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring the model on the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "--- Initializing things\n",
    "totalPredsummary = {}\n",
    "qValues = {}\n",
    "qActions = {}\n",
    "qPreds = {}\n",
    "rewards = {}\n",
    "\n",
    "for i = 1, n_s do\n",
    "    qPreds[i] = torch.zeros(n, 2)\n",
    "    qValues[i] = torch.zeros(n, 1) \n",
    "    qActions[i] = torch.zeros(n, 1)\n",
    "    rewards[i] = torch.zeros(n, 1)\n",
    "    totalPredsummary[i] = torch.LongTensor(n, n_s * k):fill(0)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = model:forward({sentences[i], queries, totalPredsummary[1]})\n",
    "qPreds[i] = preds:clone()\n",
    "qMax, qindx = torch.max(preds, 2)  -- Pulling the best actions\n",
    "-- Here's the fast way to select the optimal action for each query\n",
    "actions = torch.zeros(n, 2):scatter(2, qindx, torch.ones(preds:size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qActions[i] = actions:clone()\n",
    "qValues[i] = qMax\n",
    "predsummary = buildPredsummaryFast(predsummary, actions, sentences[i], SELECT)\n",
    "buildTotalSummaryFast(predsummary, totalPredsummary[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i=1, n_s do \n",
    "    preds = model:forward({sentences[i], queries, totalPredsummary[i]})\n",
    "    qPreds[i] = preds:clone()\n",
    "    if fast then \n",
    "        qMax, qindx = torch.max(preds, 2)  -- Pulling the best actions\n",
    "        -- Here's the fast way to select the optimal action for each query\n",
    "        actions = torch.zeros(n, 2):scatter(2, qindx, torch.ones(preds:size()))\n",
    "        qActions[i] = actions:clone()\n",
    "        qValues[i] = qMax\n",
    "        predsummary = buildPredsummaryFast(predsummary, actions, sentences[i], SELECT)\n",
    "        buildTotalSummaryFast(predsummary, totalPredsummary[i])\n",
    "    else \n",
    "        actions = torch.zeros(n, 2)\n",
    "        for j=1, n do\n",
    "            if preds[j][SELECT] > preds[j][SKIP] then\n",
    "                actions[j][SELECT] = 1\n",
    "            else\n",
    "                actions[j][SKIP] = 1\n",
    "            end\n",
    "        end\n",
    "        predsummary = buildPredsummary(predsummary, actions, sentences[i], SELECT)\n",
    "        buildTotalSummary(predsummary, totalPredsummary[i])       \n",
    "    end\n",
    "    for j = 1, n do\n",
    "        recall, prec, f1 = rougeScores( Tokenize(trueSummary[j]:totable()), \n",
    "                                        Tokenize(totalPredsummary[i][j]:totable()))\n",
    "        rewards[i][j]:fill(f1)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function feval(params, xinput, predQOnActions, predQ, reward, actions_in, gradOutput)\n",
    "    model:clearState()\n",
    "    gradParams:zero()\n",
    "    lossf = criterion:forward(predQOnActions, reward)\n",
    "    local gradOutput = criterion:backward(predQOnActions, reward)\n",
    "    local gradMaskLayer = maskLayer:backward({predQ, actions_in}, gradOutput)\n",
    "    model:backward(xinput, gradMaskLayer[1])\n",
    "    return lossf, gradParams\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0.2498 -0.0509\n",
       " 0.0634 -0.0946\n",
       "-0.0160 -0.0601\n",
       " 0.3205 -0.3642\n",
       " 0.3166  0.0761\n",
       "-0.0026 -0.0365\n",
       " 0.0227  0.0081\n",
       "-0.1962  0.3370\n",
       " 0.0839  0.0809\n",
       " 0.1397  0.1245\n",
       "[torch.DoubleTensor of size 10x2]\n",
       "\n",
       " 1  0\n",
       " 1  0\n",
       " 1  0\n",
       " 1  0\n",
       " 1  0\n",
       " 1  0\n",
       " 1  0\n",
       " 0  1\n",
       " 1  0\n",
       " 1  0\n",
       "[torch.ByteTensor of size 10x2]\n",
       "\n"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qPreds[i], qActions[i]:byte()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24749507947878\t\n"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradParams:zero()\n",
    "print(criterion:forward(qValues[i], rewards[i]))\n",
    "gradOutput = criterion:backward(qValues[i], rewards[i])\n",
    "gradMaskLayer = maskLayer:backward({qPreds[i], qActions[i]:byte()}, gradOutput:resize(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  1 : LongTensor - size: 10x7\n",
       "  2 : LongTensor - size: 10x5\n",
       "  3 : DoubleTensor - size: 10x35\n",
       "}\n"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xinput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model:backward(xinput, gradMaskLayer[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0700  0.0000\n",
       "-0.1473  0.0000\n",
       "-0.1232  0.0000\n",
       " 0.0241  0.0000\n",
       "-0.0167  0.0000\n",
       "-0.1662  0.0000\n",
       "-0.1326  0.0000\n",
       " 0.0000  0.0142\n",
       "-0.1032  0.0000\n",
       "-0.0121  0.0000\n",
       "[torch.DoubleTensor of size 10x2]\n",
       "\n"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradMaskLayer[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predQ = model:forward(xinput)\n",
    "predQOnActions = maskLayer:forward({predQ, actions_in}) \n",
    "lossf = criterion:forward(predQOnActions, reward)\n",
    "gradOutput = criterion:backward(predQOnActions, reward)\n",
    "gradMaskLayer = maskLayer:backward({predQ, actions_in}, gradOutput)\n",
    "model:backward(xinput, gradMaskLayer[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "[string \"local f = function() return model:backward(xi...\"]:1: attempt to index a number value\nstack traceback:\n\t[string \"local f = function() return model:backward(xi...\"]:1: in function 'f'\n\t[string \"local f = function() return model:backward(xi...\"]:1: in main chunk\n\t[C]: in function 'xpcall'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:210: in function <...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:174>\n\t...ojavierarceo/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:389: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x0101ed4d20",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "[string \"local f = function() return model:backward(xi...\"]:1: attempt to index a number value\nstack traceback:\n\t[string \"local f = function() return model:backward(xi...\"]:1: in function 'f'\n\t[string \"local f = function() return model:backward(xi...\"]:1: in main chunk\n\t[C]: in function 'xpcall'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:210: in function <...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:174>\n\t...ojavierarceo/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:389: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x0101ed4d20"
     ]
    }
   ],
   "source": [
    "model:backward(xinput, {gradMaskLayer[1], gradOutput[2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "...javierarceo/torch/install/share/lua/5.1/nn/Container.lua:67: \nIn 1 module of nn.Sequential:\nIn 1 module of nn.ParallelTable:\nIn 3 module of nn.Sequential:\n...ciscojavierarceo/torch/install/share/lua/5.1/nn/Tanh.lua:14: attempt to index local 'gradOutput' (a nil value)\nstack traceback:\n\t...ciscojavierarceo/torch/install/share/lua/5.1/nn/Tanh.lua:14: in function <...ciscojavierarceo/torch/install/share/lua/5.1/nn/Tanh.lua:11>\n\t[C]: in function 'xpcall'\n\t...javierarceo/torch/install/share/lua/5.1/nn/Container.lua:63: in function 'rethrowErrors'\n\t...avierarceo/torch/install/share/lua/5.1/nn/Sequential.lua:55: in function <...avierarceo/torch/install/share/lua/5.1/nn/Sequential.lua:50>\n\t[C]: in function 'xpcall'\n\t...javierarceo/torch/install/share/lua/5.1/nn/Container.lua:63: in function 'rethrowErrors'\n\t...erarceo/torch/install/share/lua/5.1/nn/ParallelTable.lua:19: in function 'updateGradInput'\n\t...scojavierarceo/torch/install/share/lua/5.1/nn/Module.lua:31: in function <...scojavierarceo/torch/install/share/lua/5.1/nn/Module.lua:29>\n\t[C]: in function 'xpcall'\n\t...javierarceo/torch/install/share/lua/5.1/nn/Container.lua:63: in function 'rethrowErrors'\n\t...\n\t[C]: in function 'xpcall'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:210: in function <...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:174>\n\t...ojavierarceo/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:389: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x0101ed4d20\n\nWARNING: If you see a stack trace below, it doesn't point to the place where this error occurred. Please use only the one above.\nstack traceback:\n\t[C]: in function 'error'\n\t...javierarceo/torch/install/share/lua/5.1/nn/Container.lua:67: in function 'rethrowErrors'\n\t...avierarceo/torch/install/share/lua/5.1/nn/Sequential.lua:88: in function 'f'\n\t[string \"local f = function() return model:backward(xi...\"]:1: in main chunk\n\t[C]: in function 'xpcall'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:210: in function <...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:174>\n\t...ojavierarceo/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:389: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x0101ed4d20",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "...javierarceo/torch/install/share/lua/5.1/nn/Container.lua:67: \nIn 1 module of nn.Sequential:\nIn 1 module of nn.ParallelTable:\nIn 3 module of nn.Sequential:\n...ciscojavierarceo/torch/install/share/lua/5.1/nn/Tanh.lua:14: attempt to index local 'gradOutput' (a nil value)\nstack traceback:\n\t...ciscojavierarceo/torch/install/share/lua/5.1/nn/Tanh.lua:14: in function <...ciscojavierarceo/torch/install/share/lua/5.1/nn/Tanh.lua:11>\n\t[C]: in function 'xpcall'\n\t...javierarceo/torch/install/share/lua/5.1/nn/Container.lua:63: in function 'rethrowErrors'\n\t...avierarceo/torch/install/share/lua/5.1/nn/Sequential.lua:55: in function <...avierarceo/torch/install/share/lua/5.1/nn/Sequential.lua:50>\n\t[C]: in function 'xpcall'\n\t...javierarceo/torch/install/share/lua/5.1/nn/Container.lua:63: in function 'rethrowErrors'\n\t...erarceo/torch/install/share/lua/5.1/nn/ParallelTable.lua:19: in function 'updateGradInput'\n\t...scojavierarceo/torch/install/share/lua/5.1/nn/Module.lua:31: in function <...scojavierarceo/torch/install/share/lua/5.1/nn/Module.lua:29>\n\t[C]: in function 'xpcall'\n\t...javierarceo/torch/install/share/lua/5.1/nn/Container.lua:63: in function 'rethrowErrors'\n\t...\n\t[C]: in function 'xpcall'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:210: in function <...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:174>\n\t...ojavierarceo/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:389: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x0101ed4d20\n\nWARNING: If you see a stack trace below, it doesn't point to the place where this error occurred. Please use only the one above.\nstack traceback:\n\t[C]: in function 'error'\n\t...javierarceo/torch/install/share/lua/5.1/nn/Container.lua:67: in function 'rethrowErrors'\n\t...avierarceo/torch/install/share/lua/5.1/nn/Sequential.lua:88: in function 'f'\n\t[string \"local f = function() return model:backward(xi...\"]:1: in main chunk\n\t[C]: in function 'xpcall'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:210: in function <...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:174>\n\t...ojavierarceo/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:389: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x0101ed4d20"
     ]
    }
   ],
   "source": [
    "model:backward(xinput, gradMaskLayer[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "...ierarceo/torch/install/share/lua/5.1/nn/MaskedSelect.lua:34: bad argument #3 to 'scatter' (Input tensor must have same dimensions as output tensor at /Users/franciscojavierarceo/torch/pkg/torch/lib/TH/generic/THTensorMath.c:349)\nstack traceback:\n\t[C]: in function 'scatter'\n\t...ierarceo/torch/install/share/lua/5.1/nn/MaskedSelect.lua:34: in function 'updateGradInput'\n\t...scojavierarceo/torch/install/share/lua/5.1/nn/Module.lua:31: in function 'backward'\n\t[string \"function feval(params, xinput, predQOnActions...\"]:7: in function 'feval'\n\t[string \"i = 1...\"]:3: in main chunk\n\t[C]: in function 'xpcall'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:210: in function <...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:174>\n\t...ojavierarceo/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:389: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x0101ed4d20",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "...ierarceo/torch/install/share/lua/5.1/nn/MaskedSelect.lua:34: bad argument #3 to 'scatter' (Input tensor must have same dimensions as output tensor at /Users/franciscojavierarceo/torch/pkg/torch/lib/TH/generic/THTensorMath.c:349)\nstack traceback:\n\t[C]: in function 'scatter'\n\t...ierarceo/torch/install/share/lua/5.1/nn/MaskedSelect.lua:34: in function 'updateGradInput'\n\t...scojavierarceo/torch/install/share/lua/5.1/nn/Module.lua:31: in function 'backward'\n\t[string \"function feval(params, xinput, predQOnActions...\"]:7: in function 'feval'\n\t[string \"i = 1...\"]:3: in main chunk\n\t[C]: in function 'xpcall'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:210: in function <...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:174>\n\t...ojavierarceo/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:389: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x0101ed4d20"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "xinput = {sentences[i], queries, totalPredsummary[i]}\n",
    "feval(params, xinput,  qValues[i], qPreds[i], rewards[i], qActions[i]:clone():byte(), gradOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1  0\n",
       " 1  0\n",
       " 1  0\n",
       " 0  1\n",
       " 1  0\n",
       " 0  1\n",
       " 1  0\n",
       " 0  1\n",
       " 1  0\n",
       " 0  1\n",
       "[torch.DoubleTensor of size 10x2]\n",
       "\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in n_s do\n",
    "    _, lossv  = optim.rmsprop(feval(params, {sentences[i], queries, totalPredsummary}, ),\n",
    "                                qValue[i], qActions[i], params, optimParams)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function stackMemory(newinput, memory_hist, memsize, adapt, use_cuda)\n",
    "    local sentMemory = torch.cat(newinput[1][1]:double(), memory_hist[1][1]:double(), 1)\n",
    "    local queryMemory = torch.cat(newinput[1][2]:double(), memory_hist[1][2]:double(), 1)\n",
    "    local sumryMemory = torch.cat(newinput[1][3]:double(), memory_hist[1][3]:double(), 1)\n",
    "    local rewardMemory = torch.cat(newinput[2]:double(), memory_hist[2]:double(), 1)\n",
    "\n",
    "    if adapt then\n",
    "        regMemory = torch.cat(newinput[4]:double(), memory_hist[4]:double(), 1)\n",
    "    end \n",
    "\n",
    "    if use_cuda then \n",
    "        actionMemory = torch.cat(newinput[3]:double(), memory_hist[3]:double(), 1)\n",
    "    else \n",
    "        actionMemory = torch.cat(newinput[3], memory_hist[3], 1)\n",
    "    end\n",
    "    --- specifying rows to index \n",
    "    if sentMemory:size(1) <= memsize then\n",
    "        nend = sentMemory:size(1)\n",
    "        nstart = 1\n",
    "    else \n",
    "        nstart = math.max(memsize - sentMemory:size(1), 1)\n",
    "        nend = memsize + nstart\n",
    "    end\n",
    "    --- Selecting n last data points\n",
    "    sentMemory = sentMemory[{{nstart, nend}}]\n",
    "    queryMemory = queryMemory[{{nstart, nend}}]\n",
    "    sumryMemory = sumryMemory[{{nstart, nend}}]\n",
    "    rewardMemory = rewardMemory[{{nstart, nend}}]\n",
    "    actionMemory = actionMemory[{{nstart, nend}}]\n",
    "\n",
    "    if use_cuda then\n",
    "        inputMemory = {sentMemory:cuda(), queryMemory:cuda(), sumryMemory:cuda()}\n",
    "        rewardMemory = rewardMemory:cuda()\n",
    "        actionMemory = torch.ByteTensor(#actionMemory):copy(actionMemory):cuda()\n",
    "    end\n",
    "\n",
    "    inputMemory = {sentMemory, queryMemory, sumryMemory}\n",
    "    if adapt then\n",
    "        regMemory = regMemory[{{nstart, nend}}]\n",
    "        return {inputMemory, rewardMemory, actionMemory, regMemory}\n",
    "    end \n",
    "    return {inputMemory, rewardMemory, actionMemory}\n",
    "end    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
