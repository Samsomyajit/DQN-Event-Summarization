{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "require 'nn'\n",
    "require 'rnn'\n",
    "require 'image'\n",
    "require 'optim'\n",
    "require 'parallel'\n",
    "dl = require 'dataload'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Some useful functions\n",
    "function genNbyK(n, k, a, b)\n",
    "    out = torch.LongTensor(n, k)\n",
    "    for i=1, n do\n",
    "        for j = 1, k do\n",
    "            out[i][j] = torch.random(a, b)\n",
    "        end\n",
    "    end\n",
    "    return out\n",
    "end\n",
    "\n",
    "function buildModel(model, vocabSize, embeddingSize, metric, adapt, use_cuda)\n",
    "    -- Small experiments seem to show that the Tanh activations performed better\\\n",
    "    --      than the ReLU for the bow model\n",
    "    if model == 'bow' then\n",
    "        print(string.format(\"Running bag-of-words model to learn %s\", metric))\n",
    "        sentenceLookup = nn.Sequential()\n",
    "                    :add(nn.LookupTableMaskZero(vocabSize, embeddingSize))\n",
    "                    :add(nn.Sum(2, 3, true)) -- Not averaging blows up model so keep this true\n",
    "                    :add(nn.Tanh())\n",
    "    else\n",
    "        print(string.format(\"Running LSTM model to learn %s\", metric))\n",
    "        sentenceLookup = nn.Sequential()\n",
    "                    :add(nn.LookupTableMaskZero(vocabSize, embeddingSize))\n",
    "                    :add(nn.SplitTable(2))\n",
    "                    :add(nn.Sequencer(nn.LSTM(embeddingSize, embeddingSize)))\n",
    "                    :add(nn.SelectTable(-1))            -- selects last state of the LSTM\n",
    "                    :add(nn.Linear(embeddingSize, embeddingSize))\n",
    "                    :add(nn.ReLU())\n",
    "    end\n",
    "    local queryLookup = sentenceLookup:clone(\"weight\", \"gradWeight\") \n",
    "    local summaryLookup = sentenceLookup:clone(\"weight\", \"gradWeight\")\n",
    "    local pmodule = nn.ParallelTable()\n",
    "                :add(sentenceLookup)\n",
    "                :add(queryLookup)\n",
    "                :add(summaryLookup)\n",
    "\n",
    "    if model == 'bow' then\n",
    "        nnmodel = nn.Sequential()\n",
    "            :add(pmodule)\n",
    "            :add(nn.JoinTable(2))\n",
    "            :add(nn.Tanh())\n",
    "            :add(nn.Linear(embeddingSize * 3, 2))\n",
    "    else\n",
    "        nnmodel = nn.Sequential()\n",
    "            :add(pmodule)\n",
    "            :add(nn.JoinTable(2))\n",
    "            :add(nn.ReLU())\n",
    "            :add(nn.Linear(embeddingSize * 3, 2))\n",
    "    end\n",
    "\n",
    "    if adapt then \n",
    "        print(\"Adaptive regularization\")\n",
    "        local logmod = nn.Sequential()\n",
    "            :add(nn.Linear(embeddingSize * 3, 1))\n",
    "            :add(nn.LogSigmoid())\n",
    "            :add(nn.SoftMax())\n",
    "\n",
    "        local regmod = nn.Sequential()\n",
    "            :add(nn.Linear(embeddingSize * 3, 2))\n",
    "\n",
    "        local fullmod = nn.ConcatTable()\n",
    "            :add(regmod)\n",
    "            :add(logmod)\n",
    "\n",
    "        local final = nn.Sequential()\n",
    "            :add(pmodule)\n",
    "            :add(nn.JoinTable(2))\n",
    "            :add(fullmod)\n",
    "\n",
    "        nnmodel = final\n",
    "    end\n",
    "\n",
    "    if use_cuda then\n",
    "        return nnmodel:cuda()\n",
    "    end\n",
    "    return nnmodel\n",
    "end\n",
    "\n",
    "function Tokenize(inputdic)\n",
    "    --- This function tokenizes the words into a unigram dictionary\n",
    "    local out = {}\n",
    "\n",
    "    for k, v in pairs(inputdic) do\n",
    "        if v ~= 0 then \n",
    "            if out[v] == nil then\n",
    "                out[v] = 1\n",
    "            else \n",
    "                out[v] = 1 + out[v]\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return out\n",
    "end\n",
    "\n",
    "function rougeScores(genSummary, refSummary)\n",
    "    local genTotal = 0\n",
    "    local refTotal = 0\n",
    "    local intersection = 0\n",
    "    -- Inserting the missing keys\n",
    "    for k, genCount in pairs(genSummary) do\n",
    "        if refSummary[k] == nil then\n",
    "            refSummary[k] = 0\n",
    "        end\n",
    "    end\n",
    "    for k, refCount in pairs(refSummary) do\n",
    "        local genCount = genSummary[k]\n",
    "        if genCount == nil then \n",
    "            genCount = 0 \n",
    "        end\n",
    "        intersection = intersection + math.min(refCount, genCount)\n",
    "        refTotal = refTotal + refCount\n",
    "        genTotal = genTotal + genCount\n",
    "    end\n",
    "\n",
    "    recall = intersection / refTotal\n",
    "    prec = intersection / genTotal\n",
    "    if refTotal == 0 then\n",
    "        recall = 0\n",
    "    end \n",
    "    if genTotal == 0 then\n",
    "        prec = 0\n",
    "    end\n",
    "    -- tmp = {intersection, refTotal, genTotal}\n",
    "    if recall > 0 or prec > 0 then\n",
    "        f1 = (2 * recall * prec) / (recall + prec)\n",
    "    else \n",
    "        f1 = 0\n",
    "    end\n",
    "    return recall, prec, f1\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function buildPredsummary(chosenactions, inputsentences, select_index)\n",
    "    summary = torch.zeros(inputsentences:size())\n",
    "    for i=1, chosenactions:size(1) do\n",
    "        -- the 2 is for the SELECT index, will have to make this more general later\n",
    "        if chosenactions[i][select_index] == 1 then\n",
    "            summary[i]:copy(inputsentences[i])\n",
    "        end\n",
    "    end    \n",
    "    return summary\n",
    "end\n",
    "\n",
    "function buildPredsummaryFast(chosenactions, inputsentences, select_index)\n",
    "    n = inputsentences:size(1)\n",
    "    k = inputsentences:size(2)\n",
    "    summary = torch.zeros(inputsentences:size())\n",
    "    actionmatrix = chosenactions:select(2, select_index):clone():resize(n, 1):view(n, 1):expand(n, k):clone()\n",
    "    --     This line didn't work for whatever reason...gives weird indexing...\n",
    "    --     actionmatrix = chosenactions:select(2, select_index):resize(1, n):view(n, 1):expand(n, k):clone()\n",
    "    return actionmatrix:cmul(inputsentences:double())\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function buildTotalSummary(predsummary, totalPredsummary)\n",
    "    nps = predsummary:size(1)\n",
    "    n_l = totalPredsummary:size(2)\n",
    "    indices = torch.linspace(1, n_l, n_l):long() \n",
    "    for i=1, predsummary:size(1) do\n",
    "        if predsummary[i]:sum() > 0 then \n",
    "            -- maxindex = 0\n",
    "            -- for j = 1, totalPredsummary[i]:size(1) do \n",
    "            --     if totalPredsummary[i][j] == 0 then\n",
    "            --         maxindex = maxindex + 1\n",
    "            --     end\n",
    "            -- end\n",
    "            -- lenx = predsummary[i]:size(1)\n",
    "            -- totalPredsummary[i][{{maxindex - lenx + 1, maxindex}}]:copy(predsummary[i])\n",
    "\n",
    "            minindex = 1\n",
    "            for j = 1, totalPredsummary[i]:size(1) do \n",
    "                if totalPredsummary[i][j] > 0 then\n",
    "                    minindex = minindex + 1\n",
    "                end\n",
    "            end\n",
    "            lenx = predsummary[i]:size(1)\n",
    "            totalPredsummary[i][{{minindex, minindex + lenx - 1}}]:copy(predsummary[i])\n",
    "\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "function buildTotalSummaryFast(predsummary, inputTotalSummary)\n",
    "    totalPredsummary = inputTotalSummary:clone()\n",
    "    nps = predsummary:size(1)\n",
    "    n_l = inputTotalSummary:size(2)\n",
    "    indices = torch.linspace(1, n_l, n_l):long() \n",
    "    for i=1, predsummary:size(1) do\n",
    "        if predsummary[i]:sum() > 0 then \n",
    "            -- Finding the largest index with a zero\n",
    "            -- maxindex = torch.max(indices[torch.eq(totalPredsummary[i], 0)])\n",
    "            -- totalPredsummary[i][{{maxindex - lenx + 1, maxindex}}]:copy(predsummary[i])\n",
    "            -- Finding the smallest index with a zero\n",
    "            minindex = torch.min(indices[torch.eq(inputTotalSummary[i], 0)])\n",
    "            lenx = predsummary[i]:size(1)\n",
    "            totalPredsummary[i][{{minindex, minindex + lenx - 1}}]:copy(predsummary[i])\n",
    "        end\n",
    "    end\n",
    "    return totalPredsummary\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Setting parameters\n",
    "-- n = 10\n",
    "n = 1\n",
    "n_s = 10\n",
    "k = 4\n",
    "q = 5\n",
    "a = 1\n",
    "b = 1000\n",
    "embDim = 50\n",
    "gamma = 0.\n",
    "SKIP = 1\n",
    "SELECT = 2\n",
    "epsilon = 1\n",
    "nepochs = 1000\n",
    "fast = true\n",
    "\n",
    "maskLayer = nn.MaskedSelect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulating the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Simulating streams and queries\n",
    "queries = genNbyK(n, q, a, b)\n",
    "\n",
    "-- Note that the sentences are batched by sentence index so sentences[1] is the first sentence of each article\n",
    "sentences = {}\n",
    "for i=1, n_s do\n",
    "    sentences[i] = genNbyK(n, k, a, b)\n",
    "end\n",
    "\n",
    "-- Optimal predicted summary\n",
    "trueSummary = torch.zeros(n, k * n_s)\n",
    "-- Using this to generate the optimal actions\n",
    "true_actions = {}\n",
    "for i=1, n_s do \n",
    "    ---- Simulating the data\n",
    "    trueqValues = torch.rand(n, 2)\n",
    "    \n",
    "     ---- Generating the max values and getting the indices\n",
    "    qMaxtrue, qindxtrue = torch.max(trueqValues, 2)\n",
    "    \n",
    "    --- I want to select the qindx elements for each row\n",
    "    true_actions[i] = torch.zeros(n, 2):scatter(2, qindxtrue, torch.ones(trueqValues:size()))\n",
    "    best_sentences = buildPredsummaryFast(best_sentences, true_actions[i], sentences[i], SELECT)\n",
    "    buildTotalSummaryFast(best_sentences, trueSummary)\n",
    "end\n",
    "\n",
    "qTokens = {}\n",
    "for i=1, n do\n",
    "    qTokens[i] = Tokenize(trueSummary[i]:totable())\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring the rougue metrics on the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1\t1\t1\t\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(rougeScores(Tokenize(trueSummary[1]:totable()), Tokenize(trueSummary[1]:totable())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Running LSTM model to learn f1\t\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = buildModel('lstm', b, embDim, 'f1', false, false)\n",
    "-- model = buildModel('bow', b, embDim, 'f1', false, false)\n",
    "\n",
    "params, gradParams = model:getParameters()\n",
    "criterion = nn.MSECriterion()\n",
    "maskLayer = nn.MaskedSelect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring the model on the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nepochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function buildTotalSummaryFast(predsummary, inputTotalSummary)\n",
    "    totalPredsummary = inputTotalSummary:clone()\n",
    "    nps = predsummary:size(1)\n",
    "    n_l = inputTotalSummary:size(2)\n",
    "    indices = torch.linspace(1, n_l, n_l):long() \n",
    "    for i=1, predsummary:size(1) do\n",
    "        if predsummary[i]:sum() > 0 then \n",
    "            -- Finding the largest index with a zero\n",
    "            -- maxindex = torch.max(indices[torch.eq(totalPredsummary[i], 0)])\n",
    "            -- totalPredsummary[i][{{maxindex - lenx + 1, maxindex}}]:copy(predsummary[i])\n",
    "            -- Finding the smallest index with a zero\n",
    "            minindex = torch.min(indices[torch.eq(inputTotalSummary[i], 0)])\n",
    "            lenx = predsummary[i]:size(1)\n",
    "            totalPredsummary[i][{{minindex, minindex + lenx - 1}}]:copy(predsummary[i])\n",
    "        end\n",
    "    end\n",
    "    return totalPredsummary\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "[string \"totalPredsummary = {}...\"]:10: attempt to index a nil value\nstack traceback:\n\t[string \"totalPredsummary = {}...\"]:10: in main chunk\n\t[C]: in function 'xpcall'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:210: in function <...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:174>\n\t...ojavierarceo/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:389: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x010b2ecb90",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "[string \"totalPredsummary = {}...\"]:10: attempt to index a nil value\nstack traceback:\n\t[string \"totalPredsummary = {}...\"]:10: in main chunk\n\t[C]: in function 'xpcall'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:210: in function <...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:174>\n\t...ojavierarceo/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:389: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x010b2ecb90"
     ]
    }
   ],
   "source": [
    "totalPredsummary = {}\n",
    "qValues = {}\n",
    "qActions = {}\n",
    "qPreds = {}\n",
    "rewards = {}\n",
    "lossfull = {}\n",
    "rouguef1 = {}\n",
    "\n",
    "for i=1, n_s do\n",
    "    qPreds[i]:copy(model:forward({sentences[i], queries, totalPredsummary[i]}) )\n",
    "    qMax, qindx = torch.max(qPreds[i], 2)  -- Pulling the best actions\n",
    "    -- Here's the fast way to select the optimal action for each query\n",
    "    qActions[i]:copy(qActions[i]:scatter(2, qindx, torch.ones(qPreds[i]:size())):clone())\n",
    "    qValues[i]:copy(qMax)\n",
    "    predsummary = buildPredsummaryFast(qActions[i], sentences[i], SELECT)\n",
    "    buildTotalSummaryFast(predsummary, totalPredsummary[i]:resize(1, n * n_s))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "[string \"optimParams = { learningRate = 1e-10}...\"]:69: attempt to index a number value\nstack traceback:\n\t[string \"optimParams = { learningRate = 1e-10}...\"]:69: in main chunk\n\t[C]: in function 'xpcall'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:210: in function <...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:174>\n\t...ojavierarceo/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:389: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x010b2ecb90",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "[string \"optimParams = { learningRate = 1e-10}...\"]:69: attempt to index a number value\nstack traceback:\n\t[string \"optimParams = { learningRate = 1e-10}...\"]:69: in main chunk\n\t[C]: in function 'xpcall'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:210: in function <...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:174>\n\t...ojavierarceo/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:389: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x010b2ecb90"
     ]
    }
   ],
   "source": [
    "optimParams = { learningRate = 1e-10}\n",
    "\n",
    "totalPredsummary = {}\n",
    "qValues = {}\n",
    "qActions = {}\n",
    "qPreds = {}\n",
    "rewards = {}\n",
    "lossfull = {}\n",
    "rouguef1 = {}\n",
    "\n",
    "memsize = n * n_s\n",
    "queryMemory = torch.zeros(memsize, q)\n",
    "qActionMemory = torch.zeros(memsize, 2)\n",
    "predSummaryMemory = torch.zeros(memsize, n_s * k)\n",
    "sentenceMemory = torch.zeros(memsize, k)\n",
    "qPredsMemory = torch.zeros(memsize, 2)\n",
    "qValuesMemory = torch.zeros(memsize, 1)\n",
    "rewardMemory = torch.zeros(memsize, 1)\n",
    "\n",
    "--- Initializing things\n",
    "for i = 1, n_s do\n",
    "    qPreds[i] = torch.zeros(n, 2)\n",
    "    qValues[i] = torch.zeros(n, 1) \n",
    "    qActions[i] = torch.zeros(n, 2)\n",
    "    rewards[i] = torch.zeros(n, 1)\n",
    "    totalPredsummary[i] = torch.LongTensor(n, n_s * k):fill(0)\n",
    "end \n",
    "\n",
    "for epoch=1, nepochs do\n",
    "    --- Reset things at the start of each epoch\n",
    "    for i=1, n_s do\n",
    "        qPreds[i]:fill(0)\n",
    "        qValues[i]:fill(0)\n",
    "        qActions[i]:fill(0)\n",
    "        rewards[i]:fill(0)\n",
    "        totalPredsummary[i]:fill(0)\n",
    "    end\n",
    "\n",
    "    for i=1, n_s do\n",
    "        if torch.uniform(0, 1) <= epsilon then \n",
    "            qPreds[i]:copy(torch.rand(n, 2))\n",
    "            -- Need to run a forward pass for the backward to work...wonky\n",
    "            ignore = model:forward({sentences[i], queries, totalPredsummary[i]})\n",
    "        else \n",
    "            qPreds[i]:copy(model:forward({sentences[i], queries, totalPredsummary[i]}) )\n",
    "        end \n",
    "        if fast then \n",
    "            qMax, qindx = torch.max(qPreds[i], 2)  -- Pulling the best actions\n",
    "            -- Here's the fast way to select the optimal action for each query\n",
    "            qActions[i]:copy(qActions[i]:scatter(2, qindx, torch.ones(qPreds[i]:size())):clone())\n",
    "            qValues[i]:copy(qMax)\n",
    "            predsummary = buildPredsummaryFast(qActions[i], sentences[i], SELECT)\n",
    "            buildTotalSummaryFast(predsummary, totalPredsummary[i]:resize(1, n * n_s))\n",
    "        else \n",
    "            for j=1, n do\n",
    "                if qPreds[i][j][SELECT] > qPreds[i][j][SKIP] then\n",
    "                    qActions[i][j][SELECT] = 1\n",
    "                    qValues[i][j]:fill(qPreds[i][j][SELECT])\n",
    "                else\n",
    "                    qActions[i][j][SKIP] = 1\n",
    "                    qValues[i][j]:fill(qPreds[i][j][SKIP])\n",
    "                end\n",
    "            end\n",
    "            predsummary = buildPredsummary(qActions[i], sentences[i], SELECT)\n",
    "            buildTotalSummary(predsummary, totalPredsummary[i])\n",
    "        end\n",
    "        for j = 1, n do\n",
    "            recall, prec, f1 = rougeScores( qTokens[j],\n",
    "                                            Tokenize(totalPredsummary[i][j]:totable()))\n",
    "            rewards[i][j]:fill(f1)\n",
    "        end\n",
    "        if i > 1 then\n",
    "            -- Calculating change in rougue f1\n",
    "            rewards[i]:copy(rewards[i] - rewards[i-1])\n",
    "        end\n",
    "        -- Update memory sequentially until it's full \n",
    "        qActionMemory[{{n * (i-1) + 1, n * i}}]:copy(qActions[i])\n",
    "        predSummaryMemory[{{n * (i-1) + 1, n * i}}]:copy(totalPredsummary[i])\n",
    "        sentenceMemory[{{n * (i-1) + 1, n * i}}]:copy(sentences[i])\n",
    "        qPredsMemory[{{n * (i-1) + 1, n * i}}]:copy(qPreds[i])\n",
    "        qValuesMemory[{{n * (i-1) + 1, n * i}}]:copy(qValues[i])\n",
    "        queryMemory[{{n * (i-1) + 1, n * i}}]:copy(queries)\n",
    "        if i  < n_s then\n",
    "            rewardMemory[{{n * (i-1) + 1, n * i}}]:copy(rewards[i] + gamma * rewards[i + 1] )\n",
    "        else\n",
    "            rewardMemory[{{n * (i-1) + 1, n * i}}]:copy(rewards[i] )\n",
    "        end\n",
    "    end\n",
    "    -- Adding back the delta for the last one\n",
    "    rouguef1[epoch] = (rewards[n_s] + rewards[ n_s - 1] ):mean()\n",
    "    -- rouguef1[epoch] = rewards[n_s]:mean()\n",
    "\n",
    "    loss = {}\n",
    "    local dataloader = dl.TensorLoader({queryMemory, sentenceMemory, predSummaryMemory, \n",
    "                            qPredsMemory, qActionMemory, qValuesMemory}, rewardMemory)\n",
    "    c = 1\n",
    "    for k, xin, reward in dataloader:sampleiter(batch_size, memsize) do\n",
    "        local function feval(params)\n",
    "            gradParams:zero()\n",
    "            if adapt then\n",
    "                local ignore = model:forward({xin[1], xin[2], xin[3]})\n",
    "                local predQOnActions = maskLayer:forward({qPredsMemory, actions_in}) \n",
    "                local ones = torch.ones(predQ:size(1)):resize(predQ:size(1))\n",
    "                lossf = criterion:forward({qValuesMemory, predReg}, {reward, ones})\n",
    "                local gradOutput = criterion:backward({qActionMemory, predReg}, {reward, ones})\n",
    "                local gradMaskLayer = maskLayer:backward({qPredsMemory, qActionMemory}, gradOutput[1])\n",
    "                model:backward({xin[1], xin[2], xin[3]}, {gradMaskLayer[1], gradOutput[2]})\n",
    "            else \n",
    "                local ignore = model:forward({xin[1], xin[2], xin[3]})\n",
    "                local predQOnActions = maskLayer:forward({xin[4], xin[5]:byte()}) \n",
    "                lossf = criterion:forward(predQOnActions, reward)\n",
    "                local gradOutput = criterion:backward(predQOnActions, reward)\n",
    "                local gradMaskLayer = maskLayer:backward({xin[4], xin[5]:byte()}, gradOutput)\n",
    "                model:backward({xin[1], xin[2], xin[3]}, gradMaskLayer[1])\n",
    "            end \n",
    "            return lossf, gradParams\n",
    "        end\n",
    "        --- optim.rmsprop returns \\theta, f(\\theta):= loss function\n",
    "         _, lossv  = optim.rmsprop(feval, params, optimParams)\n",
    "        loss[c] = lossv[1]\n",
    "        c = c + 1\n",
    "    end\n",
    "\n",
    "    lossfull[epoch] = torch.Tensor(loss):sum() / #lossv\n",
    "    if print_perf then\n",
    "        print(\n",
    "            string.format('epoch = %i; rougue = %.6f; epsilon = %.6f; loss = %.6f' , \n",
    "                epoch, rouguef1[epoch], epsilon, lossfull[epoch])\n",
    "            )\n",
    "    end\n",
    "    epsilon = epsilon - (1/10.)\n",
    "    if epsilon < 0 then\n",
    "        epsilon = 0\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 879  302  675  489\n",
       "[torch.LongTensor of size 1x4]\n",
       "\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 10\n",
       "  2\n",
       "[torch.LongStorage of size 2]\n",
       "\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentenceMemory:size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0  0\n",
       "[torch.DoubleTensor of size 1x2]\n",
       "\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentenceMemory[{{n * (i-1) + 1, n * i}}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">\n",
       "$(function() {\n",
       "    if (typeof (window._bokeh_onload_callbacks) === \"undefined\"){\n",
       "  window._bokeh_onload_callbacks = [];\n",
       "    }\n",
       "    function load_lib(url, callback){\n",
       "  window._bokeh_onload_callbacks.push(callback);\n",
       "  if (window._bokeh_is_loading){\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", new Date());\n",
       "      return null;\n",
       "  }\n",
       "  console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", new Date());\n",
       "  window._bokeh_is_loading = true;\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = function(){\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh-0.7.0.min.css\");\n",
       "      window._bokeh_onload_callbacks.forEach(function(callback){callback()});\n",
       "  };\n",
       "  s.onerror = function(){\n",
       "      console.warn(\"failed to load library \" + url);\n",
       "  };\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "\n",
       "    bokehjs_url = \"https://cdn.pydata.org/bokeh-0.7.0.min.js\"\n",
       "\n",
       "    var elt = document.getElementById(\"3dd3c44c-19a3-495b-c93f-c480dbfd62f0\");\n",
       "    if(elt==null) {\n",
       "  console.log(\"Bokeh: ERROR: autoload.js configured with elementid '3dd3c44c-19a3-495b-c93f-c480dbfd62f0'\"\n",
       "        + \"but no matching script tag was found. \")\n",
       "  return false;\n",
       "    }\n",
       "\n",
       "    if(typeof(Bokeh) !== \"undefined\") {\n",
       "  console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "  var modelid = \"79655b13-5385-4dc0-cd9f-9c4590daca1a\";\n",
       "  var modeltype = \"Plot\";\n",
       "  var all_models = [{\"id\":\"49639db4-a51b-49a2-c935-bff03b70d736\",\"type\":\"ColumnDataSource\",\"attributes\":{\"data\":{\"y\":[0,0.33333333333333,0,0.33333333333333,0.33333333333333,0,0.33333333333333,0,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333],\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000]},\"column_names\":[\"y\",\"x\"],\"cont_ranges\":{},\"discrete_ranges\":{},\"selected\":[],\"id\":\"49639db4-a51b-49a2-c935-bff03b70d736\",\"doc\":null,\"tags\":[]}},{\"id\":\"ae0cac3b-e2e6-4604-c6ee-599419a12508\",\"type\":\"Line\",\"attributes\":{\"fill_alpha\":{\"units\":\"data\",\"value\":0.2},\"line_alpha\":{\"units\":\"data\",\"value\":1},\"doc\":null,\"size\":{\"units\":\"screen\",\"value\":10},\"fill_color\":{\"value\":\"blue\"},\"line_color\":{\"value\":\"blue\"},\"x\":{\"units\":\"data\",\"field\":\"x\"},\"id\":\"ae0cac3b-e2e6-4604-c6ee-599419a12508\",\"y\":{\"units\":\"data\",\"field\":\"y\"},\"tags\":[]}},{\"id\":\"68582311-abde-4789-c715-78a4d0c0d2cc\",\"type\":\"Line\",\"attributes\":{\"fill_alpha\":{\"units\":\"data\",\"value\":0.2},\"line_alpha\":{\"units\":\"data\",\"value\":1},\"doc\":null,\"size\":{\"units\":\"screen\",\"value\":10},\"fill_color\":{\"value\":\"blue\"},\"line_color\":{\"value\":\"blue\"},\"x\":{\"units\":\"data\",\"field\":\"x\"},\"id\":\"68582311-abde-4789-c715-78a4d0c0d2cc\",\"y\":{\"units\":\"data\",\"field\":\"y\"},\"tags\":[]}},{\"id\":\"54fb40ed-43a2-470e-c73d-0ef9f3665fbd\",\"type\":\"GlyphRenderer\",\"attributes\":{\"name\":null,\"nonselection_glyph\":{\"type\":\"Line\",\"id\":\"68582311-abde-4789-c715-78a4d0c0d2cc\"},\"doc\":null,\"server_data_source\":null,\"data_source\":{\"type\":\"ColumnDataSource\",\"id\":\"49639db4-a51b-49a2-c935-bff03b70d736\"},\"glyph\":{\"type\":\"Line\",\"id\":\"ae0cac3b-e2e6-4604-c6ee-599419a12508\"},\"selection_glyph\":null,\"id\":\"54fb40ed-43a2-470e-c73d-0ef9f3665fbd\",\"tags\":[]}},{\"id\":\"238a02b3-e1bd-462f-c61c-130de7b273f2\",\"type\":\"DataRange1d\",\"attributes\":{\"sources\":[{\"columns\":[\"x\"],\"source\":{\"type\":\"ColumnDataSource\",\"id\":\"49639db4-a51b-49a2-c935-bff03b70d736\"}}],\"id\":\"238a02b3-e1bd-462f-c61c-130de7b273f2\",\"tags\":[],\"doc\":null}},{\"id\":\"1f4bd097-4ffe-4fdd-cf4e-0f2c2a6b99f4\",\"type\":\"DataRange1d\",\"attributes\":{\"sources\":[{\"columns\":[\"y\"],\"source\":{\"type\":\"ColumnDataSource\",\"id\":\"49639db4-a51b-49a2-c935-bff03b70d736\"}}],\"id\":\"1f4bd097-4ffe-4fdd-cf4e-0f2c2a6b99f4\",\"tags\":[],\"doc\":null}},{\"id\":\"1f07709c-9180-46ad-c656-0a5c28549279\",\"type\":\"ToolEvents\",\"attributes\":{\"tags\":[],\"id\":\"1f07709c-9180-46ad-c656-0a5c28549279\",\"geometries\":[],\"doc\":null}},{\"id\":\"c9ad566e-7db4-47e1-c7e4-45f268caa4b7\",\"type\":\"BasicTickFormatter\",\"attributes\":{\"id\":\"c9ad566e-7db4-47e1-c7e4-45f268caa4b7\",\"tags\":[],\"doc\":null}},{\"id\":\"726d9651-26a7-4dbc-cde2-0196622ddb44\",\"type\":\"BasicTicker\",\"attributes\":{\"num_minor_ticks\":5,\"id\":\"726d9651-26a7-4dbc-cde2-0196622ddb44\",\"tags\":[],\"doc\":null}},{\"id\":\"86917684-608d-4f6b-cff3-146a5c5197c7\",\"type\":\"LinearAxis\",\"attributes\":{\"formatter\":{\"type\":\"BasicTickFormatter\",\"id\":\"c9ad566e-7db4-47e1-c7e4-45f268caa4b7\"},\"ticker\":{\"type\":\"BasicTicker\",\"id\":\"726d9651-26a7-4dbc-cde2-0196622ddb44\"},\"plot\":{\"id\":\"79655b13-5385-4dc0-cd9f-9c4590daca1a\",\"type\":\"Plot\",\"subtype\":\"Figure\"},\"axis_label\":null,\"id\":\"86917684-608d-4f6b-cff3-146a5c5197c7\",\"doc\":null,\"tags\":[]}},{\"id\":\"c63eef67-c46d-48fa-c885-6d2dba752924\",\"type\":\"Grid\",\"attributes\":{\"dimension\":0,\"plot\":{\"id\":\"79655b13-5385-4dc0-cd9f-9c4590daca1a\",\"type\":\"Plot\",\"subtype\":\"Figure\"},\"ticker\":{\"type\":\"BasicTicker\",\"id\":\"726d9651-26a7-4dbc-cde2-0196622ddb44\"},\"id\":\"c63eef67-c46d-48fa-c885-6d2dba752924\",\"doc\":null,\"tags\":[]}},{\"id\":\"fa53e8bb-63ea-4b35-cbbb-060d9521a3d0\",\"type\":\"BasicTickFormatter\",\"attributes\":{\"id\":\"fa53e8bb-63ea-4b35-cbbb-060d9521a3d0\",\"tags\":[],\"doc\":null}},{\"id\":\"26b910f7-7142-4da8-cdf7-b9d847846dc8\",\"type\":\"BasicTicker\",\"attributes\":{\"num_minor_ticks\":5,\"id\":\"26b910f7-7142-4da8-cdf7-b9d847846dc8\",\"tags\":[],\"doc\":null}},{\"id\":\"7e53a95b-08ef-4339-c32e-16902ef9f67f\",\"type\":\"LinearAxis\",\"attributes\":{\"formatter\":{\"type\":\"BasicTickFormatter\",\"id\":\"fa53e8bb-63ea-4b35-cbbb-060d9521a3d0\"},\"ticker\":{\"type\":\"BasicTicker\",\"id\":\"26b910f7-7142-4da8-cdf7-b9d847846dc8\"},\"plot\":{\"id\":\"79655b13-5385-4dc0-cd9f-9c4590daca1a\",\"type\":\"Plot\",\"subtype\":\"Figure\"},\"axis_label\":null,\"id\":\"7e53a95b-08ef-4339-c32e-16902ef9f67f\",\"doc\":null,\"tags\":[]}},{\"id\":\"7c1c3b2b-9737-4c82-cc53-e49f48d9a50e\",\"type\":\"Grid\",\"attributes\":{\"dimension\":1,\"plot\":{\"id\":\"79655b13-5385-4dc0-cd9f-9c4590daca1a\",\"type\":\"Plot\",\"subtype\":\"Figure\"},\"ticker\":{\"type\":\"BasicTicker\",\"id\":\"26b910f7-7142-4da8-cdf7-b9d847846dc8\"},\"id\":\"7c1c3b2b-9737-4c82-cc53-e49f48d9a50e\",\"doc\":null,\"tags\":[]}},{\"id\":\"9afa655f-a39a-4528-c531-0b258e1aed23\",\"type\":\"PanTool\",\"attributes\":{\"plot\":{\"id\":\"79655b13-5385-4dc0-cd9f-9c4590daca1a\",\"type\":\"Plot\",\"subtype\":\"Figure\"},\"dimensions\":[\"width\",\"height\"],\"id\":\"9afa655f-a39a-4528-c531-0b258e1aed23\",\"doc\":null,\"tags\":[]}},{\"id\":\"d0d4b845-1e1d-4bba-cbfe-d0a02a0f6a0b\",\"type\":\"WheelZoomTool\",\"attributes\":{\"plot\":{\"id\":\"79655b13-5385-4dc0-cd9f-9c4590daca1a\",\"type\":\"Plot\",\"subtype\":\"Figure\"},\"dimensions\":[\"width\",\"height\"],\"id\":\"d0d4b845-1e1d-4bba-cbfe-d0a02a0f6a0b\",\"doc\":null,\"tags\":[]}},{\"id\":\"b3fb7d0d-10e3-4e01-ce5f-102b01eb5a09\",\"type\":\"BoxZoomTool\",\"attributes\":{\"plot\":{\"id\":\"79655b13-5385-4dc0-cd9f-9c4590daca1a\",\"type\":\"Plot\",\"subtype\":\"Figure\"},\"id\":\"b3fb7d0d-10e3-4e01-ce5f-102b01eb5a09\",\"tags\":[],\"doc\":null}},{\"id\":\"170864b3-51c1-48bb-c8b5-822ee5016f7f\",\"type\":\"PreviewSaveTool\",\"attributes\":{\"plot\":{\"id\":\"79655b13-5385-4dc0-cd9f-9c4590daca1a\",\"type\":\"Plot\",\"subtype\":\"Figure\"},\"id\":\"170864b3-51c1-48bb-c8b5-822ee5016f7f\",\"tags\":[],\"doc\":null}},{\"id\":\"4687fd86-8de1-480a-c815-083767da76ca\",\"type\":\"ResizeTool\",\"attributes\":{\"plot\":{\"id\":\"79655b13-5385-4dc0-cd9f-9c4590daca1a\",\"type\":\"Plot\",\"subtype\":\"Figure\"},\"id\":\"4687fd86-8de1-480a-c815-083767da76ca\",\"tags\":[],\"doc\":null}},{\"id\":\"f4cb1888-616c-45a0-c59e-109620a30fc8\",\"type\":\"ResetTool\",\"attributes\":{\"plot\":{\"id\":\"79655b13-5385-4dc0-cd9f-9c4590daca1a\",\"type\":\"Plot\",\"subtype\":\"Figure\"},\"id\":\"f4cb1888-616c-45a0-c59e-109620a30fc8\",\"tags\":[],\"doc\":null}},{\"id\":\"79655b13-5385-4dc0-cd9f-9c4590daca1a\",\"type\":\"Plot\",\"attributes\":{\"x_range\":{\"type\":\"DataRange1d\",\"id\":\"238a02b3-e1bd-462f-c61c-130de7b273f2\"},\"tool_events\":{\"type\":\"ToolEvents\",\"id\":\"1f07709c-9180-46ad-c656-0a5c28549279\"},\"below\":[{\"type\":\"LinearAxis\",\"id\":\"86917684-608d-4f6b-cff3-146a5c5197c7\"}],\"renderers\":[{\"type\":\"GlyphRenderer\",\"id\":\"54fb40ed-43a2-470e-c73d-0ef9f3665fbd\"},{\"type\":\"LinearAxis\",\"id\":\"86917684-608d-4f6b-cff3-146a5c5197c7\"},{\"type\":\"Grid\",\"id\":\"c63eef67-c46d-48fa-c885-6d2dba752924\"},{\"type\":\"LinearAxis\",\"id\":\"7e53a95b-08ef-4339-c32e-16902ef9f67f\"},{\"type\":\"Grid\",\"id\":\"7c1c3b2b-9737-4c82-cc53-e49f48d9a50e\"}],\"above\":[],\"tools\":[{\"type\":\"PanTool\",\"id\":\"9afa655f-a39a-4528-c531-0b258e1aed23\"},{\"type\":\"WheelZoomTool\",\"id\":\"d0d4b845-1e1d-4bba-cbfe-d0a02a0f6a0b\"},{\"type\":\"BoxZoomTool\",\"id\":\"b3fb7d0d-10e3-4e01-ce5f-102b01eb5a09\"},{\"type\":\"PreviewSaveTool\",\"id\":\"170864b3-51c1-48bb-c8b5-822ee5016f7f\"},{\"type\":\"ResizeTool\",\"id\":\"4687fd86-8de1-480a-c815-083767da76ca\"},{\"type\":\"ResetTool\",\"id\":\"f4cb1888-616c-45a0-c59e-109620a30fc8\"}],\"doc\":null,\"right\":[],\"title\":\"Plot of Rougue-F1\",\"extra_x_ranges\":{},\"left\":[{\"type\":\"LinearAxis\",\"id\":\"7e53a95b-08ef-4339-c32e-16902ef9f67f\"}],\"y_range\":{\"type\":\"DataRange1d\",\"id\":\"1f4bd097-4ffe-4fdd-cf4e-0f2c2a6b99f4\"},\"id\":\"79655b13-5385-4dc0-cd9f-9c4590daca1a\",\"extra_y_ranges\":{},\"tags\":[]}}];\n",
       "  Bokeh.load_models(all_models);\n",
       "  var model = Bokeh.Collections(modeltype).get(modelid);\n",
       "  $(\"#3dd3c44c-19a3-495b-c93f-c480dbfd62f0\").html(''); // clear any previous plot in window_id\n",
       "  var view = new model.default_view({model: model, el: \"#3dd3c44c-19a3-495b-c93f-c480dbfd62f0\"});\n",
       "    } else {\n",
       "  load_lib(bokehjs_url, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", new Date())\n",
       "      var modelid = \"79655b13-5385-4dc0-cd9f-9c4590daca1a\";\n",
       "      var modeltype = \"Plot\";\n",
       "      var all_models = [{\"id\":\"49639db4-a51b-49a2-c935-bff03b70d736\",\"type\":\"ColumnDataSource\",\"attributes\":{\"data\":{\"y\":[0,0.33333333333333,0,0.33333333333333,0.33333333333333,0,0.33333333333333,0,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333,0.33333333333333],\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000]},\"column_names\":[\"y\",\"x\"],\"cont_ranges\":{},\"discrete_ranges\":{},\"selected\":[],\"id\":\"49639db4-a51b-49a2-c935-bff03b70d736\",\"doc\":null,\"tags\":[]}},{\"id\":\"ae0cac3b-e2e6-4604-c6ee-599419a12508\",\"type\":\"Line\",\"attributes\":{\"fill_alpha\":{\"units\":\"data\",\"value\":0.2},\"line_alpha\":{\"units\":\"data\",\"value\":1},\"doc\":null,\"size\":{\"units\":\"screen\",\"value\":10},\"fill_color\":{\"value\":\"blue\"},\"line_color\":{\"value\":\"blue\"},\"x\":{\"units\":\"data\",\"field\":\"x\"},\"id\":\"ae0cac3b-e2e6-4604-c6ee-599419a12508\",\"y\":{\"units\":\"data\",\"field\":\"y\"},\"tags\":[]}},{\"id\":\"68582311-abde-4789-c715-78a4d0c0d2cc\",\"type\":\"Line\",\"attributes\":{\"fill_alpha\":{\"units\":\"data\",\"value\":0.2},\"line_alpha\":{\"units\":\"data\",\"value\":1},\"doc\":null,\"size\":{\"units\":\"screen\",\"value\":10},\"fill_color\":{\"value\":\"blue\"},\"line_color\":{\"value\":\"blue\"},\"x\":{\"units\":\"data\",\"field\":\"x\"},\"id\":\"68582311-abde-4789-c715-78a4d0c0d2cc\",\"y\":{\"units\":\"data\",\"field\":\"y\"},\"tags\":[]}},{\"id\":\"54fb40ed-43a2-470e-c73d-0ef9f3665fbd\",\"type\":\"GlyphRenderer\",\"attributes\":{\"name\":null,\"nonselection_glyph\":{\"type\":\"Line\",\"id\":\"68582311-abde-4789-c715-78a4d0c0d2cc\"},\"doc\":null,\"server_data_source\":null,\"data_source\":{\"type\":\"ColumnDataSource\",\"id\":\"49639db4-a51b-49a2-c935-bff03b70d736\"},\"glyph\":{\"type\":\"Line\",\"id\":\"ae0cac3b-e2e6-4604-c6ee-599419a12508\"},\"selection_glyph\":null,\"id\":\"54fb40ed-43a2-470e-c73d-0ef9f3665fbd\",\"tags\":[]}},{\"id\":\"238a02b3-e1bd-462f-c61c-130de7b273f2\",\"type\":\"DataRange1d\",\"attributes\":{\"sources\":[{\"columns\":[\"x\"],\"source\":{\"type\":\"ColumnDataSource\",\"id\":\"49639db4-a51b-49a2-c935-bff03b70d736\"}}],\"id\":\"238a02b3-e1bd-462f-c61c-130de7b273f2\",\"tags\":[],\"doc\":null}},{\"id\":\"1f4bd097-4ffe-4fdd-cf4e-0f2c2a6b99f4\",\"type\":\"DataRange1d\",\"attributes\":{\"sources\":[{\"columns\":[\"y\"],\"source\":{\"type\":\"ColumnDataSource\",\"id\":\"49639db4-a51b-49a2-c935-bff03b70d736\"}}],\"id\":\"1f4bd097-4ffe-4fdd-cf4e-0f2c2a6b99f4\",\"tags\":[],\"doc\":null}},{\"id\":\"1f07709c-9180-46ad-c656-0a5c28549279\",\"type\":\"ToolEvents\",\"attributes\":{\"tags\":[],\"id\":\"1f07709c-9180-46ad-c656-0a5c28549279\",\"geometries\":[],\"doc\":null}},{\"id\":\"c9ad566e-7db4-47e1-c7e4-45f268caa4b7\",\"type\":\"BasicTickFormatter\",\"attributes\":{\"id\":\"c9ad566e-7db4-47e1-c7e4-45f268caa4b7\",\"tags\":[],\"doc\":null}},{\"id\":\"726d9651-26a7-4dbc-cde2-0196622ddb44\",\"type\":\"BasicTicker\",\"attributes\":{\"num_minor_ticks\":5,\"id\":\"726d9651-26a7-4dbc-cde2-0196622ddb44\",\"tags\":[],\"doc\":null}},{\"id\":\"86917684-608d-4f6b-cff3-146a5c5197c7\",\"type\":\"LinearAxis\",\"attributes\":{\"formatter\":{\"type\":\"BasicTickFormatter\",\"id\":\"c9ad566e-7db4-47e1-c7e4-45f268caa4b7\"},\"ticker\":{\"type\":\"BasicTicker\",\"id\":\"726d9651-26a7-4dbc-cde2-0196622ddb44\"},\"plot\":{\"id\":\"79655b13-5385-4dc0-cd9f-9c4590daca1a\",\"type\":\"Plot\",\"subtype\":\"Figure\"},\"axis_label\":null,\"id\":\"86917684-608d-4f6b-cff3-146a5c5197c7\",\"doc\":null,\"tags\":[]}},{\"id\":\"c63eef67-c46d-48fa-c885-6d2dba752924\",\"type\":\"Grid\",\"attributes\":{\"dimension\":0,\"plot\":{\"id\":\"79655b13-5385-4dc0-cd9f-9c4590daca1a\",\"type\":\"Plot\",\"subtype\":\"Figure\"},\"ticker\":{\"type\":\"BasicTicker\",\"id\":\"726d9651-26a7-4dbc-cde2-0196622ddb44\"},\"id\":\"c63eef67-c46d-48fa-c885-6d2dba752924\",\"doc\":null,\"tags\":[]}},{\"id\":\"fa53e8bb-63ea-4b35-cbbb-060d9521a3d0\",\"type\":\"BasicTickFormatter\",\"attributes\":{\"id\":\"fa53e8bb-63ea-4b35-cbbb-060d9521a3d0\",\"tags\":[],\"doc\":null}},{\"id\":\"26b910f7-7142-4da8-cdf7-b9d847846dc8\",\"type\":\"BasicTicker\",\"attributes\":{\"num_minor_ticks\":5,\"id\":\"26b910f7-7142-4da8-cdf7-b9d847846dc8\",\"tags\":[],\"doc\":null}},{\"id\":\"7e53a95b-08ef-4339-c32e-16902ef9f67f\",\"type\":\"LinearAxis\",\"attributes\":{\"formatter\":{\"type\":\"BasicTickFormatter\",\"id\":\"fa53e8bb-63ea-4b35-cbbb-060d9521a3d0\"},\"ticker\":{\"type\":\"BasicTicker\",\"id\":\"26b910f7-7142-4da8-cdf7-b9d847846dc8\"},\"plot\":{\"id\":\"79655b13-5385-4dc0-cd9f-9c4590daca1a\",\"type\":\"Plot\",\"subtype\":\"Figure\"},\"axis_label\":null,\"id\":\"7e53a95b-08ef-4339-c32e-16902ef9f67f\",\"doc\":null,\"tags\":[]}},{\"id\":\"7c1c3b2b-9737-4c82-cc53-e49f48d9a50e\",\"type\":\"Grid\",\"attributes\":{\"dimension\":1,\"plot\":{\"id\":\"79655b13-5385-4dc0-cd9f-9c4590daca1a\",\"type\":\"Plot\",\"subtype\":\"Figure\"},\"ticker\":{\"type\":\"BasicTicker\",\"id\":\"26b910f7-7142-4da8-cdf7-b9d847846dc8\"},\"id\":\"7c1c3b2b-9737-4c82-cc53-e49f48d9a50e\",\"doc\":null,\"tags\":[]}},{\"id\":\"9afa655f-a39a-4528-c531-0b258e1aed23\",\"type\":\"PanTool\",\"attributes\":{\"plot\":{\"id\":\"79655b13-5385-4dc0-cd9f-9c4590daca1a\",\"type\":\"Plot\",\"subtype\":\"Figure\"},\"dimensions\":[\"width\",\"height\"],\"id\":\"9afa655f-a39a-4528-c531-0b258e1aed23\",\"doc\":null,\"tags\":[]}},{\"id\":\"d0d4b845-1e1d-4bba-cbfe-d0a02a0f6a0b\",\"type\":\"WheelZoomTool\",\"attributes\":{\"plot\":{\"id\":\"79655b13-5385-4dc0-cd9f-9c4590daca1a\",\"type\":\"Plot\",\"subtype\":\"Figure\"},\"dimensions\":[\"width\",\"height\"],\"id\":\"d0d4b845-1e1d-4bba-cbfe-d0a02a0f6a0b\",\"doc\":null,\"tags\":[]}},{\"id\":\"b3fb7d0d-10e3-4e01-ce5f-102b01eb5a09\",\"type\":\"BoxZoomTool\",\"attributes\":{\"plot\":{\"id\":\"79655b13-5385-4dc0-cd9f-9c4590daca1a\",\"type\":\"Plot\",\"subtype\":\"Figure\"},\"id\":\"b3fb7d0d-10e3-4e01-ce5f-102b01eb5a09\",\"tags\":[],\"doc\":null}},{\"id\":\"170864b3-51c1-48bb-c8b5-822ee5016f7f\",\"type\":\"PreviewSaveTool\",\"attributes\":{\"plot\":{\"id\":\"79655b13-5385-4dc0-cd9f-9c4590daca1a\",\"type\":\"Plot\",\"subtype\":\"Figure\"},\"id\":\"170864b3-51c1-48bb-c8b5-822ee5016f7f\",\"tags\":[],\"doc\":null}},{\"id\":\"4687fd86-8de1-480a-c815-083767da76ca\",\"type\":\"ResizeTool\",\"attributes\":{\"plot\":{\"id\":\"79655b13-5385-4dc0-cd9f-9c4590daca1a\",\"type\":\"Plot\",\"subtype\":\"Figure\"},\"id\":\"4687fd86-8de1-480a-c815-083767da76ca\",\"tags\":[],\"doc\":null}},{\"id\":\"f4cb1888-616c-45a0-c59e-109620a30fc8\",\"type\":\"ResetTool\",\"attributes\":{\"plot\":{\"id\":\"79655b13-5385-4dc0-cd9f-9c4590daca1a\",\"type\":\"Plot\",\"subtype\":\"Figure\"},\"id\":\"f4cb1888-616c-45a0-c59e-109620a30fc8\",\"tags\":[],\"doc\":null}},{\"id\":\"79655b13-5385-4dc0-cd9f-9c4590daca1a\",\"type\":\"Plot\",\"attributes\":{\"x_range\":{\"type\":\"DataRange1d\",\"id\":\"238a02b3-e1bd-462f-c61c-130de7b273f2\"},\"tool_events\":{\"type\":\"ToolEvents\",\"id\":\"1f07709c-9180-46ad-c656-0a5c28549279\"},\"below\":[{\"type\":\"LinearAxis\",\"id\":\"86917684-608d-4f6b-cff3-146a5c5197c7\"}],\"renderers\":[{\"type\":\"GlyphRenderer\",\"id\":\"54fb40ed-43a2-470e-c73d-0ef9f3665fbd\"},{\"type\":\"LinearAxis\",\"id\":\"86917684-608d-4f6b-cff3-146a5c5197c7\"},{\"type\":\"Grid\",\"id\":\"c63eef67-c46d-48fa-c885-6d2dba752924\"},{\"type\":\"LinearAxis\",\"id\":\"7e53a95b-08ef-4339-c32e-16902ef9f67f\"},{\"type\":\"Grid\",\"id\":\"7c1c3b2b-9737-4c82-cc53-e49f48d9a50e\"}],\"above\":[],\"tools\":[{\"type\":\"PanTool\",\"id\":\"9afa655f-a39a-4528-c531-0b258e1aed23\"},{\"type\":\"WheelZoomTool\",\"id\":\"d0d4b845-1e1d-4bba-cbfe-d0a02a0f6a0b\"},{\"type\":\"BoxZoomTool\",\"id\":\"b3fb7d0d-10e3-4e01-ce5f-102b01eb5a09\"},{\"type\":\"PreviewSaveTool\",\"id\":\"170864b3-51c1-48bb-c8b5-822ee5016f7f\"},{\"type\":\"ResizeTool\",\"id\":\"4687fd86-8de1-480a-c815-083767da76ca\"},{\"type\":\"ResetTool\",\"id\":\"f4cb1888-616c-45a0-c59e-109620a30fc8\"}],\"doc\":null,\"right\":[],\"title\":\"Plot of Rougue-F1\",\"extra_x_ranges\":{},\"left\":[{\"type\":\"LinearAxis\",\"id\":\"7e53a95b-08ef-4339-c32e-16902ef9f67f\"}],\"y_range\":{\"type\":\"DataRange1d\",\"id\":\"1f4bd097-4ffe-4fdd-cf4e-0f2c2a6b99f4\"},\"id\":\"79655b13-5385-4dc0-cd9f-9c4590daca1a\",\"extra_y_ranges\":{},\"tags\":[]}}];\n",
       "      Bokeh.load_models(all_models);\n",
       "      var model = Bokeh.Collections(modeltype).get(modelid);\n",
       "      $(\"#3dd3c44c-19a3-495b-c93f-c480dbfd62f0\").html(''); // clear any previous plot in window_id\n",
       "      var view = new model.default_view({model: model, el: \"#3dd3c44c-19a3-495b-c93f-c480dbfd62f0\"});\n",
       "  });\n",
       "    }\n",
       "});\n",
       "</script>\n",
       "<div class=\"plotdiv\" id=\"3dd3c44c-19a3-495b-c93f-c480dbfd62f0\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Plot = require 'itorch.Plot'\n",
    "\n",
    "loss = torch.Tensor(lossfull)\n",
    "rougue = torch.Tensor(rouguef1)\n",
    "indices = torch.linspace(1, loss:size(1), loss:size(1)):long() \n",
    "-- plot = Plot():line(indices, loss, 'red', 'hi'):title('Plot of loss'):draw()\n",
    "plot = Plot():line(indices, rougue, 'blue', 'hi'):title('Plot of Rougue-F1'):draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  53 : 1\n",
       "  278 : 1\n",
       "  871 : 1\n",
       "  997 : 1\n",
       "  489 : 1\n",
       "  879 : 1\n",
       "  288 : 1\n",
       "  302 : 1\n",
       "  675 : 1\n",
       "  796 : 1\n",
       "  147 : 1\n",
       "  161 : 1\n",
       "  859 : 1\n",
       "  269 : 1\n",
       "  650 : 1\n",
       "  826 : 1\n",
       "  130 : 1\n",
       "  528 : 1\n",
       "  806 : 1\n",
       "  701 : 1\n",
       "}\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tokenize(trueSummary:resize(40):totable())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Columns 1 to 16\n",
       " 859   53  871  161    0    0    0    0    0    0    0    0    0    0    0    0\n",
       "\n",
       "Columns 17 to 32\n",
       "   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
       "\n",
       "Columns 33 to 40\n",
       "   0    0    0    0    0    0    0    0\n",
       "[torch.LongTensor of size 1x40]\n",
       "\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalPredsummary[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  130 : 1\n",
       "  269 : 1\n",
       "  288 : 1\n",
       "  826 : 1\n",
       "}\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tokenize(totalPredsummary[n_s]:totable())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  130 : 1\n",
       "  269 : 1\n",
       "  288 : 1\n",
       "  826 : 1\n",
       "}\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tokenize(totalPredsummary[n_s]:resize(40):totable())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SELECT, SKIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0  1\n",
       "[torch.DoubleTensor of size 1x2]\n",
       "\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       " 0  1\n",
       "[torch.DoubleTensor of size 1x2]\n",
       "\n",
       " 0  1\n",
       "[torch.DoubleTensor of size 1x2]\n",
       "\n",
       " 1  0\n",
       "[torch.DoubleTensor of size 1x2]\n",
       "\n",
       " 1  0\n",
       "[torch.DoubleTensor of size 1x2]\n",
       "\n",
       " 1  0\n",
       "[torch.DoubleTensor of size 1x2]\n",
       "\n",
       " 1  0\n",
       "[torch.DoubleTensor of size 1x2]\n",
       "\n",
       " 1  0\n",
       "[torch.DoubleTensor of size 1x2]\n",
       "\n",
       " 0  1\n",
       "[torch.DoubleTensor of size 1x2]\n",
       "\n",
       " 0  1\n",
       "[torch.DoubleTensor of size 1x2]\n",
       "\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.unpack(true_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0  1\n",
       "[torch.DoubleTensor of size 1x2]\n",
       "\n",
       " 0  1\n",
       "[torch.DoubleTensor of size 1x2]\n",
       "\n",
       " 0  1\n",
       "[torch.DoubleTensor of size 1x2]\n",
       "\n",
       " 0  1\n",
       "[torch.DoubleTensor of size 1x2]\n",
       "\n",
       " 0  1\n",
       "[torch.DoubleTensor of size 1x2]\n",
       "\n",
       " 0  1\n",
       "[torch.DoubleTensor of size 1x2]\n",
       "\n",
       " 0  1\n",
       "[torch.DoubleTensor of size 1x2]\n",
       "\n",
       " 0  1\n",
       "[torch.DoubleTensor of size 1x2]\n",
       "\n",
       " 0  1\n",
       "[torch.DoubleTensor of size 1x2]\n",
       "\n",
       " 0  1\n",
       "[torch.DoubleTensor of size 1x2]\n",
       "\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.unpack(qActions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
