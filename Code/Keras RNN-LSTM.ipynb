{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import scipy.io\n",
    "import stat\n",
    "import subprocess\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = '/Users/franciscojavierarceo/GitHub/ECBME6040/e6040_hw4_fja2114/data/atis.fold3.pkl.gz'\n",
    "f = gzip.open(filename, 'rb')\n",
    "train_set, valid_set, test_set, dic = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx2label = dict((k, v) for v, k in dic['labels2idx'].items())\n",
    "idx2word = dict((k, v) for v, k in dic['words2idx'].items())\n",
    "\n",
    "# unpack dataset\n",
    "train_lex, train_ne, train_y = train_set\n",
    "valid_lex, valid_ne, valid_y = valid_set\n",
    "test_lex, test_ne, test_y = test_set\n",
    "\n",
    "vocsize = len(dic['words2idx'])\n",
    "nclasses = len(dic['labels2idx'])\n",
    "nsequences = len(train_lex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sumy = 0\n",
    "sumn = 0\n",
    "for i, (x, y) in enumerate(zip(train_lex, train_y)):\n",
    "    if len(x)==len(y):\n",
    "        sumy+=1\n",
    "    else:\n",
    "        sumn+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3983, 0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumy, sumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(572, 127, 3983)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocsize, nclasses, nsequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(554, 'what'),\n",
       " (23, 'aircraft'),\n",
       " (241, 'is'),\n",
       " (534, 'used'),\n",
       " (358, 'on'),\n",
       " (136, 'delta'),\n",
       " (193, 'flight'),\n",
       " (11, 'DIGITDIGITDIGITDIGIT'),\n",
       " (208, 'from'),\n",
       " (251, 'kansas'),\n",
       " (104, 'city'),\n",
       " (502, 'to'),\n",
       " (413, 'salt'),\n",
       " (256, 'lake'),\n",
       " (104, 'city')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(w, idx2word[w]) for w in train_lex[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# verifying this\n",
    "for j in range(len(train_ne)):\n",
    "    [(w, idx2word[w]) for w in train_ne[j]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, \"'d\"),\n",
       " (0, \"'d\"),\n",
       " (0, \"'d\"),\n",
       " (0, \"'d\"),\n",
       " (0, \"'d\"),\n",
       " (6, '72s'),\n",
       " (0, \"'d\"),\n",
       " (52, 'arizona'),\n",
       " (0, \"'d\"),\n",
       " (18, 'actually'),\n",
       " (109, 'co'),\n",
       " (0, \"'d\"),\n",
       " (18, 'actually'),\n",
       " (109, 'co'),\n",
       " (109, 'co')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(w, idx2word[w]) for w in train_ne[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(126, 'O'),\n",
       " (126, 'O'),\n",
       " (126, 'O'),\n",
       " (126, 'O'),\n",
       " (126, 'O'),\n",
       " (2, 'B-airline_name'),\n",
       " (126, 'O'),\n",
       " (43, 'B-flight_number'),\n",
       " (126, 'O'),\n",
       " (48, 'B-fromloc.city_name'),\n",
       " (109, 'I-fromloc.city_name'),\n",
       " (126, 'O'),\n",
       " (78, 'B-toloc.city_name'),\n",
       " (123, 'I-toloc.city_name'),\n",
       " (123, 'I-toloc.city_name')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(w, idx2label[w]) for w in train_y[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'B-aircraft_code',\n",
       " 1: 'B-airline_code',\n",
       " 2: 'B-airline_name',\n",
       " 3: 'B-airport_code',\n",
       " 4: 'B-airport_name',\n",
       " 5: 'B-arrive_date.date_relative',\n",
       " 6: 'B-arrive_date.day_name',\n",
       " 7: 'B-arrive_date.day_number',\n",
       " 8: 'B-arrive_date.month_name',\n",
       " 9: 'B-arrive_date.today_relative',\n",
       " 10: 'B-arrive_time.end_time',\n",
       " 11: 'B-arrive_time.period_mod',\n",
       " 12: 'B-arrive_time.period_of_day',\n",
       " 13: 'B-arrive_time.start_time',\n",
       " 14: 'B-arrive_time.time',\n",
       " 15: 'B-arrive_time.time_relative',\n",
       " 16: 'B-booking_class',\n",
       " 17: 'B-city_name',\n",
       " 18: 'B-class_type',\n",
       " 19: 'B-compartment',\n",
       " 20: 'B-connect',\n",
       " 21: 'B-cost_relative',\n",
       " 22: 'B-day_name',\n",
       " 23: 'B-day_number',\n",
       " 24: 'B-days_code',\n",
       " 25: 'B-depart_date.date_relative',\n",
       " 26: 'B-depart_date.day_name',\n",
       " 27: 'B-depart_date.day_number',\n",
       " 28: 'B-depart_date.month_name',\n",
       " 29: 'B-depart_date.today_relative',\n",
       " 30: 'B-depart_date.year',\n",
       " 31: 'B-depart_time.end_time',\n",
       " 32: 'B-depart_time.period_mod',\n",
       " 33: 'B-depart_time.period_of_day',\n",
       " 34: 'B-depart_time.start_time',\n",
       " 35: 'B-depart_time.time',\n",
       " 36: 'B-depart_time.time_relative',\n",
       " 37: 'B-economy',\n",
       " 38: 'B-fare_amount',\n",
       " 39: 'B-fare_basis_code',\n",
       " 40: 'B-flight',\n",
       " 41: 'B-flight_days',\n",
       " 42: 'B-flight_mod',\n",
       " 43: 'B-flight_number',\n",
       " 44: 'B-flight_stop',\n",
       " 45: 'B-flight_time',\n",
       " 46: 'B-fromloc.airport_code',\n",
       " 47: 'B-fromloc.airport_name',\n",
       " 48: 'B-fromloc.city_name',\n",
       " 49: 'B-fromloc.state_code',\n",
       " 50: 'B-fromloc.state_name',\n",
       " 51: 'B-meal',\n",
       " 52: 'B-meal_code',\n",
       " 53: 'B-meal_description',\n",
       " 54: 'B-mod',\n",
       " 55: 'B-month_name',\n",
       " 56: 'B-or',\n",
       " 57: 'B-period_of_day',\n",
       " 58: 'B-restriction_code',\n",
       " 59: 'B-return_date.date_relative',\n",
       " 60: 'B-return_date.day_name',\n",
       " 61: 'B-return_date.day_number',\n",
       " 62: 'B-return_date.month_name',\n",
       " 63: 'B-return_date.today_relative',\n",
       " 64: 'B-return_time.period_mod',\n",
       " 65: 'B-return_time.period_of_day',\n",
       " 66: 'B-round_trip',\n",
       " 67: 'B-state_code',\n",
       " 68: 'B-state_name',\n",
       " 69: 'B-stoploc.airport_code',\n",
       " 70: 'B-stoploc.airport_name',\n",
       " 71: 'B-stoploc.city_name',\n",
       " 72: 'B-stoploc.state_code',\n",
       " 73: 'B-time',\n",
       " 74: 'B-time_relative',\n",
       " 75: 'B-today_relative',\n",
       " 76: 'B-toloc.airport_code',\n",
       " 77: 'B-toloc.airport_name',\n",
       " 78: 'B-toloc.city_name',\n",
       " 79: 'B-toloc.country_name',\n",
       " 80: 'B-toloc.state_code',\n",
       " 81: 'B-toloc.state_name',\n",
       " 82: 'B-transport_type',\n",
       " 83: 'I-airline_name',\n",
       " 84: 'I-airport_name',\n",
       " 85: 'I-arrive_date.day_number',\n",
       " 86: 'I-arrive_time.end_time',\n",
       " 87: 'I-arrive_time.period_of_day',\n",
       " 88: 'I-arrive_time.start_time',\n",
       " 89: 'I-arrive_time.time',\n",
       " 90: 'I-arrive_time.time_relative',\n",
       " 91: 'I-city_name',\n",
       " 92: 'I-class_type',\n",
       " 93: 'I-cost_relative',\n",
       " 94: 'I-depart_date.day_number',\n",
       " 95: 'I-depart_date.today_relative',\n",
       " 96: 'I-depart_time.end_time',\n",
       " 97: 'I-depart_time.period_of_day',\n",
       " 98: 'I-depart_time.start_time',\n",
       " 99: 'I-depart_time.time',\n",
       " 100: 'I-depart_time.time_relative',\n",
       " 101: 'I-economy',\n",
       " 102: 'I-fare_amount',\n",
       " 103: 'I-fare_basis_code',\n",
       " 104: 'I-flight_mod',\n",
       " 105: 'I-flight_number',\n",
       " 106: 'I-flight_stop',\n",
       " 107: 'I-flight_time',\n",
       " 108: 'I-fromloc.airport_name',\n",
       " 109: 'I-fromloc.city_name',\n",
       " 110: 'I-fromloc.state_name',\n",
       " 111: 'I-meal_code',\n",
       " 112: 'I-meal_description',\n",
       " 113: 'I-restriction_code',\n",
       " 114: 'I-return_date.date_relative',\n",
       " 115: 'I-return_date.day_number',\n",
       " 116: 'I-return_date.today_relative',\n",
       " 117: 'I-round_trip',\n",
       " 118: 'I-state_name',\n",
       " 119: 'I-stoploc.city_name',\n",
       " 120: 'I-time',\n",
       " 121: 'I-today_relative',\n",
       " 122: 'I-toloc.airport_name',\n",
       " 123: 'I-toloc.city_name',\n",
       " 124: 'I-toloc.state_name',\n",
       " 125: 'I-transport_type',\n",
       " 126: 'O'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: \"'d\",\n",
       " 1: \"'ll\",\n",
       " 2: \"'m\",\n",
       " 3: \"'re\",\n",
       " 4: \"'s\",\n",
       " 5: \"'t\",\n",
       " 6: '72s',\n",
       " 7: '<UNK>',\n",
       " 8: 'DIGIT',\n",
       " 9: 'DIGITDIGIT',\n",
       " 10: 'DIGITDIGITDIGIT',\n",
       " 11: 'DIGITDIGITDIGITDIGIT',\n",
       " 12: 'DIGITDIGITDIGITDIGITDIGITDIGIT',\n",
       " 13: 'a',\n",
       " 14: 'abbreviation',\n",
       " 15: 'abbreviations',\n",
       " 16: 'about',\n",
       " 17: 'ac',\n",
       " 18: 'actually',\n",
       " 19: 'after',\n",
       " 20: 'afternoon',\n",
       " 21: 'again',\n",
       " 22: 'air',\n",
       " 23: 'aircraft',\n",
       " 24: 'airfare',\n",
       " 25: 'airline',\n",
       " 26: 'airlines',\n",
       " 27: 'airplane',\n",
       " 28: 'airplanes',\n",
       " 29: 'airport',\n",
       " 30: 'airports',\n",
       " 31: 'alaska',\n",
       " 32: 'all',\n",
       " 33: 'along',\n",
       " 34: 'also',\n",
       " 35: 'am',\n",
       " 36: 'america',\n",
       " 37: 'american',\n",
       " 38: 'amount',\n",
       " 39: 'an',\n",
       " 40: 'and',\n",
       " 41: 'angeles',\n",
       " 42: 'another',\n",
       " 43: 'any',\n",
       " 44: 'anywhere',\n",
       " 45: 'ap',\n",
       " 46: 'ap57',\n",
       " 47: 'ap80',\n",
       " 48: 'approximately',\n",
       " 49: 'april',\n",
       " 50: 'are',\n",
       " 51: 'area',\n",
       " 52: 'arizona',\n",
       " 53: 'around',\n",
       " 54: 'arrange',\n",
       " 55: 'arrangements',\n",
       " 56: 'arrival',\n",
       " 57: 'arrivals',\n",
       " 58: 'arrive',\n",
       " 59: 'arrives',\n",
       " 60: 'arriving',\n",
       " 61: 'as',\n",
       " 62: 'at',\n",
       " 63: 'atl',\n",
       " 64: 'atlanta',\n",
       " 65: 'august',\n",
       " 66: 'available',\n",
       " 67: 'b',\n",
       " 68: 'back',\n",
       " 69: 'baltimore',\n",
       " 70: 'be',\n",
       " 71: 'beach',\n",
       " 72: 'before',\n",
       " 73: 'between',\n",
       " 74: 'boeing',\n",
       " 75: 'book',\n",
       " 76: 'booking',\n",
       " 77: 'boston',\n",
       " 78: 'both',\n",
       " 79: 'bound',\n",
       " 80: 'breakfast',\n",
       " 81: 'burbank',\n",
       " 82: 'business',\n",
       " 83: 'but',\n",
       " 84: 'buy',\n",
       " 85: 'bwi',\n",
       " 86: 'by',\n",
       " 87: 'c',\n",
       " 88: 'california',\n",
       " 89: 'can',\n",
       " 90: 'canada',\n",
       " 91: 'canadian',\n",
       " 92: 'capacity',\n",
       " 93: 'car',\n",
       " 94: 'carolina',\n",
       " 95: 'carries',\n",
       " 96: 'cars',\n",
       " 97: 'charlotte',\n",
       " 98: 'cheap',\n",
       " 99: 'cheapest',\n",
       " 100: 'chicago',\n",
       " 101: 'choices',\n",
       " 102: 'cincinnati',\n",
       " 103: 'cities',\n",
       " 104: 'city',\n",
       " 105: 'class',\n",
       " 106: 'classes',\n",
       " 107: 'cleveland',\n",
       " 108: 'close',\n",
       " 109: 'co',\n",
       " 110: 'coach',\n",
       " 111: 'code',\n",
       " 112: 'codes',\n",
       " 113: 'colorado',\n",
       " 114: 'columbus',\n",
       " 115: 'coming',\n",
       " 116: 'connect',\n",
       " 117: 'connecting',\n",
       " 118: 'connection',\n",
       " 119: 'connections',\n",
       " 120: 'continental',\n",
       " 121: 'cost',\n",
       " 122: 'costs',\n",
       " 123: 'could',\n",
       " 124: 'county',\n",
       " 125: 'cp',\n",
       " 126: 'd',\n",
       " 127: 'daily',\n",
       " 128: 'dallas',\n",
       " 129: 'database',\n",
       " 130: 'day',\n",
       " 131: 'days',\n",
       " 132: 'dc',\n",
       " 133: 'dc10',\n",
       " 134: 'december',\n",
       " 135: 'define',\n",
       " 136: 'delta',\n",
       " 137: 'denver',\n",
       " 138: 'depart',\n",
       " 139: 'departing',\n",
       " 140: 'departs',\n",
       " 141: 'departure',\n",
       " 142: 'departures',\n",
       " 143: 'describe',\n",
       " 144: 'destination',\n",
       " 145: 'detroit',\n",
       " 146: 'dfw',\n",
       " 147: 'diego',\n",
       " 148: 'difference',\n",
       " 149: 'different',\n",
       " 150: 'dinner',\n",
       " 151: 'dinnertime',\n",
       " 152: 'direct',\n",
       " 153: 'display',\n",
       " 154: 'distance',\n",
       " 155: 'dl',\n",
       " 156: 'do',\n",
       " 157: 'does',\n",
       " 158: 'dollars',\n",
       " 159: 'downtown',\n",
       " 160: 'dulles',\n",
       " 161: 'during',\n",
       " 162: 'ea',\n",
       " 163: 'each',\n",
       " 164: 'earlier',\n",
       " 165: 'earliest',\n",
       " 166: 'early',\n",
       " 167: 'eastern',\n",
       " 168: 'economy',\n",
       " 169: 'eight',\n",
       " 170: 'eighteenth',\n",
       " 171: 'eighth',\n",
       " 172: 'either',\n",
       " 173: 'eleventh',\n",
       " 174: 'evening',\n",
       " 175: 'ewr',\n",
       " 176: 'expensive',\n",
       " 177: 'explain',\n",
       " 178: 'express',\n",
       " 179: 'f',\n",
       " 180: 'f28',\n",
       " 181: 'far',\n",
       " 182: 'fare',\n",
       " 183: 'fares',\n",
       " 184: 'february',\n",
       " 185: 'ff',\n",
       " 186: 'field',\n",
       " 187: 'fifteenth',\n",
       " 188: 'fifth',\n",
       " 189: 'find',\n",
       " 190: 'first',\n",
       " 191: 'fit',\n",
       " 192: 'flies',\n",
       " 193: 'flight',\n",
       " 194: 'flights',\n",
       " 195: 'florida',\n",
       " 196: 'fly',\n",
       " 197: 'flying',\n",
       " 198: 'fn',\n",
       " 199: 'following',\n",
       " 200: 'for',\n",
       " 201: 'fort',\n",
       " 202: 'four',\n",
       " 203: 'fourteenth',\n",
       " 204: 'fourth',\n",
       " 205: 'francisco',\n",
       " 206: 'friday',\n",
       " 207: 'friends',\n",
       " 208: 'from',\n",
       " 209: 'general',\n",
       " 210: 'georgia',\n",
       " 211: 'get',\n",
       " 212: 'give',\n",
       " 213: 'go',\n",
       " 214: 'goes',\n",
       " 215: 'going',\n",
       " 216: 'great',\n",
       " 217: 'ground',\n",
       " 218: 'guardia',\n",
       " 219: 'h',\n",
       " 220: 'has',\n",
       " 221: 'have',\n",
       " 222: 'heading',\n",
       " 223: 'hello',\n",
       " 224: 'help',\n",
       " 225: 'here',\n",
       " 226: 'hi',\n",
       " 227: 'highest',\n",
       " 228: 'hours',\n",
       " 229: 'houston',\n",
       " 230: 'how',\n",
       " 231: 'hp',\n",
       " 232: 'i',\n",
       " 233: 'if',\n",
       " 234: 'in',\n",
       " 235: 'include',\n",
       " 236: 'indianapolis',\n",
       " 237: 'information',\n",
       " 238: 'interested',\n",
       " 239: 'international',\n",
       " 240: 'into',\n",
       " 241: 'is',\n",
       " 242: 'it',\n",
       " 243: 'itinerary',\n",
       " 244: 'january',\n",
       " 245: 'jersey',\n",
       " 246: 'jfk',\n",
       " 247: 'jose',\n",
       " 248: 'july',\n",
       " 249: 'june',\n",
       " 250: 'just',\n",
       " 251: 'kansas',\n",
       " 252: 'kind',\n",
       " 253: 'kinds',\n",
       " 254: 'know',\n",
       " 255: 'la',\n",
       " 256: 'lake',\n",
       " 257: 'land',\n",
       " 258: 'landing',\n",
       " 259: 'landings',\n",
       " 260: 'las',\n",
       " 261: 'last',\n",
       " 262: 'lastest',\n",
       " 263: 'late',\n",
       " 264: 'later',\n",
       " 265: 'latest',\n",
       " 266: 'layover',\n",
       " 267: 'least',\n",
       " 268: 'leave',\n",
       " 269: 'leaves',\n",
       " 270: 'leaving',\n",
       " 271: 'less',\n",
       " 272: 'let',\n",
       " 273: 'like',\n",
       " 274: 'limo',\n",
       " 275: 'limousine',\n",
       " 276: 'list',\n",
       " 277: 'listing',\n",
       " 278: 'live',\n",
       " 279: 'lives',\n",
       " 280: 'located',\n",
       " 281: 'logan',\n",
       " 282: 'long',\n",
       " 283: 'look',\n",
       " 284: 'looking',\n",
       " 285: 'los',\n",
       " 286: 'louis',\n",
       " 287: 'love',\n",
       " 288: 'lowest',\n",
       " 289: 'lufthansa',\n",
       " 290: 'lunch',\n",
       " 291: 'm',\n",
       " 292: 'm80',\n",
       " 293: 'make',\n",
       " 294: 'makes',\n",
       " 295: 'making',\n",
       " 296: 'many',\n",
       " 297: 'march',\n",
       " 298: 'maximum',\n",
       " 299: 'may',\n",
       " 300: 'mco',\n",
       " 301: 'me',\n",
       " 302: 'meal',\n",
       " 303: 'meals',\n",
       " 304: 'mean',\n",
       " 305: 'meaning',\n",
       " 306: 'memphis',\n",
       " 307: 'miami',\n",
       " 308: 'michigan',\n",
       " 309: 'midnight',\n",
       " 310: 'midway',\n",
       " 311: 'midwest',\n",
       " 312: 'milwaukee',\n",
       " 313: 'minneapolis',\n",
       " 314: 'minnesota',\n",
       " 315: 'missouri',\n",
       " 316: 'mitchell',\n",
       " 317: 'monday',\n",
       " 318: 'month',\n",
       " 319: 'montreal',\n",
       " 320: 'more',\n",
       " 321: 'morning',\n",
       " 322: 'mornings',\n",
       " 323: 'most',\n",
       " 324: 'much',\n",
       " 325: 'my',\n",
       " 326: 'name',\n",
       " 327: 'names',\n",
       " 328: 'nashville',\n",
       " 329: 'nationair',\n",
       " 330: 'near',\n",
       " 331: 'need',\n",
       " 332: 'new',\n",
       " 333: 'newark',\n",
       " 334: 'next',\n",
       " 335: 'night',\n",
       " 336: 'nineteenth',\n",
       " 337: 'ninth',\n",
       " 338: 'no',\n",
       " 339: 'nonstop',\n",
       " 340: 'nonstops',\n",
       " 341: 'noon',\n",
       " 342: 'noontime',\n",
       " 343: 'north',\n",
       " 344: 'northwest',\n",
       " 345: 'november',\n",
       " 346: 'now',\n",
       " 347: 'number',\n",
       " 348: 'numbers',\n",
       " 349: 'nw',\n",
       " 350: \"o'clock\",\n",
       " 351: 'oakland',\n",
       " 352: 'october',\n",
       " 353: 'of',\n",
       " 354: 'offer',\n",
       " 355: 'offers',\n",
       " 356: 'ohio',\n",
       " 357: 'okay',\n",
       " 358: 'on',\n",
       " 359: 'one',\n",
       " 360: 'only',\n",
       " 361: 'ontario',\n",
       " 362: 'options',\n",
       " 363: 'or',\n",
       " 364: 'ord',\n",
       " 365: 'originate',\n",
       " 366: 'originating',\n",
       " 367: 'orlando',\n",
       " 368: 'other',\n",
       " 369: 'out',\n",
       " 370: 'over',\n",
       " 371: 'passengers',\n",
       " 372: 'paul',\n",
       " 373: 'pennsylvania',\n",
       " 374: 'people',\n",
       " 375: 'petersburg',\n",
       " 376: 'philadelphia',\n",
       " 377: 'philly',\n",
       " 378: 'phoenix',\n",
       " 379: 'pittsburgh',\n",
       " 380: 'plan',\n",
       " 381: 'plane',\n",
       " 382: 'planes',\n",
       " 383: 'please',\n",
       " 384: 'pm',\n",
       " 385: 'possible',\n",
       " 386: 'price',\n",
       " 387: 'prices',\n",
       " 388: 'provide',\n",
       " 389: 'provided',\n",
       " 390: 'q',\n",
       " 391: 'qo',\n",
       " 392: 'quebec',\n",
       " 393: 'qw',\n",
       " 394: 'qx',\n",
       " 395: 'rate',\n",
       " 396: 'rates',\n",
       " 397: 'reaching',\n",
       " 398: 'rent',\n",
       " 399: 'rental',\n",
       " 400: 'rentals',\n",
       " 401: 'repeat',\n",
       " 402: 'requesting',\n",
       " 403: 'reservation',\n",
       " 404: 'reservations',\n",
       " 405: 'restriction',\n",
       " 406: 'restrictions',\n",
       " 407: 'return',\n",
       " 408: 'returning',\n",
       " 409: 'right',\n",
       " 410: 'round',\n",
       " 411: 's',\n",
       " 412: 'sa',\n",
       " 413: 'salt',\n",
       " 414: 'same',\n",
       " 415: 'san',\n",
       " 416: 'saturday',\n",
       " 417: 'saturdays',\n",
       " 418: 'say',\n",
       " 419: 'schedule',\n",
       " 420: 'schedules',\n",
       " 421: 'seating',\n",
       " 422: 'seats',\n",
       " 423: 'seattle',\n",
       " 424: 'second',\n",
       " 425: 'see',\n",
       " 426: 'september',\n",
       " 427: 'serve',\n",
       " 428: 'served',\n",
       " 429: 'serves',\n",
       " 430: 'service',\n",
       " 431: 'serviced',\n",
       " 432: 'services',\n",
       " 433: 'serving',\n",
       " 434: 'seventeenth',\n",
       " 435: 'seventh',\n",
       " 436: 'sfo',\n",
       " 437: 'shortest',\n",
       " 438: 'should',\n",
       " 439: 'show',\n",
       " 440: 'six',\n",
       " 441: 'sixteen',\n",
       " 442: 'sixteenth',\n",
       " 443: 'sixth',\n",
       " 444: 'smallest',\n",
       " 445: 'snack',\n",
       " 446: 'so',\n",
       " 447: 'some',\n",
       " 448: 'sometime',\n",
       " 449: 'soon',\n",
       " 450: 'sorry',\n",
       " 451: 'southwest',\n",
       " 452: 'st.',\n",
       " 453: 'stand',\n",
       " 454: 'stands',\n",
       " 455: 'stapleton',\n",
       " 456: 'starting',\n",
       " 457: 'still',\n",
       " 458: 'stop',\n",
       " 459: 'stopover',\n",
       " 460: 'stopovers',\n",
       " 461: 'stopping',\n",
       " 462: 'stops',\n",
       " 463: 'sunday',\n",
       " 464: 'sundays',\n",
       " 465: 'sure',\n",
       " 466: 'tacoma',\n",
       " 467: 'take',\n",
       " 468: 'takeoff',\n",
       " 469: 'takeoffs',\n",
       " 470: 'taking',\n",
       " 471: 'tampa',\n",
       " 472: 'taxi',\n",
       " 473: 'tell',\n",
       " 474: 'ten',\n",
       " 475: 'tennessee',\n",
       " 476: 'tenth',\n",
       " 477: 'texas',\n",
       " 478: 'than',\n",
       " 479: 'thank',\n",
       " 480: 'that',\n",
       " 481: 'the',\n",
       " 482: 'their',\n",
       " 483: 'then',\n",
       " 484: 'there',\n",
       " 485: 'these',\n",
       " 486: 'they',\n",
       " 487: 'third',\n",
       " 488: 'thirteenth',\n",
       " 489: 'thirtieth',\n",
       " 490: 'thirty',\n",
       " 491: 'this',\n",
       " 492: 'those',\n",
       " 493: 'three',\n",
       " 494: 'thrift',\n",
       " 495: 'through',\n",
       " 496: 'thursday',\n",
       " 497: 'thursdays',\n",
       " 498: 'ticket',\n",
       " 499: 'tickets',\n",
       " 500: 'time',\n",
       " 501: 'times',\n",
       " 502: 'to',\n",
       " 503: 'today',\n",
       " 504: 'tomorrow',\n",
       " 505: 'too',\n",
       " 506: 'toronto',\n",
       " 507: 'total',\n",
       " 508: 'tower',\n",
       " 509: 'train',\n",
       " 510: 'transcontinental',\n",
       " 511: 'transport',\n",
       " 512: 'transportation',\n",
       " 513: 'travel',\n",
       " 514: 'traveling',\n",
       " 515: 'trip',\n",
       " 516: 'trips',\n",
       " 517: 'trying',\n",
       " 518: 'tuesday',\n",
       " 519: 'tuesdays',\n",
       " 520: 'turboprop',\n",
       " 521: 'twa',\n",
       " 522: 'twelfth',\n",
       " 523: 'twentieth',\n",
       " 524: 'twenty',\n",
       " 525: 'two',\n",
       " 526: 'type',\n",
       " 527: 'types',\n",
       " 528: 'ua',\n",
       " 529: 'under',\n",
       " 530: 'united',\n",
       " 531: 'up',\n",
       " 532: 'us',\n",
       " 533: 'use',\n",
       " 534: 'used',\n",
       " 535: 'uses',\n",
       " 536: 'using',\n",
       " 537: 'utah',\n",
       " 538: 'various',\n",
       " 539: 'vegas',\n",
       " 540: 'very',\n",
       " 541: 'via',\n",
       " 542: 'want',\n",
       " 543: 'washington',\n",
       " 544: 'way',\n",
       " 545: 'we',\n",
       " 546: 'wednesday',\n",
       " 547: 'wednesdays',\n",
       " 548: 'week',\n",
       " 549: 'weekday',\n",
       " 550: 'weekdays',\n",
       " 551: 'well',\n",
       " 552: 'west',\n",
       " 553: 'westchester',\n",
       " 554: 'what',\n",
       " 555: 'when',\n",
       " 556: 'where',\n",
       " 557: 'which',\n",
       " 558: 'who',\n",
       " 559: 'will',\n",
       " 560: 'wish',\n",
       " 561: 'with',\n",
       " 562: 'within',\n",
       " 563: 'without',\n",
       " 564: 'worth',\n",
       " 565: 'would',\n",
       " 566: 'y',\n",
       " 567: 'yes',\n",
       " 568: 'yn',\n",
       " 569: 'york',\n",
       " 570: 'you',\n",
       " 571: 'your'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GT 750M (CNMeM is disabled, cuDNN 5004)\n",
      "/Users/franciscojavierarceo/anaconda2/lib/python2.7/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n",
      "verbose: True\n",
      "normal: True\n",
      "win: 5\n",
      "savemodel: False\n",
      "fold: 3\n",
      "seed: 345\n",
      "emb_dimension: 50\n",
      "nepochs: 5\n",
      "nhidden: 200\n",
      "decay: True\n",
      "lr: 0.0970806646813\n",
      "folder: ../result\n",
      "... loading the dataset\n",
      "... building the model\n",
      "... training\n",
      "Downloading conlleval.pl from http://www-etud.iro.umontreal.ca/~mesnilgr/atis/conlleval.pl\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'urllib' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d07dded32efe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Experiment 1 -- a simple rnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhw4a\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_rnnslu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnhidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_dimension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/franciscojavierarceo/GitHub/DeepNLPQLearning/Code/hw4a.pyc\u001b[0m in \u001b[0;36mtest_rnnslu\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m                              \u001b[0mwords_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m                              \u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'folder'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/current.test.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m                              param['folder'])\n\u001b[0m\u001b[1;32m    286\u001b[0m         res_valid = conlleval(predictions_valid,\n\u001b[1;32m    287\u001b[0m                               \u001b[0mgroundtruth_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/franciscojavierarceo/GitHub/DeepNLPQLearning/Code/hw4_utils.py\u001b[0m in \u001b[0;36mconlleval\u001b[0;34m(p, g, w, filename, script_path)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_perf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_perf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/franciscojavierarceo/GitHub/DeepNLPQLearning/Code/hw4_utils.py\u001b[0m in \u001b[0;36mget_perf\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'http://www-etud.iro.umontreal.ca/~mesnilgr/atis/conlleval.pl'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Downloading conlleval.pl from %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_conlleval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_conlleval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mS_IRWXU\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# give the execute permissions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'urllib' is not defined"
     ]
    }
   ],
   "source": [
    "# Experiment 1 -- a simple rnn\n",
    "import hw4a as h\n",
    "h.test_rnnslu(nhidden=200, win=5, emb_dimension=50, normal=True, nepochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN-LSTM in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GT 750M (CNMeM is disabled, cuDNN 5004)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding\n",
    "from keras.layers import LSTM, SimpleRNN, GRU\n",
    "from keras.datasets import imdb\n",
    "from keras.optimizers import RMSprop\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 200\n",
    "maxlen = 80  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Oirginal dictionary has the numeric index as values and the keys as the words --- not helpful for revsering\n",
    "windx = imdb.get_word_index()\n",
    "# Fastest way to handle this is to reverse the dictionary to get the numeric values to be the dictionary keys \n",
    "rwindx = {v:k for k,v in windx.iteritems()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34701\n",
      "fawn\n"
     ]
    }
   ],
   "source": [
    "# Proof it works\n",
    "print( windx['fawn'])\n",
    "print( rwindx[34701])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'as',\n",
       " 'you',\n",
       " 'with',\n",
       " 'out',\n",
       " 'and',\n",
       " 'and',\n",
       " 'and',\n",
       " 'and',\n",
       " 'their',\n",
       " 'and',\n",
       " 'and',\n",
       " 'had',\n",
       " 'and',\n",
       " 'of',\n",
       " 'lot',\n",
       " 'from',\n",
       " 'and',\n",
       " 'to',\n",
       " 'have',\n",
       " 'after',\n",
       " 'out',\n",
       " 'and',\n",
       " 'never',\n",
       " 'more',\n",
       " 'and',\n",
       " 'and',\n",
       " 'it',\n",
       " 'so',\n",
       " 'and',\n",
       " 'and',\n",
       " 'to',\n",
       " 'years',\n",
       " 'of',\n",
       " 'every',\n",
       " 'never',\n",
       " 'going',\n",
       " 'and',\n",
       " 'and',\n",
       " 'and',\n",
       " 'or',\n",
       " 'of',\n",
       " 'every',\n",
       " 'and',\n",
       " 'and',\n",
       " 'movie',\n",
       " 'and',\n",
       " 'her',\n",
       " 'was',\n",
       " 'and',\n",
       " 'of',\n",
       " 'enough',\n",
       " 'more',\n",
       " 'with',\n",
       " 'is',\n",
       " 'now',\n",
       " 'and',\n",
       " 'film',\n",
       " 'as',\n",
       " 'you',\n",
       " 'of',\n",
       " 'and',\n",
       " 'and',\n",
       " 'and',\n",
       " 'of',\n",
       " 'you',\n",
       " 'than',\n",
       " 'him',\n",
       " 'that',\n",
       " 'with',\n",
       " 'out',\n",
       " 'and',\n",
       " 'her',\n",
       " 'get',\n",
       " 'for',\n",
       " 'was',\n",
       " 'and',\n",
       " 'of',\n",
       " 'you',\n",
       " 'movie',\n",
       " 'and',\n",
       " 'movie',\n",
       " 'that',\n",
       " 'with',\n",
       " 'and',\n",
       " 'but',\n",
       " 'and',\n",
       " 'to',\n",
       " 'story',\n",
       " 'and',\n",
       " 'that',\n",
       " 'in',\n",
       " 'and',\n",
       " 'in',\n",
       " 'character',\n",
       " 'to',\n",
       " 'of',\n",
       " 'and',\n",
       " 'and',\n",
       " 'with',\n",
       " 'and',\n",
       " 'had',\n",
       " 'and',\n",
       " 'they',\n",
       " 'of',\n",
       " 'here',\n",
       " 'that',\n",
       " 'with',\n",
       " 'her',\n",
       " 'and',\n",
       " 'to',\n",
       " 'have',\n",
       " 'does',\n",
       " 'when',\n",
       " 'from',\n",
       " 'why',\n",
       " 'what',\n",
       " 'have',\n",
       " 'and',\n",
       " 'they',\n",
       " 'is',\n",
       " 'you',\n",
       " 'that',\n",
       " 'and',\n",
       " 'one',\n",
       " 'will',\n",
       " 'very',\n",
       " 'to',\n",
       " 'as',\n",
       " 'and',\n",
       " 'with',\n",
       " 'other',\n",
       " 'and',\n",
       " 'in',\n",
       " 'of',\n",
       " 'seen',\n",
       " 'over',\n",
       " 'and',\n",
       " 'for',\n",
       " 'and',\n",
       " 'of',\n",
       " 'and',\n",
       " 'br',\n",
       " 'and',\n",
       " 'to',\n",
       " 'and',\n",
       " 'from',\n",
       " 'than',\n",
       " 'out',\n",
       " 'and',\n",
       " 'and',\n",
       " 'he',\n",
       " 'and',\n",
       " 'and',\n",
       " 'some',\n",
       " 'br',\n",
       " 'of',\n",
       " 'and',\n",
       " 'and',\n",
       " 'was',\n",
       " 'two',\n",
       " 'most',\n",
       " 'of',\n",
       " 'and',\n",
       " 'for',\n",
       " 'and',\n",
       " 'any',\n",
       " 'an',\n",
       " 'and',\n",
       " 'she',\n",
       " 'he',\n",
       " 'should',\n",
       " 'is',\n",
       " 'thought',\n",
       " 'and',\n",
       " 'but',\n",
       " 'of',\n",
       " 'and',\n",
       " 'you',\n",
       " 'not',\n",
       " 'while',\n",
       " 'and',\n",
       " 'he',\n",
       " 'and',\n",
       " 'to',\n",
       " 'real',\n",
       " 'at',\n",
       " 'and',\n",
       " 'but',\n",
       " 'when',\n",
       " 'from',\n",
       " 'one',\n",
       " 'and',\n",
       " 'then',\n",
       " 'have',\n",
       " 'two',\n",
       " 'of',\n",
       " 'and',\n",
       " 'their',\n",
       " 'with',\n",
       " 'her',\n",
       " 'and',\n",
       " 'most',\n",
       " 'that',\n",
       " 'with',\n",
       " 'and',\n",
       " 'to',\n",
       " 'with',\n",
       " 'and',\n",
       " 'acting',\n",
       " 'watch',\n",
       " 'an',\n",
       " 'for',\n",
       " 'with',\n",
       " 'and',\n",
       " 'film',\n",
       " 'want',\n",
       " 'an']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What this looks like for a single row\n",
    "[rwindx[x] for x in X_train[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " \"don't\",\n",
       " 'be',\n",
       " 'and',\n",
       " 'and',\n",
       " 'movie',\n",
       " 'give',\n",
       " 'say',\n",
       " 'to',\n",
       " 'and',\n",
       " 'with',\n",
       " 'and',\n",
       " 'his',\n",
       " 'in',\n",
       " 'and',\n",
       " 'of',\n",
       " 'and',\n",
       " 'br',\n",
       " 'of',\n",
       " 'you',\n",
       " 'and',\n",
       " 'and',\n",
       " 'with',\n",
       " 'and',\n",
       " 'movie',\n",
       " 'and',\n",
       " 'br',\n",
       " 'and',\n",
       " 'and',\n",
       " 'it',\n",
       " 'of',\n",
       " 'and',\n",
       " 'in',\n",
       " 'as',\n",
       " 'and',\n",
       " 'was',\n",
       " 'and',\n",
       " 'her',\n",
       " 'film',\n",
       " 'be',\n",
       " 'and',\n",
       " 'was',\n",
       " 'after',\n",
       " 'and',\n",
       " 'only',\n",
       " 'and',\n",
       " 'and',\n",
       " 'of',\n",
       " 'br',\n",
       " 'of',\n",
       " 'on',\n",
       " 'these',\n",
       " 'and',\n",
       " 'we',\n",
       " 'in',\n",
       " 'as',\n",
       " 'and',\n",
       " 'be',\n",
       " 'and',\n",
       " 'br',\n",
       " 'and',\n",
       " 'with',\n",
       " 'and',\n",
       " 'and',\n",
       " 'movie',\n",
       " 'all',\n",
       " 'real',\n",
       " 'one',\n",
       " 'will',\n",
       " 'and',\n",
       " 'but',\n",
       " 'that']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What this looks like for a single row\n",
    "[rwindx[x] for x in X_test[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "X_train shape: (25000, 80)\n",
      "X_test shape: (25000, 80)\n",
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "emb_dim = 50\n",
    "# RNN for the regular embedings\n",
    "print('Loading data...')\n",
    "\n",
    "# (X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features)\n",
    "\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "X_train_rnn = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test_rnn = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "print('X_train shape:', X_train_rnn.shape)\n",
    "print('X_test shape:', X_test_rnn.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, emb_dim, dropout=0.2))\n",
    "model.add(LSTM(emb_dim, input_shape=(maxlen, len(rwindx))))\n",
    "model.add(Dense(len(rwindx)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "fit() takes at least 3 arguments (6 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-56ec22a10974>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m model.fit(X_train_rnn, batch_size=batch_size, nb_epoch=1,\n\u001b[0;32m----> 3\u001b[0;31m           validation_data=(X_test_rnn), verbose=0)\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: fit() takes at least 3 arguments (6 given)"
     ]
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(X_train_rnn, batch_size=batch_size, nb_epoch=1,\n",
    "          validation_data=(X_test_rnn), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out = model.evaluate(X_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the original RNN-MLP for IMDB Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This is the original IMDB model, it's an LSTM feeding into an MLP\n",
    "\n",
    "print('Loading data...')\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features)\n",
    "\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, dropout=0.2))\n",
    "model.add(LSTM(128, dropout_W=0.2, dropout_U=0.2))  # try using a GRU instead, for fun\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=1,\n",
    "          validation_data=(X_test, y_test), verbose=0)\n",
    "score, acc = model.evaluate(X_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "ypred_tst = model.predict(X_test)\n",
    "# Pretty good AUC\n",
    "print('Test AUC:', roc_auc_score(y_test,ypred_tst))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nietzsche Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 600901\n",
      "total chars: 59\n",
      "nb sequences: 200287\n",
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.data_utils import get_file\n",
    "\n",
    "path = get_file('nietzsche.txt', origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\")\n",
    "text = open(path).read().lower()\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "print('Vectorization...')\n",
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200287, 40, 59)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       ..., \n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False]], dtype=bool)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False,  True, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False], dtype=bool)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "\n",
    "path = get_file('nietzsche.txt', origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\")\n",
    "text = open(path).read().lower()\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "print('Vectorization...')\n",
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "\n",
    "# build the model: a single LSTM\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "# train the model, output generated text after each iteration\n",
    "for iteration in range(1, 60):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    model.fit(X, y, batch_size=128, nb_epoch=1)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print()\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
