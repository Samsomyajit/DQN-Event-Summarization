{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN-LSTMs in Keras Demo with IMDB data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is my own RNN-LSTM to predict the last word in a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GT 750M (CNMeM is disabled, cuDNN 5004)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding\n",
    "from keras.layers import LSTM, SimpleRNN, GRU\n",
    "from keras.datasets import imdb\n",
    "from keras.optimizers import RMSprop\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34701\n",
      "fawn\n",
      "[1, 14, 22, 16, 43, 2, 2, 2, 2, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100, 43, 2, 112, 50, 2, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 2, 17, 2, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2, 19, 14, 22, 4, 2, 2, 469, 4, 22, 71, 87, 12, 16, 43, 2, 38, 76, 15, 13, 2, 4, 22, 17, 2, 17, 12, 16, 2, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2, 2, 16, 480, 66, 2, 33, 4, 130, 12, 16, 38, 2, 5, 25, 124, 51, 36, 135, 48, 25, 2, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 2, 5, 2, 36, 71, 43, 2, 476, 26, 400, 317, 46, 7, 4, 2, 2, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 2, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178]\n",
      "['the', 'as', 'you', 'with', 'out', 'and', 'and', 'and', 'and', 'their', 'becomes', 'and', 'had', 'and', 'of', 'lot', 'from', 'anyone', 'to', 'have', 'after', 'out', 'and', 'never', 'more', 'and', 'and', 'it', 'so', 'heart', 'shows', 'to', 'years', 'of', 'every', 'never', 'going', 'and', 'help', 'moments', 'or', 'of', 'every', 'and', 'and', 'movie', 'and', 'her', 'was', 'several', 'of', 'enough', 'more', 'with', 'is', 'now', 'and', 'film', 'as', 'you', 'of', 'and', 'and', 'unfortunately', 'of', 'you', 'than', 'him', 'that', 'with', 'out', 'and', 'her', 'get', 'for', 'was', 'and', 'of', 'you', 'movie', 'and', 'movie', 'that', 'with', 'and', 'but', 'and', 'to', 'story', 'wonderful', 'that', 'in', 'seeing', 'in', 'character', 'to', 'of', 'and', 'and', 'with', 'heart', 'had', 'and', 'they', 'of', 'here', 'that', 'with', 'her', 'and', 'to', 'have', 'does', 'when', 'from', 'why', 'what', 'have', 'and', 'they', 'is', 'you', 'that', \"isn't\", 'one', 'will', 'very', 'to', 'as', 'itself', 'with', 'other', 'and', 'in', 'of', 'seen', 'over', 'and', 'for', 'anyone', 'of', 'and', 'br', 'and', 'to', 'and', 'from', 'than', 'out', 'and', 'history', 'he', 'name', 'half', 'some', 'br', 'of', 'and', 'and', 'was', 'two', 'most', 'of', 'mean', 'for', '1', 'any', 'an', 'and', 'she', 'he', 'should', 'is', 'thought', 'and', 'but', 'of', 'script', 'you', 'not', 'while', 'history', 'he', 'heart', 'to', 'real', 'at', 'and', 'but', 'when', 'from', 'one', 'bit', 'then', 'have', 'two', 'of', 'script', 'their', 'with', 'her', 'and', 'most', 'that', 'with', \"wasn't\", 'to', 'with', 'and', 'acting', 'watch', 'an', 'for', 'with', 'and', 'film', 'want']\n"
     ]
    }
   ],
   "source": [
    "max_features = 500\n",
    "emb_dim = 10\n",
    "maxlen = 80  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features)\n",
    "\n",
    "# Taking the last element of X_train[i] as our outcome\n",
    "Y_train = np.asarray([i[-1] for i in X_train]) \n",
    "Y_test = np.asarray([ i[-1] for i in X_test])\n",
    "\n",
    "# Removing the last element of X_train[i]\n",
    "X_train = np.asarray([i[:-1] for i in X_train]) \n",
    "X_test = np.asarray([i[:-1] for i in X_test] )\n",
    "\n",
    "\n",
    "windx = imdb.get_word_index()\n",
    "rwindx = dict((k,v) for v,k in windx.items())\n",
    "\n",
    "# Proof it the index swap worked\n",
    "print( windx['fawn'])\n",
    "print( rwindx[34701])\n",
    "\n",
    "# Looking at one set of data\n",
    "print( X_train[0])\n",
    "\n",
    "# Let's see the words\n",
    "print( [rwindx[x] for x in X_train[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "X_train shape: (25000, 80)\n",
      "X_test shape: (25000, 80)\n"
     ]
    }
   ],
   "source": [
    "# RNN for the regular embedings\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "# The pad sequences stuff seems suspicious\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Train...\n",
      "25000/25000 [==============================] - 53s    \n",
      "Cross Entropy Loss of 3.005\n",
      "Classification Accuracy is 0.516\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, emb_dim, dropout=0.2))\n",
    "model.add(LSTM(150))\n",
    "model.add(Dense(max_features))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "print('Train...')\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=1,\n",
    "          validation_data=(X_test, Y_test), verbose=0)\n",
    "\n",
    "# Running the predictions\n",
    "out = model.evaluate(X_test, Y_test, batch_size=batch_size)\n",
    "ypred_tst = model.predict(X_test)\n",
    "\n",
    "print('Cross Entropy Loss of %0.3f' % out)\n",
    "\n",
    "yprdtst = np.argmax(ypred_tst, axis=1)\n",
    "x = confusion_matrix(yprdtst, Y_test).astype('float') \n",
    "# Normalizing it\n",
    "x_n = x/ x.sum(axis=0)\n",
    "\n",
    "print ('Classification Accuracy is %0.3f' % (np.diag(x).sum()/ x.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Look at the confusion matrix image\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.imshow(x_n, cmap='OrRd')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.unique(X_train), len(np.unique(X_train)), len(range(np.min(X_train), np.max(X_train)+1)) # Have to do +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.get_weights()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[ [j.shape for j in l.get_weights()] for l in model.layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Embedding layers\n",
    "embs = model.layers[0].get_weights()[0]\n",
    "embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(embs.dot(embs.T), cmap='OrRd')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uniqs = np.unique(X_train)\n",
    "min_x, max_x = np.min(X_train), np.max(X_train)\n",
    "print(uniqs)\n",
    "print(\"%i unique values\"% len(uniqs) )\n",
    "print('Ranging from %i to %i' % (min_x, max_x) )\n",
    "if min_x==0:\n",
    "    max_x+=1\n",
    "print(\"Which means we have an index matrix with %i words\" % (max_x-min_x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# The indexing below is correct\n",
    "# -- note that we don't have a 3 in our np.unique(X_train) so we only have 40 unique values b/c we start at 0\n",
    "# -- which is inserted by the padding function, and note that we don't have 0 in our dictionary list\n",
    "# --\n",
    "sims = embs.dot( embs[windx['film']].reshape((10,1)))\n",
    "for i, si in enumerate(sims):\n",
    "    if i==0:\n",
    "        continue\n",
    "    print(i, rwindx[i], si)\n",
    "        \n",
    "print('Closest match is %i' % np.delete(sims, windx['film']).argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rwindx[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here are the words in the model\n",
    "print( [rwindx[x] for x in np.unique(X_train).tolist() if x!=0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "----\n",
    "\n",
    "## This is the original RNN-LSTM-MLP for IMDB Classification {0,1}\n",
    "\n",
    "Original Source: https://github.com/fchollet/keras/blob/master/examples/imdb_lstm.py\n",
    "\n",
    "    from __future__ import print_function\n",
    "    import numpy as np\n",
    "    np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "    from keras.preprocessing import sequence\n",
    "    from keras.utils import np_utils\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Dropout, Activation, Embedding\n",
    "    from keras.layers import LSTM, SimpleRNN, GRU\n",
    "    from keras.datasets import imdb\n",
    "    from keras.optimizers import RMSprop\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "\n",
    "    max_features = 200\n",
    "    maxlen = 80  # cut texts after this number of words (among top max_features most common words)\n",
    "    batch_size = 32\n",
    "\n",
    "    # This is the original IMDB model, it's an LSTM feeding into an MLP\n",
    "\n",
    "    print('Loading data...')\n",
    "\n",
    "    (X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features)\n",
    "\n",
    "    print(len(X_train), 'train sequences')\n",
    "    print(len(X_test), 'test sequences')\n",
    "\n",
    "    print('Pad sequences (samples x time)')\n",
    "    X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "    X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "    print('X_train shape:', X_train.shape)\n",
    "    print('X_test shape:', X_test.shape)\n",
    "\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, 128, dropout=0.2))\n",
    "    model.add(LSTM(128, dropout_W=0.2, dropout_U=0.2))  # try using a GRU instead, for fun\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    # try using different optimizers and different optimizer configs\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    print('Train...')\n",
    "    model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=1,\n",
    "              validation_data=(X_test, y_test), verbose=0)\n",
    "    score, acc = model.evaluate(X_test, y_test,\n",
    "                                batch_size=batch_size)\n",
    "    print('Test score:', score)\n",
    "    print('Test accuracy:', acc)\n",
    "    ypred_tst = model.predict(X_test)\n",
    "    # Pretty good AUC\n",
    "    print('Test AUC:', roc_auc_score(y_test,ypred_tst))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
