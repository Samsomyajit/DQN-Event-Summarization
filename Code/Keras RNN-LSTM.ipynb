{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN-LSTMs in Keras Demo with IMDB data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the original RNN-LSTM-MLP for IMDB Classification {0,1}\n",
    "\n",
    "Original Source: https://github.com/fchollet/keras/blob/master/examples/imdb_lstm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding\n",
    "from keras.layers import LSTM, SimpleRNN, GRU\n",
    "from keras.datasets import imdb\n",
    "from keras.optimizers import RMSprop\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "max_features = 200\n",
    "maxlen = 80  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32\n",
    "\n",
    "# This is the original IMDB model, it's an LSTM feeding into an MLP\n",
    "\n",
    "print('Loading data...')\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features)\n",
    "\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, dropout=0.2))\n",
    "model.add(LSTM(128, dropout_W=0.2, dropout_U=0.2))  # try using a GRU instead, for fun\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=1,\n",
    "          validation_data=(X_test, y_test), verbose=0)\n",
    "score, acc = model.evaluate(X_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "ypred_tst = model.predict(X_test)\n",
    "# Pretty good AUC\n",
    "print('Test AUC:', roc_auc_score(y_test,ypred_tst))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is my own RNN-LSTM to predict the last word in a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding\n",
    "from keras.layers import LSTM, SimpleRNN, GRU\n",
    "from keras.datasets import imdb\n",
    "from keras.optimizers import RMSprop\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 500\n",
    "emb_dim = 10\n",
    "maxlen = 80  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features)\n",
    "windx = imdb.get_word_index()\n",
    "rwindx = dict((k,v) for v,k in windx.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Proof it works\n",
    "print( windx['fawn'])\n",
    "print( rwindx[34701])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(np.unique(X_train)), len(windx), len(rwindx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print( X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What this looks like for a single row\n",
    "print( [rwindx[x] for x in X_train[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Removing the last element\n",
    "X_train = np.asarray([i[:-1] for i in X_train]) # Removing the last element\n",
    "X_test = np.asarray([i[:-1] for i in X_test] )\n",
    "\n",
    "# Using just the last elemnt instead\n",
    "Y_train = np.asarray([i[-1] for i in X_train]) \n",
    "Y_test = np.asarray([ i[-1] for i in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_test.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RNN for the regular embedings\n",
    "print('Loading data...')\n",
    "\n",
    "# (X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features)\n",
    "\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "# The pad sequences stuff seems suspicious\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, emb_dim, dropout=0.2))\n",
    "model.add(LSTM(150))\n",
    "#model.add(LSTM(150, input_shape=(maxlen, len(rwindx))))\n",
    "model.add(Dense(max_features))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Train...')\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=1,\n",
    "          validation_data=(X_test, Y_test), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out = model.evaluate(X_test, Y_test, batch_size=batch_size)\n",
    "ypred_tst = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Cross Entropy Loss of %0.3f' % out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yprdtst = np.argmax(ypred_tst, axis=1)\n",
    "x = confusion_matrix(yprdtst, Y_test).astype('float') \n",
    "# Normalizing it\n",
    "x_n = x/ x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'Accuracy is %0.3f' % (np.diag(x).sum()/ x.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.imshow(x_n, cmap='OrRd')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.unique(X_train), len(np.unique(X_train)), len(range(np.min(X_train), np.max(X_train)+1)) # Have to do +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.get_weights()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[ [j.shape for j in l.get_weights()] for l in model.layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Embedding layers\n",
    "embs = model.layers[0].get_weights()[0]\n",
    "embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(embs.dot(embs.T), cmap='OrRd')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uniqs = np.unique(X_train)\n",
    "min_x, max_x = np.min(X_train), np.max(X_train)\n",
    "print(uniqs)\n",
    "print(\"%i unique values\"% len(uniqs) )\n",
    "print('Ranging from %i to %i' % (min_x, max_x) )\n",
    "if min_x==0:\n",
    "    max_x+=1\n",
    "print(\"Which means we have an index matrix with %i words\" % (max_x-min_x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# The indexing below is correct\n",
    "# -- note that we don't have a 3 in our np.unique(X_train) so we only have 40 unique values b/c we start at 0\n",
    "# -- which is inserted by the padding function, and note that we don't have 0 in our dictionary list\n",
    "# --\n",
    "sims = embs.dot( embs[windx['film']].reshape((10,1)))\n",
    "for i, si in enumerate(sims):\n",
    "    if i==0:\n",
    "        continue\n",
    "    print(i, rwindx[i], si)\n",
    "        \n",
    "print('Closest match is %i' % np.delete(sims, windx['film']).argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rwindx[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here are the words in the model\n",
    "print( [rwindx[x] for x in np.unique(X_train).tolist() if x!=0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
