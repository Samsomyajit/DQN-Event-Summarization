{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "require 'rnn'\n",
    "require 'torch'\n",
    "require 'cunn'\n",
    "require 'paths'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainset, validset, testset = loadPTB({opt.batchsize,1,1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "local trainset, validset, testset = dl.loadPTB({opt.batchsize,1,1})\n",
    "print(\"Vocabulary size : \"..#trainset.ivocab) \n",
    "print(\"Train set split into \"..opt.batchsize..\" sequences of length \"..trainset:size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "--[[ language model ]]--\n",
    "local lm = nn.Sequential()\n",
    "\n",
    "-- input layer (i.e. word embedding space)\n",
    "local lookup = nn.LookupTable(#trainset.ivocab, opt.inputsize)\n",
    "lookup.maxnormout = -1 -- prevent weird maxnormout behaviour\n",
    "lm:add(lookup) -- input is seqlen x batchsize\n",
    "if opt.dropout > 0 and not opt.gru then  -- gru has a dropout option\n",
    "   lm:add(nn.Dropout(opt.dropout))\n",
    "end\n",
    "lm:add(nn.SplitTable(1)) -- tensor to table of tensors\n",
    "\n",
    "-- rnn layers\n",
    "local stepmodule = nn.Sequential() -- applied at each time-step\n",
    "local inputsize = opt.inputsize\n",
    "for i,hiddensize in ipairs(opt.hiddensize) do \n",
    "   local rnn\n",
    "   \n",
    "   if opt.gru then -- Gated Recurrent Units\n",
    "      rnn = nn.GRU(inputsize, hiddensize, nil, opt.dropout/2)\n",
    "   elseif opt.lstm then -- Long Short Term Memory units\n",
    "      require 'nngraph'\n",
    "      nn.FastLSTM.usenngraph = true -- faster\n",
    "      nn.FastLSTM.bn = opt.bn\n",
    "      rnn = nn.FastLSTM(inputsize, hiddensize)\n",
    "   else -- simple recurrent neural network\n",
    "      local rm =  nn.Sequential() -- input is {x[t], h[t-1]}\n",
    "         :add(nn.ParallelTable()\n",
    "            :add(i==1 and nn.Identity() or nn.Linear(inputsize, hiddensize)) -- input layer\n",
    "            :add(nn.Linear(hiddensize, hiddensize))) -- recurrent layer\n",
    "         :add(nn.CAddTable()) -- merge\n",
    "         :add(nn.Sigmoid()) -- transfer\n",
    "      rnn = nn.Recurrence(rm, hiddensize, 1)\n",
    "   end\n",
    "\n",
    "   stepmodule:add(rnn)\n",
    "   \n",
    "   if opt.dropout > 0 then\n",
    "      stepmodule:add(nn.Dropout(opt.dropout))\n",
    "   end\n",
    "   \n",
    "   inputsize = hiddensize\n",
    "end\n",
    "\n",
    "-- output layer\n",
    "stepmodule:add(nn.Linear(inputsize, #trainset.ivocab))\n",
    "stepmodule:add(nn.LogSoftMax())\n",
    "\n",
    "-- encapsulate stepmodule into a Sequencer\n",
    "lm:add(nn.Sequencer(stepmodule))\n",
    "\n",
    "-- remember previous state between batches\n",
    "lm:remember((opt.lstm or opt.gru) and 'both' or 'eval')\n",
    "\n",
    "if not opt.silent then\n",
    "   print\"Language Model:\"\n",
    "   print(lm)\n",
    "end\n",
    "\n",
    "if opt.uniform > 0 then\n",
    "   for k,param in ipairs(lm:parameters()) do\n",
    "      param:uniform(-opt.uniform, opt.uniform)\n",
    "   end\n",
    "end\n",
    "\n",
    "--[[ loss function ]]--\n",
    "\n",
    "local crit = nn.ClassNLLCriterion()\n",
    "\n",
    "-- target is also seqlen x batchsize.\n",
    "local targetmodule = nn.SplitTable(1)\n",
    "if opt.cuda then\n",
    "   targetmodule = nn.Sequential()\n",
    "      :add(nn.Convert())\n",
    "      :add(targetmodule)\n",
    "end\n",
    " \n",
    "local criterion = nn.SequencerCriterion(crit)\n",
    "\n",
    "--[[ CUDA ]]--\n",
    "\n",
    "if opt.cuda then\n",
    "   lm:cuda()\n",
    "   criterion:cuda()\n",
    "   targetmodule:cuda()\n",
    "end\n",
    "\n",
    "--[[ experiment log ]]--\n",
    "\n",
    "-- is saved to file every time a new validation minima is found\n",
    "local xplog = {}\n",
    "xplog.opt = opt -- save all hyper-parameters and such\n",
    "xplog.dataset = 'PennTreeBank'\n",
    "xplog.vocab = trainset.vocab\n",
    "-- will only serialize params\n",
    "xplog.model = nn.Serial(lm)\n",
    "xplog.model:mediumSerial()\n",
    "xplog.criterion = criterion\n",
    "xplog.targetmodule = targetmodule\n",
    "-- keep a log of NLL for each epoch\n",
    "xplog.trainppl = {}\n",
    "xplog.valppl = {}\n",
    "-- will be used for early-stopping\n",
    "xplog.minvalppl = 99999999\n",
    "xplog.epoch = 0\n",
    "local ntrial = 0\n",
    "paths.mkdir(opt.savepath)\n",
    "\n",
    "local epoch = 1\n",
    "opt.lr = opt.startlr\n",
    "opt.trainsize = opt.trainsize == -1 and trainset:size() or opt.trainsize\n",
    "opt.validsize = opt.validsize == -1 and validset:size() or opt.validsize\n",
    "while opt.maxepoch <= 0 or epoch <= opt.maxepoch do\n",
    "   print(\"\")\n",
    "   print(\"Epoch #\"..epoch..\" :\")\n",
    "\n",
    "   -- 1. training\n",
    "   \n",
    "   local a = torch.Timer()\n",
    "   lm:training()\n",
    "   local sumErr = 0\n",
    "   for i, inputs, targets in trainset:subiter(opt.seqlen, opt.trainsize) do\n",
    "      targets = targetmodule:forward(targets)\n",
    "      \n",
    "      -- forward\n",
    "      local outputs = lm:forward(inputs)\n",
    "      local err = criterion:forward(outputs, targets)\n",
    "      sumErr = sumErr + err\n",
    "      \n",
    "      -- backward \n",
    "      local gradOutputs = criterion:backward(outputs, targets)\n",
    "      lm:zeroGradParameters()\n",
    "      lm:backward(inputs, gradOutputs)\n",
    "      \n",
    "      -- update\n",
    "      if opt.cutoff > 0 then\n",
    "         local norm = lm:gradParamClip(opt.cutoff) -- affects gradParams\n",
    "         opt.meanNorm = opt.meanNorm and (opt.meanNorm*0.9 + norm*0.1) or norm\n",
    "      end\n",
    "      lm:updateGradParameters(opt.momentum) -- affects gradParams\n",
    "      lm:updateParameters(opt.lr) -- affects params\n",
    "      lm:maxParamNorm(opt.maxnormout) -- affects params\n",
    "\n",
    "      if opt.progress then\n",
    "         xlua.progress(math.min(i + opt.seqlen, opt.trainsize), opt.trainsize)\n",
    "      end\n",
    "\n",
    "      if i % 1000 == 0 then\n",
    "         collectgarbage()\n",
    "      end\n",
    "\n",
    "   end\n",
    "   \n",
    "   -- learning rate decay\n",
    "   if opt.schedule then\n",
    "      opt.lr = opt.schedule[epoch] or opt.lr\n",
    "   else\n",
    "      opt.lr = opt.lr + (opt.minlr - opt.startlr)/opt.saturate\n",
    "   end\n",
    "   opt.lr = math.max(opt.minlr, opt.lr)\n",
    "   \n",
    "   if not opt.silent then\n",
    "      print(\"learning rate\", opt.lr)\n",
    "      if opt.meanNorm then\n",
    "         print(\"mean gradParam norm\", opt.meanNorm)\n",
    "      end\n",
    "   end\n",
    "\n",
    "   if cutorch then cutorch.synchronize() end\n",
    "   local speed = a:time().real/opt.trainsize\n",
    "   print(string.format(\"Speed : %f sec/batch \", speed))\n",
    "\n",
    "   local ppl = torch.exp(sumErr/opt.trainsize)\n",
    "   print(\"Training PPL : \"..ppl)\n",
    "\n",
    "   xplog.trainppl[epoch] = ppl\n",
    "\n",
    "   -- 2. cross-validation\n",
    "\n",
    "   lm:evaluate()\n",
    "   local sumErr = 0\n",
    "   for i, inputs, targets in validset:subiter(opt.seqlen, opt.validsize) do\n",
    "      targets = targetmodule:forward(targets)\n",
    "      local outputs = lm:forward(inputs)\n",
    "      local err = criterion:forward(outputs, targets)\n",
    "      sumErr = sumErr + err\n",
    "   end\n",
    "\n",
    "   local ppl = torch.exp(sumErr/opt.validsize)\n",
    "   -- Note :\n",
    "   -- Perplexity = exp( sum ( NLL ) / #w)\n",
    "   -- Bits Per Word = log2(Perplexity)\n",
    "   print(\"Validation PPL : \"..ppl)\n",
    "\n",
    "   xplog.valppl[epoch] = ppl\n",
    "   ntrial = ntrial + 1\n",
    "\n",
    "   -- early-stopping\n",
    "   if ppl < xplog.minvalppl then\n",
    "      -- save best version of model\n",
    "      xplog.minvalppl = ppl\n",
    "      xplog.epoch = epoch \n",
    "      local filename = paths.concat(opt.savepath, opt.id..'.t7')\n",
    "      print(\"Found new minima. Saving to \"..filename)\n",
    "      torch.save(filename, xplog)\n",
    "      ntrial = 0\n",
    "   elseif ntrial >= opt.earlystop then\n",
    "      print(\"No new minima found after \"..ntrial..\" epochs.\")\n",
    "      print(\"Stopping experiment.\")\n",
    "      break\n",
    "   end\n",
    "\n",
    "   collectgarbage()\n",
    "   epoch = epoch + 1\n",
    "end\n",
    "print(\"Evaluate model using : \")\n",
    "print(\"th scripts/evaluate-rnnlm.lua --xplogpath \"..paths.concat(opt.savepath, opt.id..'.t7')..(opt.cuda and ' --cuda' or ''))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
