{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "require 'torch'\n",
    "require 'nn'\n",
    "require 'rnn'\n",
    "require 'csvigo'\n",
    "require 'cutorch'\n",
    "require 'cunn'\n",
    "require 'cunnx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "...Utils file loaded\t\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model = 'lstm'\n",
    "nepochs = 200\n",
    "K_tokens = 20\n",
    "J_sentences = 10\n",
    "batch_size = 200\n",
    "thresh = 0.00\n",
    "embed_dim = 50\n",
    "learning_rate = 0.1\n",
    "print_every = 1\n",
    "usecuda = false\n",
    "epsilon = 1\n",
    "cuts = 4\n",
    "base_explore_rate = 0.0\n",
    "skip_rate = 0.\n",
    "metric = \"f1\"\n",
    "\n",
    "--- Loading utility script\n",
    "dofile(\"utils.lua\")\n",
    "dofile(\"model_utils.lua\")\n",
    "\n",
    "torch.manualSeed(420)\n",
    "math.randomseed(420)\n",
    "\n",
    "data_path = '~/GitHub/DeepNLPQLearning/DO_NOT_UPLOAD_THIS_DATA/0-output/'\n",
    "\n",
    "query_fn = data_path .. 'queries_numtext.csv'\n",
    "query_file =  csvigo.load({path = query_fn, mode = \"large\", verbose = false})\n",
    "queries = buildTermDocumentTable(query_file, nil)\n",
    "\n",
    "aurora = {\n",
    "        ['inputs'] = '2012_aurora_shooting_first_sentence_numtext2.csv', \n",
    "        ['nuggets'] = 'aurora_nuggets_numtext.csv',\n",
    "        ['query'] = queries[3],\n",
    "        ['query_name'] = 'aurora'\n",
    "}\n",
    "\n",
    "inputs = {\n",
    "        aurora\n",
    "    }\n",
    "--- Only using epsilon greedy strategy for (nepochs/cuts)% of the epochs\n",
    "delta = 1./(nepochs/cuts) \n",
    "crit = nn.MSECriterion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_path, query_file, batch_size, nepochs, inputs = data_path, query_file, batch_size, nepochs, inputs\n",
    "nn_model, crit, thresh, embed_dim, epsilon, delta = nn_model, crit, thresh, embed_dim, epsilon, delta\n",
    "base_explore_rate, print_every = base_explore_rate, print_every\n",
    "learning_rate, J_sentences, K_tokens, use_cuda = learning_rate, J_sentences, K_tokens, usecuda\n",
    "skiprate, emetric = skip_rate, metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "...running on CPU\t\n",
       "training model with metric = f1, learning rate = 0.100, K = 20, J = 10, threshold = 0.000, embedding size = 50\t\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Running LSTM model\t\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if use_cuda then\n",
    "  Tensor = torch.CudaTensor\n",
    "  LongTensor = torch.CudaLongTensor\n",
    "  crit = crit:cuda()\n",
    "  print(\"...running on GPU\")\n",
    "else\n",
    "  Tensor = torch.Tensor\n",
    "  LongTensor = torch.LongTensor\n",
    "  print(\"...running on CPU\")\n",
    "end\n",
    "\n",
    "print_string = string.format(\n",
    "    \"training model with metric = %s, learning rate = %.3f, K = %i, J = %i, threshold = %.3f, embedding size = %i\",\n",
    "            emetric, learning_rate, K_tokens, J_sentences, thresh, embed_dim, batch_size\n",
    "            )\n",
    "\n",
    "print(print_string)\n",
    "\n",
    "vocab_size = 0\n",
    "maxseqlen = 0\n",
    "maxseqlenq = getMaxseq(query_file)\n",
    "\n",
    "action_query_list = {}\n",
    "yrougue_query_list = {}\n",
    "pred_query_list = {}\n",
    "\n",
    "--- Initializing query information\n",
    "for query_id = 1, #inputs do\n",
    "    input_fn = inputs[query_id]['inputs']\n",
    "    nugget_fn = inputs[query_id]['nuggets']\n",
    "\n",
    "    input_file = csvigo.load({path = input_path .. input_fn, mode = \"large\", verbose = false})\n",
    "    nugget_file = csvigo.load({path = input_path .. nugget_fn, mode = \"large\", verbose = false})\n",
    "    input_file = geti_n(input_file, 2, #input_file) \n",
    "    nugget_file = geti_n(nugget_file, 2, #nugget_file) \n",
    "\n",
    "    vocab_sized = getVocabSize(input_file)\n",
    "    vocab_sizeq = getVocabSize(query_file)\n",
    "    vocab_size = math.max(vocab_size, vocab_sized, vocab_sizeq)\n",
    "\n",
    "    maxseqlend = getMaxseq(input_file)\n",
    "    maxseqlen = math.max(maxseqlen, maxseqlenq, maxseqlend)\n",
    "    action_list = torch.totable(torch.round(torch.rand(#input_file)))\n",
    "\n",
    "    --- initialize the query specific lists\n",
    "    action_query_list[query_id] = action_list\n",
    "    yrougue_query_list[query_id] = torch.totable(torch.randn(#input_file, 1)) --- Actual\n",
    "    pred_query_list[query_id] = torch.totable(torch.zeros(#input_file, 1))    --- Predicted\n",
    "end\n",
    "model  = build_model(nn_model, vocab_size, embed_dim, use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Epoch 0, epsilon = 1.000, sum(y)/len(y) = 999/999, {Recall = 0.797317, Precision = 0.118764, F1 = 0.206734}\t\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 1, epsilon = 0.980, sum(y)/len(y) = 999/999, {Recall = 0.797317, Precision = 0.118764, F1 = 0.206734}\t\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 2, epsilon = 0.960, sum(y)/len(y) = 559/999, {Recall = 0.730714, Precision = 0.195538, F1 = 0.308517}\t\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 3, epsilon = 0.940, sum(y)/len(y) = 999/999, {Recall = 0.797317, Precision = 0.118764, F1 = 0.206734}\t\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for epoch=0, nepochs, 1 do\n",
    "    loss = 0.                    --- Compute a new MSE loss each time\n",
    "    --- Looping over each bach of sentences for a given query\n",
    "    for query_id = 1, #inputs do\n",
    "        --- Grabbing all of the input data\n",
    "        qs = inputs[query_id]['query']\n",
    "        input_file = csvigo.load({path = input_path .. inputs[query_id]['inputs'], mode = \"large\", verbose = false})\n",
    "        nugget_file = csvigo.load({path = input_path .. inputs[query_id]['nuggets'], mode = \"large\", verbose = false})\n",
    "        --- Dropping the headers\n",
    "        input_file = geti_n(input_file, 2, #input_file) \n",
    "        nugget_file = geti_n(nugget_file, 2, #nugget_file) \n",
    "\n",
    "        --- Building table of all of the input sentences\n",
    "        nuggets = buildTermDocumentTable(nugget_file, nil)\n",
    "        xtdm  = buildTermDocumentTable(input_file, K_tokens)\n",
    "\n",
    "        --- Extracting the query specific summaries, actions, and rougue\n",
    "        action_list = action_query_list[query_id]\n",
    "        yrougue = yrougue_query_list[query_id] \n",
    "        preds = pred_query_list[query_id]\n",
    "\n",
    "        --- Loop over file to execute forward pass to estimate expected rougue\n",
    "        for minibatch = 1, #xtdm do\n",
    "            --- Notice that the actionlist is optimized at after each iteration\n",
    "            local summaries = padZeros(buildCurrentSummary(action_list, xtdm, \n",
    "                                    K_tokens * J_sentences), \n",
    "                                    K_tokens * J_sentences)\n",
    "            sentence = LongTensor(padZeros( {xtdm[minibatch]}, K_tokens) ):t()\n",
    "            summary = LongTensor({ summaries[minibatch] }):t()\n",
    "            query = LongTensor( padZeros({qs}, 5) ):t()\n",
    "\n",
    "            --- Retrieve intermediate optimal action in model.get(3).output\n",
    "            pred_rougue = model:forward({sentence, summary, query})   \n",
    "            pred_actions = torch.totable(model:get(3).output)\n",
    "            opt_action = (pred_actions[1][1] > pred_actions[1][2]) and 1 or 0\n",
    "\n",
    "            -- Updating our book-keeping tables\n",
    "            preds[minibatch] = pred_rougue[1]\n",
    "            action_list[minibatch] = opt_action\n",
    "        end\n",
    "\n",
    "        --- Note setting the skip_rate = 0 means no random skipping of delta calculation\n",
    "        yrougue = score_model(action_list, \n",
    "                            xtdm,\n",
    "                            epsilon, \n",
    "                            thresh, \n",
    "                            skiprate, \n",
    "                            emetric)\n",
    "        \n",
    "        --- Updating variables\n",
    "        pred_query_list[query_id] = preds\n",
    "        yrougue_query_list[query_id] = yrougue\n",
    "        action_query_list[query_id] = action_list\n",
    "\n",
    "        --- Rerunning on the scoring on the full data and rescoring cumulatively\n",
    "        --- Execute policy and evaluation based on our E[ROUGUE] after all of the minibatches\n",
    "            --- Notice that pred_rougue gives us our optimal action by returning\n",
    "            ---  E[ROUGUE | Select ] > E[ROUGUE | Skip]\n",
    "        predsummary = buildPredSummary(action_list, xtdm, nil)\n",
    "        predsummary = predsummary[#predsummary]\n",
    "\n",
    "        rscore = rougeRecall({predsummary}, nuggets)\n",
    "        pscore = rougePrecision({predsummary}, nuggets)\n",
    "        fscore = rougeF1({predsummary}, nuggets)\n",
    "\n",
    "        if (epoch % print_every)==0 then\n",
    "            perf_string = string.format(\n",
    "                \"Epoch %i, epsilon = %.3f, sum(y)/len(y) = %i/%i, {Recall = %.6f, Precision = %.6f, F1 = %.6f}\", \n",
    "                epoch, epsilon, sumTable(action_list), #action_list, rscore, pscore, fscore\n",
    "                )\n",
    "            print(perf_string)\n",
    "        end\n",
    "\n",
    "        --- creating the indices we want\n",
    "        -- local qindices = {}\n",
    "        xindices = {}\n",
    "        for i=1, batch_size do\n",
    "            -- qindices[i] = math.random(1, #inputs)\n",
    "            xindices[i] = math.random(1, #xtdm)\n",
    "        end\n",
    "\n",
    "        summaries = padZeros(buildCurrentSummary(action_list, xtdm, \n",
    "                                    K_tokens * J_sentences), \n",
    "                                    K_tokens * J_sentences)\n",
    "\n",
    "        --- Backward step\n",
    "        for i= 1, batch_size do\n",
    "            sentence = LongTensor(padZeros( {xtdm[xindices[i]]}, K_tokens) ):t()\n",
    "            summary = LongTensor({summaries[xindices[i]]}):t()\n",
    "            query = LongTensor(padZeros({qs}, 5)):t()\n",
    "\n",
    "            labels = Tensor({yrougue[xindices[i]]})\n",
    "            pred_rougue = Tensor({preds[xindices[i]]})\n",
    "\n",
    "            --- Backprop model\n",
    "            loss = loss + crit:forward(pred_rougue, labels)\n",
    "            local grads = crit:backward(pred_rougue, labels)\n",
    "            model:zeroGradParameters()\n",
    "            --- For some reason doing this fixes it\n",
    "            local tmp = model:forward({sentence, summary, query})\n",
    "            model:backward({sentence, summary, query}, grads)\n",
    "            model:updateParameters(learning_rate)\n",
    "        end \n",
    "    end -- ends the query loop\n",
    "    if (epsilon - delta) <= base_explore_rate then                --- and leaving a random exploration rate\n",
    "        epsilon = base_explore_rate\n",
    "    else \n",
    "        epsilon = epsilon - delta           --- Decreasing the epsilon greedy strategy\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grads = crit:backward(pred_rougue, labels)\n",
    "model:zeroGradParameters()\n",
    "model:backward({sentence, summary, query}, grads)\n",
    "model:updateParameters(learning_rate)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "--- Updating variables\n",
    "pred_query_list[query_id] = preds\n",
    "yrougue_query_list[query_id] = yrougue\n",
    "action_query_list[query_id] = opt_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- #query, #sentence, #summary, #pred_rougue, #labels\n",
    "i = 1\n",
    "sentence = LongTensor(padZeros( {xtdm[xindices[i]]}, K_tokens) ):t()\n",
    "summary = LongTensor({summaries[xindices[i]]}):t()\n",
    "query = LongTensor(padZeros({qs}, 5)):t()\n",
    "\n",
    "labels = Tensor({yrougue[xindices[i]]})\n",
    "pred_rougue = Tensor({preds[xindices[i]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "--- Backprop model\n",
    "loss = loss + crit:forward(pred_rougue, labels)\n",
    "grads = crit:backward(pred_rougue, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01 *\n",
       " 7.6549\n",
       "[torch.DoubleTensor of size 1]\n",
       "\n",
       " 0.2104\n",
       "[torch.DoubleTensor of size 1]\n",
       "\n",
       "-0.2678\n",
       "[torch.DoubleTensor of size 1]\n",
       "\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_rougue, labels, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0.1565\n",
       "[torch.DoubleTensor of size 1]\n",
       "\n"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model:forward({sentence, summary, query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  1 : LongTensor - size: 20x1\n",
       "  2 : LongTensor - size: 200x1\n",
       "  3 : LongTensor - size: 5x1\n",
       "}\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model:backward({sentence, summary, query}, grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model:zeroGradParameters()\n",
    "model:backward({sentence, summary, query}, grads)\n",
    "model:updateParameters(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "-- #query, #sentence, #summary, #pred_rougue, #labels\n",
    "-- query, sentence, #summary, pred_rougue, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "running\t1\t\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "...javierarceo/torch/install/share/lua/5.1/nn/Container.lua:67: \nIn 1 module of nn.Sequential:\nIn 1 module of nn.ParallelTable:\nIn 3 module of nn.Sequential:\n...iscojavierarceo/torch/install/share/lua/5.1/rnn/LSTM.lua:184: assertion failed!\nstack traceback:\n\t[C]: in function 'assert'\n\t...iscojavierarceo/torch/install/share/lua/5.1/rnn/LSTM.lua:184: in function '_updateGradInput'\n\t...eo/torch/install/share/lua/5.1/rnn/AbstractRecurrent.lua:59: in function 'updateGradInput'\n\t...avierarceo/torch/install/share/lua/5.1/rnn/Sequencer.lua:121: in function <...avierarceo/torch/install/share/lua/5.1/rnn/Sequencer.lua:106>\n\t[C]: in function 'xpcall'\n\t...javierarceo/torch/install/share/lua/5.1/nn/Container.lua:63: in function 'rethrowErrors'\n\t...avierarceo/torch/install/share/lua/5.1/nn/Sequential.lua:55: in function <...avierarceo/torch/install/share/lua/5.1/nn/Sequential.lua:50>\n\t[C]: in function 'xpcall'\n\t...javierarceo/torch/install/share/lua/5.1/nn/Container.lua:63: in function 'rethrowErrors'\n\t...erarceo/torch/install/share/lua/5.1/nn/ParallelTable.lua:19: in function 'updateGradInput'\n\t...\n\t[C]: in function 'xpcall'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:210: in function <...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:174>\n\t...ojavierarceo/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:389: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x0105660b90\n\nWARNING: If you see a stack trace below, it doesn't point to the place where this error occurred. Please use only the one above.\nstack traceback:\n\t[C]: in function 'error'\n\t...javierarceo/torch/install/share/lua/5.1/nn/Container.lua:67: in function 'rethrowErrors'\n\t...avierarceo/torch/install/share/lua/5.1/nn/Sequential.lua:88: in function 'backward'\n\t[string \"for i= 1, batch_size do...\"]:14: in main chunk\n\t[C]: in function 'xpcall'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:210: in function <...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:174>\n\t...ojavierarceo/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:389: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x0105660b90",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "...javierarceo/torch/install/share/lua/5.1/nn/Container.lua:67: \nIn 1 module of nn.Sequential:\nIn 1 module of nn.ParallelTable:\nIn 3 module of nn.Sequential:\n...iscojavierarceo/torch/install/share/lua/5.1/rnn/LSTM.lua:184: assertion failed!\nstack traceback:\n\t[C]: in function 'assert'\n\t...iscojavierarceo/torch/install/share/lua/5.1/rnn/LSTM.lua:184: in function '_updateGradInput'\n\t...eo/torch/install/share/lua/5.1/rnn/AbstractRecurrent.lua:59: in function 'updateGradInput'\n\t...avierarceo/torch/install/share/lua/5.1/rnn/Sequencer.lua:121: in function <...avierarceo/torch/install/share/lua/5.1/rnn/Sequencer.lua:106>\n\t[C]: in function 'xpcall'\n\t...javierarceo/torch/install/share/lua/5.1/nn/Container.lua:63: in function 'rethrowErrors'\n\t...avierarceo/torch/install/share/lua/5.1/nn/Sequential.lua:55: in function <...avierarceo/torch/install/share/lua/5.1/nn/Sequential.lua:50>\n\t[C]: in function 'xpcall'\n\t...javierarceo/torch/install/share/lua/5.1/nn/Container.lua:63: in function 'rethrowErrors'\n\t...erarceo/torch/install/share/lua/5.1/nn/ParallelTable.lua:19: in function 'updateGradInput'\n\t...\n\t[C]: in function 'xpcall'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:210: in function <...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:174>\n\t...ojavierarceo/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:389: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x0105660b90\n\nWARNING: If you see a stack trace below, it doesn't point to the place where this error occurred. Please use only the one above.\nstack traceback:\n\t[C]: in function 'error'\n\t...javierarceo/torch/install/share/lua/5.1/nn/Container.lua:67: in function 'rethrowErrors'\n\t...avierarceo/torch/install/share/lua/5.1/nn/Sequential.lua:88: in function 'backward'\n\t[string \"for i= 1, batch_size do...\"]:14: in main chunk\n\t[C]: in function 'xpcall'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:210: in function <...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:174>\n\t...ojavierarceo/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:389: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x0105660b90"
     ]
    }
   ],
   "source": [
    "for i= 1, batch_size do\n",
    "    print('running',i)\n",
    "    sentence = LongTensor(padZeros( {xtdm[xindices[i]]}, K_tokens) ):t()\n",
    "    summary = LongTensor({summaries[xindices[i]]}):t()\n",
    "    query = LongTensor(padZeros({qs}, 5)):t()\n",
    "\n",
    "    labels = Tensor({yrougue[xindices[i]]})\n",
    "    pred_rougue = Tensor({preds[xindices[i]]})\n",
    "\n",
    "    --- Backprop model\n",
    "    loss = loss + crit:forward(pred_rougue, labels)\n",
    "    local grads = crit:backward(pred_rougue, labels)\n",
    "    model:zeroGradParameters()\n",
    "    model:backward({sentence, summary, query}, grads)\n",
    "    model:updateParameters(learning_rate)\n",
    "end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
