{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "...Utils file loaded\t\n",
       "...running on CPU\t\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "require 'optim'\n",
    "require 'io'\n",
    "require 'torch'\n",
    "require 'nn'\n",
    "require 'rnn'\n",
    "require 'csvigo'\n",
    "require 'cutorch'\n",
    "require 'cunn'\n",
    "require 'cunnx'\n",
    "\n",
    "dl = require 'dataload'\n",
    "cmd = torch.CmdLine()\n",
    "\n",
    "cmd:option('--nepochs', 5, 'running for 50 epochs')\n",
    "cmd:option('--learning_rate', 1e-5, 'using a learning rate of 1e-5')\n",
    "cmd:option('--gamma', 0., 'Discount rate parameter in backprop step')\n",
    "cmd:option('--epsilon', 1., 'Random search rate')\n",
    "cmd:option('--cuts', 4, 'Discount rate parameter in backprop step')\n",
    "cmd:option('--base_explore_rate', 0.0, 'Base rate')\n",
    "cmd:option('--mem_size', 100, 'Memory size')\n",
    "cmd:option('--batch_size', 200,'Batch Size')\n",
    "cmd:option('--model','bow','BOW/LSTM option')\n",
    "cmd:option('--embeddingSize', 64,'Embedding dimension')\n",
    "cmd:option('--usecuda', false, 'running on cuda')\n",
    "cmd:option('--metric', \"f1\", 'Metric to learn')\n",
    "cmd:option('--n_samples', 500, 'Number of samples to use')\n",
    "cmd:option('--maxSummarySize', 300, 'Maximum summary size')\n",
    "cmd:option('--end_baserate', 5, 'Epoch number at which the base_rate ends')\n",
    "cmd:option('--K_tokens', 25, 'Maximum number of tokens for each sentence')\n",
    "cmd:option('--thresh', 0, 'Threshold operator')\n",
    "cmd:option('--n_backprops', 3, 'Number of times to backprop through the data')\n",
    "cmd:text()\n",
    "--- this retrieves the commands and stores them in opt.variable (e.g., opt.model)\n",
    " opt = cmd:parse(arg or {})\n",
    "\n",
    "dofile(\"utils.lua\")\n",
    "dofile(\"model_utils.lua\")\n",
    "\n",
    "input_path = '~/GitHub/DeepNLPQLearning/DO_NOT_UPLOAD_THIS_DATA/0-output/'\n",
    "query_fn = input_path .. 'queries_numtext.csv'\n",
    "query_file =  csvigo.load({path = query_fn, mode = \"large\", verbose = false})\n",
    "queries = padZeros(buildTermDocumentTable(query_file, nil), 5)\n",
    "\n",
    " pakistan = {\n",
    "        ['inputs'] = '2012_pakistan_garment_factory_fires_first_sentence_numtext2.csv',\n",
    "        ['nuggets'] ='pakistan_nuggets_numtext.csv',\n",
    "        ['query'] = queries[2],\n",
    "        ['query_name'] = 'pakistan'\n",
    "}\n",
    " aurora = {\n",
    "        ['inputs'] = '2012_aurora_shooting_first_sentence_numtext2.csv', \n",
    "        ['nuggets'] = 'aurora_nuggets_numtext.csv',\n",
    "        ['query'] = queries[3],\n",
    "        ['query_name'] = 'aurora'\n",
    "}\n",
    " sandy = {\n",
    "        ['inputs'] = 'hurricane_sandy_first_sentence_numtext2.csv',\n",
    "        ['nuggets'] ='sandy_nuggets_numtext.csv',\n",
    "        ['query'] = queries[7],\n",
    "        ['query_name'] = 'sandy'\n",
    "}\n",
    "\n",
    " inputs = {\n",
    "        aurora, \n",
    "        -- pakistan,\n",
    "        -- sandy\n",
    "}\n",
    "\n",
    "if opt.usecuda then\n",
    "    Tensor = torch.CudaTensor\n",
    "    LongTensor = torch.CudaLongTensor\n",
    "    ByteTensor = torch.CudaByteTensor\n",
    "    print(\"...running on GPU\")\n",
    "else\n",
    "    torch.setnumthreads(8)\n",
    "    Tensor = torch.Tensor\n",
    "    LongTensor = torch.LongTensor\n",
    "    ByteTensor = torch.ByteTensor\n",
    "    print(\"...running on CPU\")\n",
    "end\n",
    "\n",
    "delta = 1./(opt.nepochs/opt.cuts) \n",
    "optimParams = { learningRate = opt.learning_rate }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Initializing the model variables\n",
    "vocabSize, query_data = intialize_variables(query_file, inputs, \n",
    "                                            opt.n_samples, input_path, opt.K_tokens, \n",
    "                                            opt.maxSummarySize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Running bag-of-words model to learn f1\t\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = buildModel(opt.model, vocabSize, opt.embeddingSize, opt.metric, opt.usecuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query_id = 1\n",
    "memory, rougeRecall, rougePrecision, rougeF1, qValues = forwardpass(\n",
    "            query_data, query_id, \n",
    "            model, opt.epsilon, opt.gamma, \n",
    "            opt.metric, opt.thresh, opt.use_cuda\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params, gradParams = model:getParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function buildMemory(newinput, memory_hist, memsize, use_cuda)\n",
    "    local sentMemory = torch.cat(newinput[1][1]:double(), memory_hist[1][1]:double(), 1)\n",
    "    local queryMemory = torch.cat(newinput[1][2]:double(), memory_hist[1][2]:double(), 1)\n",
    "    local sumryMemory = torch.cat(newinput[1][3]:double(), memory_hist[1][3]:double(), 1)\n",
    "    local rewardMemory = torch.cat(newinput[2]:double(), memory_hist[2]:double(), 1)\n",
    "    local actionMemory = torch.cat(newinput[3], memory_hist[3], 1)\n",
    "    --- specifying rows to index \n",
    "    if sentMemory:size(1) >= memsize then\n",
    "        -- My hack for sampling based on non-zero rewards\n",
    "        local p = torch.abs(rewardMemory) / torch.abs(rewardMemory):sum()\n",
    "        -- local p = torch.ones(memsize) / memsize\n",
    "        local indxs = torch.multinomial(p, memsize, true)\n",
    "        local sentMemory = sentMemory:index(1, indxs)\n",
    "        local queryMemory = queryMemory:index(1, indxs)\n",
    "        local sumryMemory = sumryMemory:index(1, indxs)\n",
    "        local rewardMemory = rewardMemory:index(1, indxs)\n",
    "        local actionMemory = actionMemory:index(1, indxs)\n",
    "    end\n",
    "    --- Selecting random samples of the data\n",
    "    local inputMemory = {sentMemory, queryMemory, sumryMemory}\n",
    "    return {inputMemory, rewardMemory, actionMemory}\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fullmemory  = memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  1 : \n",
       "    {\n",
       "      1 : LongTensor - size: 499x25\n",
       "      2 : LongTensor - size: 499x5\n",
       "      3 : LongTensor - size: 499x300\n",
       "    }\n",
       "  2 : DoubleTensor - size: 499\n",
       "  3 : ByteTensor - size: 499x2\n",
       "}\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullmemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "opt.use_cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fullmemory = buildMemory(memory, fullmemory, opt.mem_size, opt.batch_size, opt.use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  1 : \n",
       "    {\n",
       "      1 : DoubleTensor - size: 998x25\n",
       "      2 : DoubleTensor - size: 998x5\n",
       "      3 : DoubleTensor - size: 998x300\n",
       "    }\n",
       "  2 : DoubleTensor - size: 998\n",
       "  3 : ByteTensor - size: 998x2\n",
       "}\n"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullmemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSECriterion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = backProp(memory, params, gradParams, optimParams, model, criterion, opt.batch_size, opt.mem_size, opt.use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  1 : \n",
       "    {\n",
       "      1 : LongTensor - size: 499x25\n",
       "      2 : LongTensor - size: 499x5\n",
       "      3 : LongTensor - size: 499x300\n",
       "    }\n",
       "  2 : DoubleTensor - size: 499\n",
       "  3 : ByteTensor - size: 499x2\n",
       "}\n"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = backProp(fullmemory, params, gradParams, optimParams, model, criterion, opt.batch_size, opt.mem_size, opt.use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
