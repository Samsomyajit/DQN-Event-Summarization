{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "...Utils file loaded\t\n",
       "<csv>\tparsing file: /Users/franciscojavierarceo/GitHub/DeepNLPQLearning/DO_NOT_UPLOAD_THIS_DATA/0-output/2012_aurora_shooting_first_sentence_numtext.csv\t\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<csv>\tparsing done\t\n",
       "<csv>\tparsing file: /Users/franciscojavierarceo/GitHub/DeepNLPQLearning/DO_NOT_UPLOAD_THIS_DATA/0-output/aurora_nuggets_numtext.csv\t\n",
       "<csv>\tparsing done\t\n",
       "<csv>\tparsing file: /Users/franciscojavierarceo/GitHub/DeepNLPQLearning/DO_NOT_UPLOAD_THIS_DATA/0-output/queries_numtext.csv\t\n",
       "<csv>\tparsing done\t\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "require 'torch'\n",
    "require 'nn'\n",
    "require 'rnn'\n",
    "require 'csvigo'\n",
    "require 'cutorch'\n",
    "require 'cunn'\n",
    "require 'cunnx'\n",
    "\n",
    "--- Loading utility script\n",
    "dofile(\"utils.lua\")\n",
    "dofile(\"model_utils.lua\")\n",
    "\n",
    "aurora_fn = '~/GitHub/DeepNLPQLearning/DO_NOT_UPLOAD_THIS_DATA/0-output/2012_aurora_shooting_first_sentence_numtext.csv'\n",
    "nugget_fn = '~/GitHub/DeepNLPQLearning/DO_NOT_UPLOAD_THIS_DATA/0-output/aurora_nuggets_numtext.csv'\n",
    "query_fn = '~/GitHub/DeepNLPQLearning/DO_NOT_UPLOAD_THIS_DATA/0-output/queries_numtext.csv'\n",
    "\n",
    "data_file = csvigo.load({path = aurora_fn, mode = \"large\"})\n",
    "nugget_file = csvigo.load({path = nugget_fn, mode = \"large\"})\n",
    "query_file =  csvigo.load({path = query_fn, mode = \"large\"})\n",
    "\n",
    "rK = 50\n",
    "batch_size = 10\n",
    "nepochs = 10\n",
    "print_every = 1\n",
    "embed_dim = 10\n",
    "learning_rate = 0.1\n",
    "usecuda = true\n",
    "\n",
    "cuts = 4.                  --- This is the number of cuts we want\n",
    "epsilon = 1.\n",
    "base_explore_rate = 0.1\n",
    "delta = 1./(nepochs/cuts) --- Only using epsilon greedy strategy for (nepochs/cuts)% of the epochs\n",
    "\n",
    "torch.manualSeed(420)\n",
    "\n",
    "function build_network(vocab_size, embed_dim)\n",
    "    local model = nn.Sequential()\n",
    "    :add(nn.LookupTableMaskZero(vocab_size, embed_dim)) -- will return a sequence-length x batch-size x embedDim tensor\n",
    "    :add(nn.SplitTable(1, embed_dim)) -- splits into a sequence-length table with batch-size x embedDim entries\n",
    "    :add(nn.Sequencer(nn.LSTM(embed_dim, embed_dim)))\n",
    "    :add(nn.SelectTable(-1)) -- selects last state of the LSTM\n",
    "    :add(nn.Linear(embed_dim, embed_dim)) -- map last state to a score for classification\n",
    "    :add(nn.ReLU())\n",
    "   return model\n",
    "end\n",
    "\n",
    "function build_model(vocab_size, embed_dim, outputSize, use_cuda)\n",
    "    local mod1 = build_network(vocab_size, embed_dim)\n",
    "    local mod2 = build_network(vocab_size, embed_dim)\n",
    "    local mod3 = build_network(vocab_size, embed_dim)\n",
    "\n",
    "    local mlp1 = nn.Sequential()\n",
    "    mlp1:add(nn.Linear(1, embed_dim))\n",
    "    mlp1:add(nn.ReLU())\n",
    "\n",
    "    local ParallelModel = nn.ParallelTable()\n",
    "    ParallelModel:add(mod1)\n",
    "    ParallelModel:add(mod2)\n",
    "    ParallelModel:add(mod3)\n",
    "    ParallelModel:add(mlp1)\n",
    "\n",
    "    local FinalMLP = nn.Sequential()\n",
    "    FinalMLP:add(ParallelModel)\n",
    "    FinalMLP:add(nn.JoinTable(2))\n",
    "    FinalMLP:add( nn.Linear(embed_dim * 4, outputSize) )\n",
    "\n",
    "    if use_cuda then\n",
    "        return FinalMLP:cuda()\n",
    "    else\n",
    "        return FinalMLP\n",
    "    end\n",
    "end\n",
    "\n",
    "vocab_sized = getVocabSize(data_file)                       --- getting length of dictionary\n",
    "vocab_sizeq = getVocabSize(query_file)                      --- getting length of dictionary\n",
    "vocab_size = math.max(vocab_sized, vocab_sizeq)\n",
    "\n",
    "queries = grabNsamples(query_file, #query_file-1, nil)      --- Extracting all queries\n",
    "nuggets = grabNsamples(nugget_file, #nugget_file-1, nil)    --- Extracting all samples\n",
    "maxseqlend = getMaxseq(data_file)                             --- Extracting maximum sequence length\n",
    "maxseqlenq = getMaxseq(query_file)                            --- Extracting maximum sequence length\n",
    "maxseqlen = math.max(maxseqlenq, maxseqlend)\n",
    "\n",
    "batchLSTM = build_model(vocab_size, embed_dim, 1, usecuda)\n",
    "crit = nn.MSECriterion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "nepochs = 1 \n",
    "qs = queries[3]\n",
    "x = data_file\n",
    "model = batchLSTM\n",
    "-- crit = crit\n",
    "-- epsilon\n",
    "-- delta\n",
    "mxl = maxseqlen\n",
    "-- base_explore_rate\n",
    "-- print_every\n",
    "-- nuggets\n",
    "-- learning_rate\n",
    "K = rK\n",
    "use_cuda = usecuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Running DQN-LSTM with the GPU\t\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "training model...\t\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if use_cuda then\n",
    "  Tensor = torch.CudaTensor\n",
    "  LongTensor = torch.CudaLongTensor\n",
    "  crit = crit:cuda()\n",
    "  print(\"Running DQN-LSTM with the GPU\")\n",
    "else\n",
    "  Tensor = torch.Tensor\n",
    "  LongTensor = torch.LongTensor\n",
    "  print(\"Running DQN-LSTM with the CPU\")\n",
    "end\n",
    "\n",
    "rscores, pscores, fscores = {}, {}, {}\n",
    "yrouge = torch.totable(torch.randn(#x))\n",
    "summary_list = populateOnes(#x, K)\n",
    "action_list = torch.totable(torch.round(torch.rand(#x)))\n",
    "preds_list = torch.totable(torch.round(torch.rand(#x)))\n",
    "print(\"training model...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pass\t\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "5\t0.55555555555556\t0\t1\t\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 0, sum(y)/len(y) = 6224/12510, {Recall = 0.000000, Precision = 0.000000, F1 = 0.000000}\t\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "pass\t\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "5\t0.55555555555556\t0\t1\t\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 1, sum(y)/len(y) = 6224/12510, {Recall = 0.000000, Precision = 0.000000, F1 = 0.000000}\t\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for epoch=0, nepochs, 1 do\n",
    "    loss = 0                    --- Compute a new MSE loss each time\n",
    "    --- Reset the rougue each time\n",
    "    r_t1 , p_t1, f_t1 = 0., 0., 0.\n",
    "    --- Looping over each bach of sentences for a given query\n",
    "    nbatches = torch.floor( #x / batch_size)\n",
    "    for minibatch = 1, nbatches do\n",
    "        if minibatch == 1 then          -- Need +1 to skip the first row\n",
    "            nstart = 2\n",
    "            nend = torch.round(batch_size * minibatch)\n",
    "        end\n",
    "        if minibatch == nbatches then \n",
    "            nstart = nend + 1\n",
    "            nend = #x\n",
    "        end\n",
    "        if minibatch > 1 and minibatch < nbatches then \n",
    "            nstart = nend + 1\n",
    "            nend = torch.round(batch_size * minibatch)\n",
    "        end\n",
    "        --- This step is processing the data\n",
    "        x_ss  = geti_n(x, nstart, nend)\n",
    "        xout  = grabNsamples(x_ss, 1, #x_ss)     --- Extracting N samples\n",
    "        print('pass')\n",
    "        xs  = padZeros(xout, mxl)                 --- Padding the data by the maximum length\n",
    "        qs2 = padZeros({qs}, 5)\n",
    "        qrep = repeatQuery(qs2[1], #xs)\n",
    "        preds = geti_n(preds_list, nstart, nend)\n",
    "        sumry_ss = buildSummary(preds, xs, 0)\n",
    "\n",
    "        sentences = LongTensor(xs):t()\n",
    "        summary = LongTensor(sumry_ss):t()\n",
    "        query = LongTensor(qrep):t()\n",
    "        actions = torch.Tensor(geti_n(action_list, nstart, nend)):resize(#xs, 1)\n",
    "        labels = torch.Tensor(geti_n(yrouge, nstart, nend))\n",
    "\n",
    "        print(actions:sum(), actions:mean(), actions:min(), actions:max())\n",
    "\n",
    "        if use_cuda then\n",
    "             actions =  actions:cuda()\n",
    "             labels = labels:cuda()\n",
    "        end\n",
    "        myPreds = model:forward({sentences, summary, query, actions})\n",
    "        loss = loss + crit:forward(myPreds, labels)\n",
    "        grads = crit:backward(myPreds, labels)\n",
    "        model:backward({sentences, summary, query, actions}, grads)\n",
    "        model:updateParameters(learning_rate)        -- Update parameters after each minibatch\n",
    "        model:zeroGradParameters()\n",
    "\n",
    "        if use_cuda then\n",
    "            myPreds = myPreds:double()\n",
    "        end\n",
    "\n",
    "        preds = policy(myPreds, epsilon, #xs)\n",
    "        --- Concatenating predictions into a summary\n",
    "        predsummary = buildPredSummary(preds, xs, K)\n",
    "        --- Initializing rouge metrics at time {t-1} and save scores\n",
    "        for i=1, #predsummary do\n",
    "            --- Calculating rouge scores; Call get_i_n() to cumulatively compute rouge\n",
    "            rscores[i] = rougeRecall(geti_n(predsummary, 1, i), nuggets, K) - r_t1\n",
    "            pscores[i] = rougePrecision(geti_n(predsummary, 1, i), nuggets, K) - p_t1\n",
    "            fscores[i] = rougeF1(geti_n(predsummary, 1, i), nuggets, K) - f_t1\n",
    "            r_t1, p_t1, f_t1 = rscores[i], pscores[i], fscores[i]\n",
    "        end\n",
    "        --- Updating change in rouge\n",
    "        yrouge = updateTable(yrouge, fscores, nstart)\n",
    "        preds_list = updateTable(preds_list, preds, nstart)\n",
    "        action_list = updateTable(action_list, torch.totable(actions:double()), nstart)\n",
    "        \n",
    "        --- Calculating last one to see actual last rouge, without delta\n",
    "        rscore = rougeRecall(predsummary, nuggets, K)\n",
    "        pscore = rougePrecision(predsummary, nuggets, K)\n",
    "        fscore = rougeF1(predsummary, nuggets, K)\n",
    "        \n",
    "        perf_string = string.format(\n",
    "            \"Epoch %i, sum(y)/len(y) = %i/%i, {Recall = %.6f, Precision = %.6f, F1 = %.6f}\", \n",
    "            epoch, sumTable(preds_list), #preds_list, rscore, pscore, fscore\n",
    "            )\n",
    "        print(perf_string)\n",
    "    end\n",
    "    epsilon = epsilon - delta           --- Decreasing the epsilon greedy strategy\n",
    "    if epsilon <= 0 then                --- leave a random exploration rate\n",
    "        epsilon = base_explore_rate\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  1 : 235\n",
       "  2 : 218\n",
       "  3 : 129\n",
       "  4 : 54\n",
       "  5 : 23\n",
       "  6 : 248\n",
       "  7 : 62\n",
       "  8 : 249\n",
       "  9 : 119\n",
       "}\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predsummary[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n2 = geti_n(nuggets, 2, #nuggets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  1 : 21\n",
       "  2 : 16\n",
       "  3 : 20\n",
       "  4 : 2\n",
       "  5 : 1\n",
       "  6 : 4\n",
       "  7 : 11\n",
       "  8 : 22\n",
       "  9 : 19\n",
       "}\n",
       "0\t\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unpackZeros(xs[1]), preds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       "   0\n",
       " 132\n",
       "[torch.CudaLongTensor of size 9]\n",
       "\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 9\n",
       "[torch.LongStorage of size 1]\n",
       "\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels:size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  1 : \n",
       "    {\n",
       "      1 : 235\n",
       "      2 : 218\n",
       "      3 : 129\n",
       "      4 : 54\n",
       "      5 : 23\n",
       "      6 : 248\n",
       "      7 : 62\n",
       "      8 : 249\n",
       "      9 : 119\n",
       "    }\n",
       "  2 : \n",
       "    {\n",
       "      1 : 329\n",
       "      2 : 334\n",
       "      3 : 332\n",
       "      4 : 23\n",
       "      5 : 335\n",
       "      6 : 129\n",
       "      7 : 333\n",
       "      8 : 330\n",
       "      9 : 119\n",
       "    }\n",
       "  3 : \n",
       "    {\n",
       "      1 : 76\n",
       "      2 : 406\n",
       "      3 : 450\n",
       "      4 : 537\n",
       "      5 : 535\n",
       "      6 : 534\n",
       "      7 : 536\n",
       "    }\n",
       "  4 : \n",
       "    {\n",
       "      1 : 659\n",
       "      2 : 661\n",
       "      3 : 474\n",
       "      4 : 660\n",
       "      5 : 83\n",
       "      6 : 663\n",
       "      7 : 57\n",
       "      8 : 658\n",
       "      9 : 120\n",
       "    }\n",
       "}\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predsummary[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.015711645101664\t\n"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rougeF1(predsummary[1], nuggets, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.015711645101664\t\n"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rougeF1(geti_n(predsummary, 1, 7)[1], nuggets, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9\t\n"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sumry_ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function buildPredSummary(pred_action, xs) \n",
    "    local predsummary = {}\n",
    "    --- This looks stupid but it's right because we have to retain\n",
    "    --- the tmp1 when it's not 1, so it's a running total\n",
    "    local tmp1 = {}\n",
    "    for i=1, #xs do\n",
    "        tmp = unpackZeros(xs[i])\n",
    "        if pred_action[i]== 1 then\n",
    "            predsummary[i] = tmp1\n",
    "        else\n",
    "            predsummary[i] = tmp1\n",
    "        end\n",
    "    end\n",
    "    return predsummary\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pxs = buildPredSummary(preds, xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  1 : 21\n",
       "  2 : 16\n",
       "  3 : 20\n",
       "  4 : 2\n",
       "  5 : 1\n",
       "  6 : 4\n",
       "  7 : 11\n",
       "  8 : 22\n",
       "  9 : 19\n",
       "}\n"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unpackZeros(xs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  1 : \n",
       "    {\n",
       "      1 : 21 16 20 2 1 4 11 22 19 9 14 20 25 24 23 24 8 5 7 17 3 12\n",
       "    }\n",
       "  2 : \n",
       "    {\n",
       "      1 : 235 218 129 54 23 248 62 249 119 224 245 23 57 262 5 238 174 242 216 265 83 231 266 250 257 54 265 62 221 263 258 223 244 247 1 229 224 265 241 100 220 1 243 264 3 129 1 272 1 228 264 3 236 132 237 255 225 263 24 230 261 1 228 232 240 270 3 129 1 24 260 1 252 240 270 3 236 54 265 239 24 67 28 227 266 250 257 192 222 1 129 83 269 253\n",
       "    }\n",
       "  3 : \n",
       "    {\n",
       "      1 : 329 334 332 23 335 129 333 330 119 321 63 336\n",
       "    }\n",
       "  4 : \n",
       "    {\n",
       "      1 : 20 437 438 119 432 192 24 359 436 434 3 433 129 431 224 440 429 140 3 435 83 430 439 441\n",
       "    }\n",
       "  5 : \n",
       "    {\n",
       "      1 : 76 406 450 537 535 534 536\n",
       "    }\n",
       "  6 : \n",
       "    {\n",
       "      1 : 659 661 474 660 83 663 57 658 120 656 666 657 668 664 119 24 129 665 3 24 232 669 662\n",
       "    }\n",
       "  7 : \n",
       "    {\n",
       "      1 : 708 709 590 712 714 1 76 710 83 1 57 53 3 129 520 321 707 716 1 711\n",
       "    }\n",
       "  8 : \n",
       "    {\n",
       "      1 : 835 24 225 24 495 836 828 830 3 138 1 831 837 83 24 829 23 24 827\n",
       "    }\n",
       "  9 : \n",
       "    {\n",
       "      1 : 132 1110 1109 28 1107 1108\n",
       "    }\n",
       "}\n"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0\t\n"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rougeF1(geti_n(sumry_ss, 1, 2), nuggets, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40740740740741\t\n"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rougePrecision({unpackZeros(xs[1]), unpackZeros(xs[2]), unpackZeros(xs[3])}, nuggets, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0\t{\n",
       "  1 : 21\n",
       "  2 : 16\n",
       "  3 : 20\n",
       "  4 : 2\n",
       "  5 : 1\n",
       "  6 : 4\n",
       "  7 : 11\n",
       "  8 : 22\n",
       "  9 : 19\n",
       "}\n"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[1], unpackZeros(xs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0\t\n"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out = {}\n",
    "out[1] = zero_or_x(preds[1], unpackZeros(xs[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out[2] =  zero_or_x(preds[2], unpackZeros(xs[2]))\n",
    "out[3] =  zero_or_x(preds[3], unpackZeros(xs[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp_preds  = {preds[1], preds[2], preds[3] }\n",
    "tmp_xs = {unpackZeros(xs[1]), unpackZeros(xs[2]), unpackZeros(xs[3])} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  1 : 21\n",
       "  2 : 16\n",
       "  3 : 20\n",
       "  4 : 2\n",
       "  5 : 1\n",
       "  6 : 4\n",
       "  7 : 11\n",
       "  8 : 22\n",
       "  9 : 19\n",
       "}\n"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_xs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function buildPredSummary(preds, xs)\n",
    "    local out = {}\n",
    "    for i=1, #xs do\n",
    "        if i == 1 then \n",
    "            out[i] = zero_or_x(preds[i], unpackZeros(xs[i]))\n",
    "        else \n",
    "            --- Update it by adding xs_i and out_{i-1}\n",
    "            out[i] =  zero_or_x(preds[i], unpackZeros(xs[i]))\n",
    "        end\n",
    "    end\n",
    "    return out\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tst = buildPredSummary(geti_n(tmp_preds, 1, 2), tmp_xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006514657980456\t\n"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rougeF1(tst, nuggets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22222222222222\t\n"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.38888888888889\t\n",
       "0.36842105263158\t\n",
       "0.35\t\n",
       "0.37037037037037\t\n",
       "0.41666666666667\t\n"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.40540540540541\t\n",
       "0.39473684210526\t\n",
       "0.38461538461538\t\n"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i=1, 9 do\n",
    "    print(rougePrecision(buildPredSummary(geti_n(preds, 1, i), geti_n(xout, 1, i)), nuggets))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
