{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- require 'torch'\n",
    "-- require 'gnuplot'\n",
    "-- require 'rnn'\n",
    "-- require 'image'\n",
    "-- Plot = require 'itorch.Plot'\n",
    "-- -- scatter plots\n",
    "-- plot = Plot():circle(torch.range(1, #err), \n",
    "--                 err, 'red', 'RMSE'):draw()\n",
    "-- plot:line(torch.range(1, #err), \n",
    "--                 err, 'red', ''):redraw()\n",
    "-- plot:title('Plot of Loss Function'):redraw()\n",
    "-- plot:xaxis('Iterations'):yaxis('Mean Squared Error'):redraw()\n",
    "-- plot:legend(true)\n",
    "-- plot:redraw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true\t\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "require 'rnn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "setences = torch.LongTensor{{0, 1, 3, 4}, {2, 1, 4, 3}}:t()\n",
    "query = torch.LongTensor{{0,0,3}, {1,3,2}}:t()\n",
    "actions = torch.round(torch.rand(2, 1))\n",
    "\n",
    "batch_size = 2\n",
    "vocab_size = 4\n",
    "embed_dim = 10\n",
    "outputSize = 1\n",
    "\n",
    "lstm1 = nn.Sequential()\n",
    "lstm1:add(nn.LookupTableMaskZero(vocab_size, embed_dim)) -- will return a sequence-length x batch-size x embedDim tensor\n",
    "lstm1:add(nn.SplitTable(1, embed_dim)) -- splits into a sequence-length table with batch-size x embedDim entries\n",
    "lstm1:add(nn.Sequencer(nn.LSTM(embed_dim, embed_dim)))\n",
    "lstm1:add(nn.SelectTable(-1)) -- selects last state of the LSTM\n",
    "lstm1:add(nn.Linear(embed_dim, embed_dim)) -- map last state to a score for classification\n",
    "\n",
    "lstm2 = nn.Sequential()\n",
    "lstm2:add(nn.LookupTableMaskZero(vocab_size, embed_dim)) -- will return a sequence-length x batch-size x embedDim tensor\n",
    "lstm2:add(nn.SplitTable(1, embed_dim)) -- splits into a sequence-length table with batch-size x embedDim entries\n",
    "lstm2:add(nn.Sequencer(nn.LSTM(embed_dim, embed_dim)))\n",
    "lstm2:add(nn.SelectTable(-1)) -- selects last state of the LSTM\n",
    "lstm2:add(nn.Linear(embed_dim, embed_dim)) -- map last state to a score for classification\n",
    "\n",
    "mlp = nn.Sequential()\n",
    "mlp:add(nn.Linear(1, embed_dim))\n",
    "mlp:add(nn.Linear(embed_dim, embed_dim))\n",
    "\n",
    "p = nn.ParallelTable()\n",
    "p:add(lstm1)\n",
    "p:add(lstm2)\n",
    "p:add(mlp)\n",
    "-- p:add(nn.JoinTable(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds = p:forward{setences, query, actions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  1 : DoubleTensor - size: 2x10\n",
       "  2 : DoubleTensor - size: 2x10\n",
       "  3 : DoubleTensor - size: 2x10\n",
       "}\n"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0.2023 -0.0254  0.2167 -0.1812  0.1803  0.0503  0.0483  0.3013  0.1813  0.2048\n",
       " 0.2138 -0.0696  0.2277 -0.1014  0.1545 -0.1174 -0.0189  0.2190  0.2132  0.2489\n",
       " 0.0898  0.2730 -0.0784 -0.2054  0.1235  0.0971 -0.2758 -0.2466 -0.1786 -0.2513\n",
       " 0.1065  0.2532 -0.1920 -0.2423  0.0342  0.1038 -0.1618 -0.3776 -0.1152 -0.2435\n",
       "-0.0756 -0.3303  0.0474 -0.2289  0.9439 -0.1772  0.4493 -0.2737 -0.0069 -0.7997\n",
       "-0.1611  0.5144 -0.6788 -0.1739  0.6920 -0.1723  0.3786 -0.4050 -0.3312 -0.4858\n",
       "[torch.DoubleTensor of size 6x10]\n",
       "\n"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.JoinTable(1):forward(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  1 : DoubleTensor - size: 2x10\n",
       "  2 : DoubleTensor - size: 2x10\n",
       "  3 : DoubleTensor - size: 2x10\n",
       "}\n",
       "-0.7041 -0.0438 -0.9827 -0.3147 -0.4290 -0.4894  1.4600 -0.4579 -0.6010  0.6675\n",
       "-0.0038 -1.4663  0.2207  1.1462 -1.7009 -0.2944 -0.1020 -0.8489  1.6325  0.2674\n",
       "-1.0698 -0.7443 -0.1849  0.7632 -1.1265 -0.1791 -0.7780  0.0638  0.1231 -1.7345\n",
       "-0.7185 -0.7374  0.8541 -1.2832 -0.2002  1.3910  1.5279  0.3525  0.1520  1.5547\n",
       "-0.9035  0.8848  1.0488  2.0511  0.8989 -0.0558  0.9162 -0.4268  1.0088 -2.2604\n",
       " 0.3769  0.5136  0.4464 -0.4765 -1.8698  1.6459  1.7850  0.0352 -0.5310 -0.0497\n",
       "[torch.DoubleTensor of size 6x10]\n",
       "\n"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 10)\n",
    "y = torch.randn(2, 10)\n",
    "z = torch.randn(2, 10)\n",
    "\n",
    "tst = {x, y, z}\n",
    "\n",
    "print(tst)\n",
    "print(nn.JoinTable(1):forward(tst))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
