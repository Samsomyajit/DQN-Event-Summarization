{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true\t\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "require 'rnn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  1 : DoubleTensor - size: 2x10\n",
       "  2 : DoubleTensor - size: 2x10\n",
       "  3 : DoubleTensor - size: 2x10\n",
       "  4 : DoubleTensor - size: 2x10\n",
       "}\n",
       "2.5052e+152\n",
       "2.8376e+152\n",
       "[torch.DoubleTensor of size 2x1]\n",
       "\n"
      ]
     },
     "execution_count": 776,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function buildLSTM(vsize, edim, odim)\n",
    "    local lstm = nn.Sequential()\n",
    "    lstm:add(nn.LookupTableMaskZero(vsize, edim))\n",
    "    lstm:add(nn.SplitTable(1, edim))\n",
    "    lstm:add(nn.Sequencer(nn.LSTM(edim, edim)))\n",
    "    lstm:add(nn.SelectTable(-1))\n",
    "    return lstm\n",
    "end\n",
    "\n",
    "sentences = torch.LongTensor{{0, 1, 3, 4}, {2, 1, 4, 3}}:t()\n",
    "summary = torch.LongTensor{{0, 0, 1, 4}, {0, 2, 3, 1}}:t()\n",
    "query = torch.LongTensor{{0, 0, 4, 3}, {0, 1, 3, 2}}:t()\n",
    "actions = torch.round(torch.rand(2, 1))\n",
    "yrouge = torch.rand(2, 1)\n",
    "\n",
    "batch_size = 2\n",
    "vocab_size = 4\n",
    "embed_dim = 10\n",
    "outputSize = 1\n",
    "\n",
    "lstm1 = buildLSTM(vocab_size, embed_dim, outputSize)\n",
    "lstm2 = buildLSTM(vocab_size, embed_dim, outputSize)\n",
    "lstm3 = buildLSTM(vocab_size, embed_dim, outputSize)\n",
    "\n",
    "mlp1 = nn.Sequential()\n",
    "mlp1:add(nn.Linear(1, embed_dim))\n",
    "\n",
    "ParallelModel = nn.ParallelTable()\n",
    "ParallelModel:add(lstm1)\n",
    "ParallelModel:add(lstm2)\n",
    "ParallelModel:add(lstm3)\n",
    "ParallelModel:add(mlp1)\n",
    "\n",
    "sep_preds = ParallelModel:forward({sentences, summary, query, actions})\n",
    "\n",
    "print(sep_preds)\n",
    "\n",
    "-- Collapsing the predictions and stacking the *columns* together\n",
    "preds = nn.JoinTable(2):forward(sep_preds)\n",
    "input_dim = preds:size()[2]\n",
    "output_dim = 1\n",
    "FinalMLP = nn.Sequential()\n",
    "FinalMLP:add(nn.Linear(input_dim, output_dim))\n",
    "\n",
    "finpreds = fullMLP:forward(preds)\n",
    "print(finpreds)\n",
    "\n",
    "--- Backprop the final layer\n",
    "criterion = nn.MSECriterion()\n",
    "err = criterion:forward(finpreds, yrouge)\n",
    "grads = criterion:backward(finpreds, yrouge)\n",
    "fullMLP:backward(preds, grads)\n",
    "FinalMLP:updateParameters(0.05)\n",
    "FinalMLP:zeroGradParameters()\n",
    "\n",
    "-- Backprop the first layer ??? but how??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Think this finally works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function buildLSTM(vsize, edim, odim)\n",
    "    local lstm = nn.Sequential()\n",
    "    lstm:add(nn.LookupTableMaskZero(vsize, edim))\n",
    "    lstm:add(nn.SplitTable(1, edim))\n",
    "    lstm:add(nn.Sequencer(nn.LSTM(edim, edim)))\n",
    "    lstm:add(nn.SelectTable(-1))\n",
    "    return lstm\n",
    "end\n",
    "\n",
    "sentences = torch.LongTensor{{0, 1, 3, 4}, {2, 1, 4, 3}}:t()\n",
    "summary = torch.LongTensor{{0, 0, 1, 4}, {0, 2, 3, 1}}:t()\n",
    "query = torch.LongTensor{{0, 0, 4, 3}, {0, 1, 3, 2}}:t()\n",
    "actions = torch.round(torch.rand(2, 1))\n",
    "yrouge = torch.rand(2, 1)\n",
    "\n",
    "batch_size = 2\n",
    "vocab_size = 4\n",
    "embed_dim = 10\n",
    "outputSize = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm1 = buildLSTM(vocab_size, embed_dim, outputSize)\n",
    "lstm2 = buildLSTM(vocab_size, embed_dim, outputSize)\n",
    "lstm3 = buildLSTM(vocab_size, embed_dim, outputSize)\n",
    "\n",
    "mlp1 = nn.Sequential()\n",
    "mlp1:add(nn.Linear(1, embed_dim))\n",
    "\n",
    "ParallelModel = nn.ParallelTable()\n",
    "ParallelModel:add(lstm1)\n",
    "ParallelModel:add(lstm2)\n",
    "ParallelModel:add(lstm3)\n",
    "ParallelModel:add(mlp1)\n",
    "\n",
    "FinalMLP = nn.Sequential()\n",
    "FinalMLP:add(ParallelModel)\n",
    "FinalMLP:add(nn.JoinTable(2))\n",
    "FinalMLP:add( nn.Linear(embed_dim * 4, 1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = FinalMLP:forward({sentences, summary, query, actions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSECriterion()\n",
    "err = criterion:forward(preds, yrouge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grads = criterion:backward(preds, yrouge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0.1675\n",
       " 0.2317\n",
       "[torch.DoubleTensor of size 2x1]\n",
       "\n",
       "-0.1694\n",
       " 0.0662\n",
       "[torch.DoubleTensor of size 2x1]\n",
       "\n"
      ]
     },
     "execution_count": 809,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "...javierarceo/torch/install/share/lua/5.1/nn/Container.lua:67: \nIn 1 module of nn.Sequential:\nIn 1 module of nn.ParallelTable:\nIn 3 module of nn.Sequential:\n...iscojavierarceo/torch/install/share/lua/5.1/rnn/LSTM.lua:184: assertion failed!\nstack traceback:\n\t[C]: in function 'assert'\n\t...iscojavierarceo/torch/install/share/lua/5.1/rnn/LSTM.lua:184: in function '_updateGradInput'\n\t...eo/torch/install/share/lua/5.1/rnn/AbstractRecurrent.lua:59: in function 'updateGradInput'\n\t...avierarceo/torch/install/share/lua/5.1/rnn/Sequencer.lua:121: in function <...avierarceo/torch/install/share/lua/5.1/rnn/Sequencer.lua:106>\n\t[C]: in function 'xpcall'\n\t...javierarceo/torch/install/share/lua/5.1/nn/Container.lua:63: in function 'rethrowErrors'\n\t...avierarceo/torch/install/share/lua/5.1/nn/Sequential.lua:55: in function <...avierarceo/torch/install/share/lua/5.1/nn/Sequential.lua:50>\n\t[C]: in function 'xpcall'\n\t...javierarceo/torch/install/share/lua/5.1/nn/Container.lua:63: in function 'rethrowErrors'\n\t...erarceo/torch/install/share/lua/5.1/nn/ParallelTable.lua:19: in function 'updateGradInput'\n\t...\n\t[C]: in function 'xpcall'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:210: in function <...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:174>\n\t...ojavierarceo/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:389: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x0101c99b90\n\nWARNING: If you see a stack trace below, it doesn't point to the place where this error occurred. Please use only the one above.\nstack traceback:\n\t[C]: in function 'error'\n\t...javierarceo/torch/install/share/lua/5.1/nn/Container.lua:67: in function 'rethrowErrors'\n\t...avierarceo/torch/install/share/lua/5.1/nn/Sequential.lua:88: in function 'f'\n\t[string \"local f = function() return FinalMLP:backward...\"]:1: in main chunk\n\t[C]: in function 'xpcall'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:210: in function <...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:174>\n\t...ojavierarceo/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:389: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x0101c99b90",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "...javierarceo/torch/install/share/lua/5.1/nn/Container.lua:67: \nIn 1 module of nn.Sequential:\nIn 1 module of nn.ParallelTable:\nIn 3 module of nn.Sequential:\n...iscojavierarceo/torch/install/share/lua/5.1/rnn/LSTM.lua:184: assertion failed!\nstack traceback:\n\t[C]: in function 'assert'\n\t...iscojavierarceo/torch/install/share/lua/5.1/rnn/LSTM.lua:184: in function '_updateGradInput'\n\t...eo/torch/install/share/lua/5.1/rnn/AbstractRecurrent.lua:59: in function 'updateGradInput'\n\t...avierarceo/torch/install/share/lua/5.1/rnn/Sequencer.lua:121: in function <...avierarceo/torch/install/share/lua/5.1/rnn/Sequencer.lua:106>\n\t[C]: in function 'xpcall'\n\t...javierarceo/torch/install/share/lua/5.1/nn/Container.lua:63: in function 'rethrowErrors'\n\t...avierarceo/torch/install/share/lua/5.1/nn/Sequential.lua:55: in function <...avierarceo/torch/install/share/lua/5.1/nn/Sequential.lua:50>\n\t[C]: in function 'xpcall'\n\t...javierarceo/torch/install/share/lua/5.1/nn/Container.lua:63: in function 'rethrowErrors'\n\t...erarceo/torch/install/share/lua/5.1/nn/ParallelTable.lua:19: in function 'updateGradInput'\n\t...\n\t[C]: in function 'xpcall'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:210: in function <...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:174>\n\t...ojavierarceo/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:389: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x0101c99b90\n\nWARNING: If you see a stack trace below, it doesn't point to the place where this error occurred. Please use only the one above.\nstack traceback:\n\t[C]: in function 'error'\n\t...javierarceo/torch/install/share/lua/5.1/nn/Container.lua:67: in function 'rethrowErrors'\n\t...avierarceo/torch/install/share/lua/5.1/nn/Sequential.lua:88: in function 'f'\n\t[string \"local f = function() return FinalMLP:backward...\"]:1: in main chunk\n\t[C]: in function 'xpcall'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:210: in function <...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:174>\n\t...ojavierarceo/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:389: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x0101c99b90"
     ]
    }
   ],
   "source": [
    "FinalMLP:backward({sentences, summary, query, actions}, grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "...javierarceo/torch/install/share/lua/5.1/nn/Container.lua:67: \nIn 1 module of nn.Sequential:\nbad argument #2 to '?' (out of range at /Users/franciscojavierarceo/torch/pkg/torch/generic/Tensor.c:890)\nstack traceback:\n\t[C]: at 0x01e2d7e0\n\t[C]: in function '__index'\n\t...erarceo/torch/install/share/lua/5.1/nn/ParallelTable.lua:19: in function 'updateGradInput'\n\t...scojavierarceo/torch/install/share/lua/5.1/nn/Module.lua:31: in function <...scojavierarceo/torch/install/share/lua/5.1/nn/Module.lua:29>\n\t[C]: in function 'xpcall'\n\t...javierarceo/torch/install/share/lua/5.1/nn/Container.lua:63: in function 'rethrowErrors'\n\t...avierarceo/torch/install/share/lua/5.1/nn/Sequential.lua:88: in function 'backward'\n\t[string \"criterion = nn.MSECriterion()...\"]:4: in main chunk\n\t[C]: in function 'xpcall'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:210: in function <...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:174>\n\t...ojavierarceo/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:389: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x0101c99b90\n\nWARNING: If you see a stack trace below, it doesn't point to the place where this error occurred. Please use only the one above.\nstack traceback:\n\t[C]: in function 'error'\n\t...javierarceo/torch/install/share/lua/5.1/nn/Container.lua:67: in function 'rethrowErrors'\n\t...avierarceo/torch/install/share/lua/5.1/nn/Sequential.lua:88: in function 'backward'\n\t[string \"criterion = nn.MSECriterion()...\"]:4: in main chunk\n\t[C]: in function 'xpcall'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:210: in function <...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:174>\n\t...ojavierarceo/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:389: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x0101c99b90",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "...javierarceo/torch/install/share/lua/5.1/nn/Container.lua:67: \nIn 1 module of nn.Sequential:\nbad argument #2 to '?' (out of range at /Users/franciscojavierarceo/torch/pkg/torch/generic/Tensor.c:890)\nstack traceback:\n\t[C]: at 0x01e2d7e0\n\t[C]: in function '__index'\n\t...erarceo/torch/install/share/lua/5.1/nn/ParallelTable.lua:19: in function 'updateGradInput'\n\t...scojavierarceo/torch/install/share/lua/5.1/nn/Module.lua:31: in function <...scojavierarceo/torch/install/share/lua/5.1/nn/Module.lua:29>\n\t[C]: in function 'xpcall'\n\t...javierarceo/torch/install/share/lua/5.1/nn/Container.lua:63: in function 'rethrowErrors'\n\t...avierarceo/torch/install/share/lua/5.1/nn/Sequential.lua:88: in function 'backward'\n\t[string \"criterion = nn.MSECriterion()...\"]:4: in main chunk\n\t[C]: in function 'xpcall'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:210: in function <...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:174>\n\t...ojavierarceo/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:389: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x0101c99b90\n\nWARNING: If you see a stack trace below, it doesn't point to the place where this error occurred. Please use only the one above.\nstack traceback:\n\t[C]: in function 'error'\n\t...javierarceo/torch/install/share/lua/5.1/nn/Container.lua:67: in function 'rethrowErrors'\n\t...avierarceo/torch/install/share/lua/5.1/nn/Sequential.lua:88: in function 'backward'\n\t[string \"criterion = nn.MSECriterion()...\"]:4: in main chunk\n\t[C]: in function 'xpcall'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:210: in function <...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:174>\n\t...ojavierarceo/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:389: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x0101c99b90"
     ]
    }
   ],
   "source": [
    "FinalMLP:updateParameters(0.05)\n",
    "FinalMLP:zeroGradParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sep_preds = ParallelModel:forward({sentences, summary, query, actions})\n",
    "\n",
    "print(sep_preds)\n",
    "\n",
    "-- Collapsing the predictions and stacking the *columns* together\n",
    "preds = nn.JoinTable(2):forward(sep_preds)\n",
    "input_dim = preds:size()[2]\n",
    "output_dim = 1\n",
    "FinalMLP = nn.Sequential()\n",
    "FinalMLP:add(nn.Linear(input_dim, output_dim))\n",
    "\n",
    "finpreds = fullMLP:forward(preds)\n",
    "print(finpreds)\n",
    "\n",
    "--- Backprop the final layer\n",
    "criterion = nn.MSECriterion()\n",
    "err = criterion:forward(finpreds, yrouge)\n",
    "grads = criterion:backward(finpreds, yrouge)\n",
    "FinalMLP:backward(preds, grads)\n",
    "FinalMLP:updateParameters(0.05)\n",
    "FinalMLP:zeroGradParameters()\n",
    "\n",
    "-- Backprop the first layer ??? but how??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP join table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "...javierarceo/torch/install/share/lua/5.1/nn/Container.lua:67: \nIn 3 module of nn.ParallelTable:\n...javierarceo/torch/install/share/lua/5.1/nn/JoinTable.lua:24: attempt to get length of local 'input' (a nil value)\nstack traceback:\n\t...javierarceo/torch/install/share/lua/5.1/nn/JoinTable.lua:24: in function <...javierarceo/torch/install/share/lua/5.1/nn/JoinTable.lua:21>\n\t[C]: in function 'xpcall'\n\t...javierarceo/torch/install/share/lua/5.1/nn/Container.lua:63: in function 'rethrowErrors'\n\t...erarceo/torch/install/share/lua/5.1/nn/ParallelTable.lua:12: in function 'forward'\n\t[string \"x = torch.rand(9,1)...\"]:19: in main chunk\n\t[C]: in function 'xpcall'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:210: in function <...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:174>\n\t...ojavierarceo/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:389: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x0101c99b90\n\nWARNING: If you see a stack trace below, it doesn't point to the place where this error occurred. Please use only the one above.\nstack traceback:\n\t[C]: in function 'error'\n\t...javierarceo/torch/install/share/lua/5.1/nn/Container.lua:67: in function 'rethrowErrors'\n\t...erarceo/torch/install/share/lua/5.1/nn/ParallelTable.lua:12: in function 'forward'\n\t[string \"x = torch.rand(9,1)...\"]:19: in main chunk\n\t[C]: in function 'xpcall'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:210: in function <...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:174>\n\t...ojavierarceo/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:389: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x0101c99b90",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "...javierarceo/torch/install/share/lua/5.1/nn/Container.lua:67: \nIn 3 module of nn.ParallelTable:\n...javierarceo/torch/install/share/lua/5.1/nn/JoinTable.lua:24: attempt to get length of local 'input' (a nil value)\nstack traceback:\n\t...javierarceo/torch/install/share/lua/5.1/nn/JoinTable.lua:24: in function <...javierarceo/torch/install/share/lua/5.1/nn/JoinTable.lua:21>\n\t[C]: in function 'xpcall'\n\t...javierarceo/torch/install/share/lua/5.1/nn/Container.lua:63: in function 'rethrowErrors'\n\t...erarceo/torch/install/share/lua/5.1/nn/ParallelTable.lua:12: in function 'forward'\n\t[string \"x = torch.rand(9,1)...\"]:19: in main chunk\n\t[C]: in function 'xpcall'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:210: in function <...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:174>\n\t...ojavierarceo/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:389: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x0101c99b90\n\nWARNING: If you see a stack trace below, it doesn't point to the place where this error occurred. Please use only the one above.\nstack traceback:\n\t[C]: in function 'error'\n\t...javierarceo/torch/install/share/lua/5.1/nn/Container.lua:67: in function 'rethrowErrors'\n\t...erarceo/torch/install/share/lua/5.1/nn/ParallelTable.lua:12: in function 'forward'\n\t[string \"x = torch.rand(9,1)...\"]:19: in main chunk\n\t[C]: in function 'xpcall'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:210: in function <...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:174>\n\t...ojavierarceo/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:389: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x0101c99b90"
     ]
    }
   ],
   "source": [
    "x = torch.rand(9,1)\n",
    "y = torch.rand(5,1)\n",
    "\n",
    "hidden_size = 10\n",
    "output_size = 5\n",
    "\n",
    "mlp1 = nn.Sequential()\n",
    "mlp1:add(nn.Linear(x:size()[2], output_size) )\n",
    "\n",
    "mlp2 = nn.Sequential()\n",
    "mlp2:add(nn.Linear(y:size()[2], output_size) )\n",
    "\n",
    "p = nn.ParallelTable()\n",
    "p:add(mlp1)\n",
    "p:add(mlp2)\n",
    "p:add(nn.JoinTable(2))\n",
    "\n",
    "z = {x, y}\n",
    "prds = p:forward(z)\n",
    "\n",
    "-- mymodule = nn.JoinTable(2)\n",
    "-- print(mymodule:forward(prds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP and LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  1 : DoubleTensor - size: 2x10\n",
       "  2 : DoubleTensor - size: 2x10\n",
       "}\n"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.LongTensor{{0, 1, 3, 4}, {2, 1, 4, 3}}:t()\n",
    "z = torch.rand(2, 1)\n",
    "y = torch.rand(2, 1)\n",
    "\n",
    "batch_size = 2\n",
    "vocab_size = 4\n",
    "embed_dim = 10\n",
    "outputSize = 1\n",
    "\n",
    "lstm = nn.Sequential()\n",
    "lstm:add(nn.LookupTableMaskZero(vocab_size, embed_dim)) -- returns a sequence-length x batch-size x embed_dim tensor\n",
    "lstm:add(nn.SplitTable(1, embed_dim)) -- splits into a sequence-length table with batch-size x embed_dim entries\n",
    "lstm:add(nn.Sequencer(nn.LSTM(embed_dim, embed_dim)))\n",
    "lstm:add(nn.SelectTable(-1)) -- selects last state of the LSTM\n",
    "\n",
    "mlp = nn.Sequential()\n",
    "mlp:add(nn.Linear(1, embed_dim))\n",
    "\n",
    "p = nn.ParallelTable()\n",
    "p:add(lstm)\n",
    "p:add(mlp)\n",
    "-- p:add(nn.JoinTable(2))\n",
    "\n",
    "ipreds = p:forward{x, z}\n",
    "print(ipreds)\n",
    "\n",
    "-- Collapsing the predictions\n",
    "preds = nn.JoinTable(2):forward(p:forward{x, z})\n",
    "input_dim = preds:size()[2]\n",
    "output_dim = 1\n",
    "fullMLP = nn.Sequential()\n",
    "fullMLP:add(nn.Linear(input_dim, output_dim))\n",
    "\n",
    "finpreds = fullMLP:forward(preds)\n",
    "\n",
    "criterion = nn.MSECriterion()\n",
    "err = criterion:forward(finpreds, y)\n",
    "grads = criterion:backward(finpreds, y)\n",
    "fullMLP:backward(preds, grads)\n",
    "fullMLP:updateParameters(0.05)\n",
    "fullMLP:zeroGradParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  1 : DoubleTensor - size: 2x10\n",
       "  2 : DoubleTensor - size: 2x10\n",
       "}\n"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipreds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "...ierarceo/torch/install/share/lua/5.1/nn/MSECriterion.lua:13: attempt to call field 'new' (a nil value)\nstack traceback:\n\t...ierarceo/torch/install/share/lua/5.1/nn/MSECriterion.lua:13: in function 'forward'\n\t[string \"crits = nn.MSECriterion()...\"]:3: in main chunk\n\t[C]: in function 'xpcall'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:210: in function <...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:174>\n\t...ojavierarceo/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:389: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x0101c99b90",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "...ierarceo/torch/install/share/lua/5.1/nn/MSECriterion.lua:13: attempt to call field 'new' (a nil value)\nstack traceback:\n\t...ierarceo/torch/install/share/lua/5.1/nn/MSECriterion.lua:13: in function 'forward'\n\t[string \"crits = nn.MSECriterion()...\"]:3: in main chunk\n\t[C]: in function 'xpcall'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:210: in function <...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:174>\n\t...ojavierarceo/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:389: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x0101c99b90"
     ]
    }
   ],
   "source": [
    "crits = nn.MSECriterion()\n",
    "-- crits = nn.SequencerCriterion(crit2)\n",
    "err = crits:forward(ipreds, {x, z} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grads2 = crit2:backward(preds, {x, z} )\n",
    "p:backward(preds, grads2)\n",
    "p:updateParameters(0.05)\n",
    "p:zeroGradParameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try it a different way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function buildLSTM(vsize, edim, odim)\n",
    "    local lstm = nn.Sequential()\n",
    "    lstm:add(nn.LookupTableMaskZero(vsize, edim))\n",
    "    lstm:add(nn.SplitTable(1, edim))\n",
    "    lstm:add(nn.Sequencer(nn.LSTM(edim, edim)))\n",
    "    lstm:add(nn.SelectTable(-1))\n",
    "    lstm:add(nn.Linear(edim, odim))\n",
    "    return lstm\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "vocab_size = 4\n",
    "output_dim = 1\n",
    "embed_dim = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm1 = buildLSTM(vocab_size, embed_dim, output_dim)\n",
    "lstm2 = buildLSTM(vocab_size, embed_dim, output_dim)\n",
    "lstm3 = buildLSTM(vocab_size, embed_dim, output_dim)\n",
    "\n",
    "mlp = nn.Sequential()\n",
    "mlp:add(nn.Linear(1, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = torch.LongTensor{{0, 1, 3, 4}, {2, 1, 4, 3}}\n",
    "summary = torch.LongTensor{{0, 0, 1, 4}, {0, 2, 3, 1}}\n",
    "query = torch.LongTensor{{0, 0, 0, 3}, {0, 1, 3, 2}}\n",
    "actions = torch.round(torch.rand(2, 1))\n",
    "ys = torch.rand(2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0  1  3  4\n",
       " 2  1  4  3\n",
       "[torch.LongTensor of size 2x4]\n",
       "\n",
       " 0  0  1  4\n",
       " 0  2  3  1\n",
       "[torch.LongTensor of size 2x4]\n",
       "\n",
       " 0  0  0  3\n",
       " 0  1  3  2\n",
       "[torch.LongTensor of size 2x4]\n",
       "\n",
       " 0\n",
       " 0\n",
       "[torch.DoubleTensor of size 2x1]\n",
       "\n",
       " 0.2916\n",
       " 0.1514\n",
       "[torch.DoubleTensor of size 2x1]\n",
       "\n"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences, summary, query, actions, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.7758 -0.2990 -0.1692 -1.7618  1.6158 -0.1298\n",
       " 0.6905  1.1564 -0.2393 -0.8849  1.3835  0.4618\n",
       "[torch.DoubleTensor of size 2x6]\n",
       "\n"
      ]
     },
     "execution_count": 551,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.SelectTable(-1):forward(nn.SplitTable(1, 6):forward(nn.LookupTableMaskZero(4, 6):forward(sentences:t())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.2566\n",
       "-0.4127\n",
       "[torch.DoubleTensor of size 2x1]\n",
       "\n"
      ]
     },
     "execution_count": 552,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm1:forward(sentences:t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ss = torch.cat(sentences, summary, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0  1  3  4\n",
       " 2  1  4  3\n",
       " 0  0  1  4\n",
       " 0  2  3  1\n",
       "[torch.LongTensor of size 4x4]\n",
       "\n"
      ]
     },
     "execution_count": 658,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,.,.) = \n",
       "  0  0\n",
       "  0  3\n",
       "  0  1\n",
       "  3  2\n",
       "[torch.LongTensor of size 1x4x2]\n",
       "\n"
      ]
     },
     "execution_count": 657,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query:resize(1, 4, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  1 : LongTensor - size: 4\n",
       "  2 : LongTensor - size: 4\n",
       "  3 : LongTensor - size: 4\n",
       "  4 : LongTensor - size: 4\n",
       "}\n"
      ]
     },
     "execution_count": 656,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.SplitTable(1, 1):forward(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vsize = 4\n",
    "edim = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = nn.ParallelTable() \n",
    "c:add(nn.Sequential())\n",
    "c:add(nn.LookupTableMaskZero(vsize, edim))\n",
    "c:add(nn.SplitTable(1, edim))\n",
    "c:add(nn.Sequencer(nn.LSTM(edim, edim)))\n",
    "c:add(nn.SelectTable(-1))\n",
    "\n",
    "p = nn.ParallelTable()\n",
    "p:add(nn.Sequential())\n",
    "p:add(nn.LookupTableMaskZero(vsize, edim))\n",
    "p:add(nn.SplitTable(1, edim))\n",
    "p:add(nn.Sequencer(nn.LSTM(edim, edim)))\n",
    "p:add(nn.SelectTable(-1))\n",
    "\n",
    "\n",
    "mlp = nn.Sequential()\n",
    "mlp:add(nn.SplitTable(1, 1))\n",
    "-- mlp:add(c)\n",
    "-- mlp:add(p)\n",
    "-- mlp:add(nn.JoinTable(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred = mlp:forward(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0  1  3  4\n",
       " 2  1  4  3\n",
       " 0  0  1  4\n",
       " 0  2  3  1\n",
       "[torch.LongTensor of size 4x4]\n",
       "\n",
       "{\n",
       "  1 : LongTensor - size: 4\n",
       "  2 : LongTensor - size: 4\n",
       "  3 : LongTensor - size: 4\n",
       "  4 : LongTensor - size: 4\n",
       "}\n"
      ]
     },
     "execution_count": 661,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm1 = buildLSTM(vocab_size, embed_dim, output_dim)\n",
    "lstm2 = buildLSTM(vocab_size, embed_dim, output_dim)\n",
    "lstm3 = buildLSTM(vocab_size, embed_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = nn.ParallelTable()\n",
    "p:add(lstm1)\n",
    "p:add(lstm2)\n",
    "\n",
    "net = nn.Sequential()\n",
    "net:add(p)\n",
    "-- net:add(nn.JoinTable(2))\n",
    "-- net:add(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net:forward({sentence, summary})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net:forward({sentence, summary, query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0.5559\n",
       " 0.4333\n",
       " 0.3092\n",
       "[torch.DoubleTensor of size 3]\n",
       "\n"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.36783858107572\t\n"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = nn.ParallelTable()      -- The two Tensor slices go through two different Linear\n",
    "c:add(nn.Linear(10, 3))     -- Layers in Parallel\n",
    "c:add(nn.Linear(10, 7))\n",
    "\n",
    "p = nn.ParallelTable()      -- These tables go through two more linear layers separately\n",
    "p:add(nn.Linear(3, 2))\n",
    "p:add(nn.Linear(7, 1))\n",
    "\n",
    "mlp = nn.Sequential()       -- Create a network that takes a Tensor as input\n",
    "mlp:add(nn.SplitTable(2))\n",
    "mlp:add(c)                  -- Outputing a table with 2 elements\n",
    "mlp:add(p)\n",
    "mlp:add(nn.JoinTable(1))    -- Finally, the tables are joined together and output.\n",
    "\n",
    "pred = mlp:forward(torch.randn(10, 2))\n",
    "print(pred)\n",
    "\n",
    "x = torch.ones(10, 2)\n",
    "y = torch.Tensor(3)\n",
    "y:copy(x:select(2, 1):narrow(1, 1, 3))\n",
    "pred = mlp:forward(x)\n",
    "\n",
    "criterion = nn.MSECriterion()\n",
    "local err = criterion:forward(pred, y)\n",
    "local gradCriterion = criterion:backward(pred, y)\n",
    "mlp:zeroGradParameters()\n",
    "mlp:backward(x, gradCriterion)\n",
    "mlp:updateParameters(0.05)\n",
    "\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  1 : DoubleTensor - size: 10\n",
       "  2 : DoubleTensor - size: 10\n",
       "}\n"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.SplitTable(2):forward(torch.randn(10,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
