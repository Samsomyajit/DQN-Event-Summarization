{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "require 'torch'\n",
    "require 'nn'\n",
    "require 'rnn'\n",
    "require 'csvigo'\n",
    "require 'cutorch'\n",
    "require 'cunn'\n",
    "require 'cunnx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = 'lstm'\n",
    "nepochs = 200\n",
    "K_tokens = 20\n",
    "J_sentences = 10\n",
    "batch_size = 100\n",
    "thresh = 0.00\n",
    "embed_dim = 100\n",
    "learning_rate = 0.1\n",
    "print_every = 1\n",
    "epsilon = 1\n",
    "usecuda = false\n",
    "epsilon = 1\n",
    "cuts = 4\n",
    "base_explore_rate = 0.0\n",
    "skip_rate = 0.\n",
    "metric = 'f1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "...Utils file loaded\t\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "--- Loading utility script\n",
    "dofile(\"utils.lua\")\n",
    "dofile(\"model_utils.lua\")\n",
    "\n",
    "torch.manualSeed(420)\n",
    "math.randomseed(420)\n",
    "\n",
    "data_path = '~/GitHub/DeepNLPQLearning/DO_NOT_UPLOAD_THIS_DATA/0-output/'\n",
    "\n",
    "query_fn = data_path .. 'queries_numtext.csv'\n",
    "query_file =  csvigo.load({path = query_fn, mode = \"large\", verbose = false})\n",
    "queries = buildTermDocumentTable(query_file, nil)\n",
    "\n",
    "pakistan = {\n",
    "        ['inputs'] = '2012_pakistan_garment_factory_fires_first_sentence_numtext2.csv',\n",
    "        ['nuggets'] ='pakistan_nuggets_numtext.csv',\n",
    "        ['query'] = queries[2],\n",
    "        ['query_name'] = 'pakistan'\n",
    "}\n",
    "aurora = {\n",
    "        ['inputs'] = '2012_aurora_shooting_first_sentence_numtext2.csv', \n",
    "        ['nuggets'] = 'aurora_nuggets_numtext.csv',\n",
    "        ['query'] = queries[3],\n",
    "        ['query_name'] = 'aurora'\n",
    "}\n",
    "sandy = {\n",
    "        ['inputs'] = 'hurricane_sandy_first_sentence_numtext2.csv',\n",
    "        ['nuggets'] ='sandy_nuggets_numtext.csv',\n",
    "        ['query'] = queries[7],\n",
    "        ['query_name'] = 'sandy'\n",
    "}\n",
    "\n",
    "inputs = {\n",
    "        aurora, \n",
    "        -- pakistan,\n",
    "        -- sandy\n",
    "    }\n",
    "--- Only using epsilon greedy strategy for (nepochs/cuts)% of the epochs\n",
    "delta = 1./(nepochs/cuts) \n",
    "crit = nn.MSECriterion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_path, query_file, batch_size, nepochs, inputs = data_path, query_file, batch_size, nepochs, inputs\n",
    "model, crit, thresh, embed_dim, epsilon, delta = model, crit, thresh, embed_dim, epsilon, delta\n",
    "base_explore_rate, print_every, learning_rate, J_sentences, K_tokens, use_cuda =base_explore_rate, print_every,learning_rate, J_sentences, K_tokens, usecuda\n",
    "skiprate, emetric =  skip_rate, metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "...running on CPU\t\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if use_cuda then\n",
    "  Tensor = torch.CudaTensor\n",
    "  LongTensor = torch.CudaLongTensor\n",
    "  crit = crit:cuda()\n",
    "  print(\"...running on GPU\")\n",
    "else\n",
    "  Tensor = torch.Tensor\n",
    "  LongTensor = torch.LongTensor\n",
    "  print(\"...running on CPU\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 0\n",
    "maxseqlen = 0\n",
    "maxseqlenq = getMaxseq(query_file)\n",
    "\n",
    "action_query_list = {}\n",
    "yrougue_query_list = {}\n",
    "pred_query_list = {}\n",
    "\n",
    "for query_id = 1, #inputs do\n",
    "    input_fn = inputs[query_id]['inputs']\n",
    "    nugget_fn = inputs[query_id]['nuggets']\n",
    "\n",
    "    input_file = csvigo.load({path = input_path .. input_fn, mode = \"large\", verbose = false})\n",
    "    nugget_file = csvigo.load({path = input_path .. nugget_fn, mode = \"large\", verbose = false})\n",
    "\n",
    "    vocab_sized = getVocabSize(input_file)\n",
    "    vocab_sizeq = getVocabSize(query_file)\n",
    "    vocab_size = math.max(vocab_size, vocab_sized, vocab_sizeq)\n",
    "\n",
    "    maxseqlend = getMaxseq(input_file)\n",
    "    maxseqlen = math.max(maxseqlen, maxseqlenq, maxseqlend)\n",
    "    action_list = torch.totable(torch.round(torch.rand(#input_file)))\n",
    "    action_list[1] = 0\n",
    "\n",
    "    --- initialize the query specific lists\n",
    "    action_query_list[query_id] = action_list\n",
    "    yrougue_query_list[query_id] = torch.totable(torch.randn(#input_file, 2)) --- Actual\n",
    "    pred_query_list[query_id] = torch.totable(torch.zeros(#input_file, 2))    --- Predicted\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- model  = build_model(model, vocab_size, embed_dim, 2, use_cuda)\n",
    "model  = build_model2(model, vocab_size, embed_dim, 1, use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "minibatch = 1\n",
    "query_id = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  1 : 2\n",
       "  2 : 1\n",
       "  3 : 6\n",
       "  4 : 25\n",
       "  5 : 8\n",
       "  6 : 24\n",
       "  7 : 15\n",
       "  8 : 17\n",
       "  9 : 20\n",
       "  10 : 24\n",
       "  11 : 10\n",
       "  12 : 7\n",
       "  13 : 9\n",
       "  14 : 12\n",
       "}\n",
       "{\n",
       "  1 : 2\n",
       "  2 : 1\n",
       "  3 : 6\n",
       "  4 : 25\n",
       "  5 : 8\n",
       "  6 : 24\n",
       "  7 : 15\n",
       "  8 : 17\n",
       "  9 : 20\n",
       "  10 : 24\n",
       "  11 : 10\n",
       "  12 : 7\n",
       "  13 : 9\n",
       "  14 : 12\n",
       "}\n",
       " 1\n",
       "[torch.DoubleTensor of size 1]\n",
       "\n"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unpackZeros(torch.totable(summary:t()[1])),\n",
    "unpackZeros(torch.totable(sentences:t()[1])),\n",
    "actions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tst2 = buildCurrentSummary(torch.totable(actions1), xout, K_tokens * J_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "}\n"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function buildCurrentSummary(preds, xs, K)\n",
    "    local out = {}\n",
    "    for i = 1, #xs do\n",
    "        if i == 1 then\n",
    "            out[i] = {}\n",
    "        elseif i == 2 then\n",
    "            tmp = x_or_pass(preds[i-1], unpackZeros(xs[i-1])) \n",
    "            out[i] = getLastKElements(tmp, K)\n",
    "        else \n",
    "            local tmp = tableConcat(out[i-2], x_or_pass(preds[i-1], unpackZeros(xs[i-1])))\n",
    "            out[i] = getLastKElements(tmp, K)\n",
    "        end \n",
    "    end\n",
    "    return out \n",
    "end\n",
    "\n",
    "function buildPredSummary2(preds, xs, K)\n",
    "    --- This function is used to map the token indices to extract the summary\n",
    "    --- and produceds {token_id, 0, token_id} from any given *selected* sentence\n",
    "    local out = {}\n",
    "    for i=1, #xs do\n",
    "        if i == 1 then \n",
    "            out[i] = x_or_pass(preds[i][1], unpackZeros(xs[i]))\n",
    "        else \n",
    "            --- Update it by adding xs_i and out_{i-1}\n",
    "            local tmp = tableConcat(out[i-1], x_or_pass(preds[i][1], unpackZeros(xs[i])))\n",
    "            --- Getting the last K tokens because we want to keep last K tokens\n",
    "            out[i] =  getLastKElements(tmp, K)\n",
    "        end\n",
    "    end\n",
    "    return out\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tst1 = buildPredSummary(torch.totable(actions1), xout, K_tokens * J_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "atmp = geti_n(torch.totable(actions1),4, 6 )\n",
    "xtmp = geti_n(xout, 4, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  1 : \n",
       "    {\n",
       "      1 : 1\n",
       "    }\n",
       "  2 : \n",
       "    {\n",
       "      1 : 0\n",
       "    }\n",
       "  3 : \n",
       "    {\n",
       "      1 : 1\n",
       "    }\n",
       "}\n",
       "{\n",
       "  1 : \n",
       "    {\n",
       "      1 : 4801\n",
       "      2 : 17\n",
       "      3 : 143\n",
       "      4 : 124\n",
       "      5 : 79\n",
       "      6 : 837\n",
       "      7 : 1452\n",
       "      8 : 9\n",
       "      9 : 24\n",
       "      10 : 426\n",
       "      11 : 266\n",
       "      12 : 124\n",
       "      13 : 5872\n",
       "      14 : 9\n",
       "      15 : 432\n",
       "      16 : 79\n",
       "      17 : 24\n",
       "      18 : 1052\n",
       "      19 : 8742\n",
       "      20 : 9\n",
       "    }\n",
       "  2 : \n",
       "    {\n",
       "      1 : 193\n",
       "      2 : 192\n",
       "      3 : 11054\n",
       "      4 : 40\n",
       "      5 : 4095\n",
       "      6 : 20\n",
       "      7 : 932\n",
       "      8 : 1\n",
       "      9 : 3252\n",
       "      10 : 29\n",
       "      11 : 47\n",
       "      12 : 138\n",
       "      13 : 131\n",
       "      14 : 9\n",
       "      15 : 1038\n",
       "      16 : 24\n",
       "    }\n",
       "  3 : \n",
       "    {\n",
       "      1 : 78\n",
       "      2 : 174\n",
       "      3 : 7192\n",
       "      4 : 24\n",
       "      5 : 4801\n",
       "      6 : 1014\n",
       "      7 : 99\n",
       "      8 : 2872\n",
       "      9 : 20\n",
       "      10 : 24\n",
       "      11 : 5876\n",
       "      12 : 1052\n",
       "      13 : 7283\n",
       "      14 : 24\n",
       "    }\n",
       "}\n"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atmp, xtmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  1 : 1\n",
       "}\n"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atmp[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "}\n"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_or_pass(atmp[1], xtmp[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1\n",
       "[torch.DoubleTensor of size 1]\n",
       "\n",
       "{\n",
       "  1 : table: 0x782e39b0\n",
       "  2 : table: 0x60e5f750\n",
       "  3 : table: 0x22b8e210\n",
       "}\n"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions1[1], buildPredSummary(geti_n(torch.totable(actions1),1, 3 ), \n",
    "    geti_n(xout, 1, 3), K_tokens * J_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1\n",
       "[torch.DoubleTensor of size 1]\n",
       "\n",
       "{\n",
       "  1 : \n",
       "    {\n",
       "      1 : 9191\n",
       "      2 : 156\n",
       "      3 : 116\n",
       "      4 : 1900\n",
       "      5 : 945\n",
       "      6 : 27\n",
       "      7 : 54\n",
       "      8 : 392\n",
       "      9 : 5368\n",
       "      10 : 1\n",
       "      11 : 420\n",
       "      12 : 1014\n",
       "      13 : 3655\n",
       "      14 : 945\n",
       "    }\n",
       "}\n"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions1[1], buildPredSummary(torch.totable(actions1)[1], {xout[1]}, K_tokens * J_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0\n",
       "[torch.DoubleTensor of size 1]\n",
       "\n",
       "{\n",
       "  1 : table: 0x5fbbdce8\n",
       "}\n"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions0[1], buildPredSummary(torch.totable(actions0)[1], {xout[1]}, K_tokens * J_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0\n",
       "[torch.DoubleTensor of size 1]\n",
       "\n",
       " 1\n",
       "[torch.DoubleTensor of size 1]\n",
       "\n"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary1[1], actions1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  1 : 2\n",
       "  2 : 1\n",
       "  3 : 6\n",
       "  4 : 25\n",
       "  5 : 8\n",
       "  6 : 24\n",
       "  7 : 15\n",
       "  8 : 17\n",
       "  9 : 20\n",
       "  10 : 24\n",
       "  11 : 10\n",
       "  12 : 7\n",
       "  13 : 9\n",
       "  14 : 12\n",
       "}\n"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "  1 : 2\n",
       "  2 : 1\n",
       "  3 : 6\n",
       "  4 : 25\n",
       "  5 : 8\n",
       "  6 : 24\n",
       "  7 : 15\n",
       "  8 : 17\n",
       "  9 : 20\n",
       "  10 : 24\n",
       "  11 : 10\n",
       "  12 : 7\n",
       "  13 : 9\n",
       "  14 : 12\n",
       "}\n"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "executing deterministic policy\t\n"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "}\n"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "}\n"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "executing deterministic policy\t\n"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "}\n"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "}\n"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "executing deterministic policy\t\n"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "  1 : 1117\n",
       "  2 : 44\n",
       "  3 : 156\n",
       "  4 : 202\n",
       "  5 : 77\n",
       "  6 : 1445\n",
       "  7 : 1447\n",
       "  8 : 1\n",
       "  9 : 143\n",
       "  10 : 79\n",
       "  11 : 3261\n",
       "  12 : 614\n",
       "  13 : 445\n",
       "  14 : 54\n",
       "  15 : 1426\n",
       "  16 : 349\n",
       "  17 : 124\n",
       "  18 : 54\n",
       "  19 : 1476\n",
       "  20 : 8\n",
       "}\n"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "  1 : 1117\n",
       "  2 : 44\n",
       "  3 : 156\n",
       "  4 : 202\n",
       "  5 : 77\n",
       "  6 : 1445\n",
       "  7 : 1447\n",
       "  8 : 1\n",
       "  9 : 143\n",
       "  10 : 79\n",
       "  11 : 3261\n",
       "  12 : 614\n",
       "  13 : 445\n",
       "  14 : 54\n",
       "  15 : 1426\n",
       "  16 : 349\n",
       "  17 : 124\n",
       "  18 : 54\n",
       "  19 : 1476\n",
       "  20 : 8\n",
       "}\n"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "executing deterministic policy\t\n"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "}\n"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "}\n"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "executing deterministic policy\t\n"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "}\n"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "}\n"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "executing deterministic policy\t\n"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "  1 : 24\n",
       "  2 : 2529\n",
       "  3 : 20\n",
       "  4 : 24\n",
       "  5 : 7799\n",
       "  6 : 167\n",
       "  7 : 12229\n",
       "  8 : 9\n",
       "  9 : 156\n",
       "  10 : 266\n",
       "  11 : 70\n",
       "  12 : 1853\n",
       "  13 : 79\n",
       "  14 : 1136\n",
       "  15 : 54\n",
       "  16 : 507\n",
       "  17 : 1\n",
       "  18 : 54\n",
       "  19 : 5020\n",
       "  20 : 79\n",
       "}\n"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "  1 : 24\n",
       "  2 : 2529\n",
       "  3 : 20\n",
       "  4 : 24\n",
       "  5 : 7799\n",
       "  6 : 167\n",
       "  7 : 12229\n",
       "  8 : 9\n",
       "  9 : 156\n",
       "  10 : 266\n",
       "  11 : 70\n",
       "  12 : 1853\n",
       "  13 : 79\n",
       "  14 : 1136\n",
       "  15 : 54\n",
       "  16 : 507\n",
       "  17 : 1\n",
       "  18 : 54\n",
       "  19 : 5020\n",
       "  20 : 79\n",
       "}\n"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "executing deterministic policy\t\n"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "}\n"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "}\n"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "executing deterministic policy\t\n"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "}\n"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "}\n"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "executing deterministic policy\t\n"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "}\n"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "}\n"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "executing deterministic policy\t\n"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = 0.                    --- Compute a new MSE loss each time\n",
    "--- Grabbing all of the input data\n",
    "qs = inputs[query_id]['query']\n",
    "input_file = csvigo.load({path = input_path .. inputs[query_id]['inputs'], mode = \"large\", verbose = false})\n",
    "nugget_file = csvigo.load({path = input_path .. inputs[query_id]['nuggets'], mode = \"large\", verbose = false})\n",
    "nuggets = buildTermDocumentTable(nugget_file, nil)\n",
    "\n",
    "--- Building table of all of the input sentences\n",
    "xtdm  = buildTermDocumentTable(input_file, K_tokens)\n",
    "\n",
    "--- Extracting the query specific summaries, actions, and rougue\n",
    "action_list = action_query_list[query_id]\n",
    "yrouge = yrougue_query_list[query_id] \n",
    "preds = pred_query_list[query_id] \n",
    "\n",
    "nbatches = torch.floor( #input_file / batch_size)\n",
    "\n",
    "--- Initializing rouge metrics at time {t-1} and save scores, reset each new epoch\n",
    "r_t1 , p_t1, f_t1 = 0., 0., 0.\n",
    "for minibatch = 1, nbatches do\n",
    "    if minibatch == 1 then\n",
    "        -- Need to skip the first row because it says \"Text\"\n",
    "        nstart = 2\n",
    "        nend = torch.round(batch_size * minibatch)\n",
    "    end\n",
    "    if minibatch > 1 and minibatch < nbatches then \n",
    "        nstart = nend + 1\n",
    "        nend = torch.round(batch_size * minibatch)\n",
    "    end\n",
    "    if minibatch == nbatches then \n",
    "        nstart = nend + 1\n",
    "        nend = #input_file\n",
    "    end\n",
    "    --- Processing the input data to get {query, input_sentences, summary, actions}\n",
    "    action_out = geti_n(action_list, nstart, nend)                \n",
    "    xout  = geti_n(xtdm, nstart, nend)    --- Extracting the mini-batch from our input sentences\n",
    "    xs  = padZeros(xout, K_tokens)    --- Padding the data by K tokens because we chose this as the max value\n",
    "    qs2 = padZeros({qs}, 5)\n",
    "    qrep = repeatTable(qs2[1], #xs)\n",
    "--     sumry_list = padZeros(buildCurrentSummary(action_out, xout, K_tokens * J_sentences), K_tokens * J_sentences)\n",
    "    sumry_list = padZeros(buildPredSummary(action_out, xout, K_tokens * J_sentences), K_tokens * J_sentences)\n",
    "    --- Inserting data into tensors\n",
    "    summary0 = LongTensor(sumry_list):t()\n",
    "    sentences = LongTensor(xs):t()\n",
    "    query = LongTensor(qrep):t()\n",
    "    actions0 = Tensor(action_out):resize(#xs, 1)\n",
    "    \n",
    "    print(unpackZeros(torch.totable(summary0:t()[1])))\n",
    "    --- Forward pass to estimate expected rougue)\n",
    "    pred_rougue0 = model:forward({sentences, summary0, query, actions0})\n",
    "    \n",
    "    actions1 = torch.abs(actions0 - 1):resize(#xs, 1)\n",
    "    --- need to update this to rerun it\n",
    "    action_list = updateTable(action_list, torch.totable(actions1), nstart)\n",
    "    \n",
    "    sumry_list = padZeros(buildPredSummary(action_out, xout, J_sentences * K_tokens), J_sentences * K_tokens)\n",
    "    summary1 = LongTensor(sumry_list):t()\n",
    "\n",
    "    print(unpackZeros(torch.totable(summary1:t()[1])))\n",
    "    --- Score under oposite action\n",
    "    pred_rougue1 = model:forward({sentences, summary1, query, actions1})\n",
    "\n",
    "    pred_rougue = Tensor(pred_rougue0):cat(Tensor(pred_rougue1), 2)\n",
    "    pred_rougue = torch.totable(pred_rougue)\n",
    "\n",
    "    pred_rougue, labels, opt_action = score_model2(pred_rougue, xout, epsilon, thresh, skiprate)\n",
    "    \n",
    "    --- Note setting the skip_rate = 0 means no random skipping of delta calculation\n",
    "--     labels, opt_action = score_model(torch.totable(pred_rougue), xout, epsilon, thresh, skiprate, emetric)\n",
    "\n",
    "    -- Updating our bookkeeping tables\n",
    "    yrouge = updateTable(yrouge, torch.totable(labels), nstart)\n",
    "    preds =  updateTable(preds, pred_rougue, nstart)\n",
    "    action_list = updateTable(action_list, opt_action, nstart)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "[string \"local f = function() return pred_rougue1[1], ...\"]:1: attempt to index global 'pred_rougue1' (a nil value)\nstack traceback:\n\t[string \"local f = function() return pred_rougue1[1], ...\"]:1: in function 'f'\n\t[string \"local f = function() return pred_rougue1[1], ...\"]:1: in main chunk\n\t[C]: in function 'xpcall'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:210: in function <...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:174>\n\t...ojavierarceo/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:389: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x0103081b90",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "[string \"local f = function() return pred_rougue1[1], ...\"]:1: attempt to index global 'pred_rougue1' (a nil value)\nstack traceback:\n\t[string \"local f = function() return pred_rougue1[1], ...\"]:1: in function 'f'\n\t[string \"local f = function() return pred_rougue1[1], ...\"]:1: in main chunk\n\t[C]: in function 'xpcall'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:210: in function <...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:174>\n\t...ojavierarceo/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...vierarceo/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t...ojavierarceo/torch/install/share/lua/5.1/itorch/main.lua:389: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x0103081b90"
     ]
    }
   ],
   "source": [
    "pred_rougue1[1], pred_rougue0[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_rougue = Tensor(pred_rougue0):cat(Tensor(pred_rougue1), 2)\n",
    "sumTable(opt_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "--- Rerunning on the scoring on the full data and rescoring cumulatively\n",
    "--- Execute policy and evaluation based on our E[ROUGUE] after all of the minibatches\n",
    "    --- Notice that pred_rougue gives us our optimal action by returning\n",
    "    ---  E[ROUGUE | Select ] > E[ROUGUE | Skip]\n",
    "predsummary = buildPredSummary(action_list, xtdm, nil)\n",
    "predsummary = predsummary[#predsummary]\n",
    "\n",
    "rscore = rougeRecall({predsummary}, nuggets)\n",
    "pscore = rougePrecision({predsummary}, nuggets)\n",
    "fscore = rougeF1({predsummary}, nuggets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0\t0\t0\t\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rscore, pscore, fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "--- Updating variables\n",
    "action_query_list[query_id] = action_list\n",
    "yrougue_query_list[query_id] = yrouge\n",
    "pred_query_list[query_id] = preds\n",
    "\n",
    "if (epoch % print_every)==0 then\n",
    "    perf_string = string.format(\n",
    "        \"Epoch %i, epsilon = %.3f, sum(y)/len(y) = %i/%i, {Recall = %.6f, Precision = %.6f, F1 = %.6f}, query = %s\", \n",
    "        epoch, epsilon, sumTable(action_list), #action_list, rscore, pscore, fscore, inputs[query_id]['query_name']\n",
    "        )\n",
    "    print(perf_string)\n",
    "end\n",
    "\n",
    "--- creating the indices we want\n",
    "xindices = {}\n",
    "for i=1, 100 do\n",
    "    xindices[i] = math.random(2, #xtdm)\n",
    "end\n",
    "--- Have to skip over stupid header\n",
    "xout = getIndices(xtdm, xindices)\n",
    "action_out = getIndices(action_list, xindices)\n",
    "labels = Tensor(getIndices(yrouge, xindices)):resize(#xout, 1)\n",
    "pred_rougue = Tensor(getIndices(preds, xindices)):resize(#xout, 1)\n",
    "\n",
    "xs  = padZeros(xout, K_tokens)    --- Padding the data by K tokens because we chose this as the max value\n",
    "qs2 = padZeros({qs}, 5)\n",
    "qrep = repeatTable(qs2[1], #xs)\n",
    "\n",
    "sumry_list = buildCurrentSummary(action_out, xs, K_tokens * J_sentences)\n",
    "-- local sumry_list = buildPredSummary(action_out, xs, K_tokens * J_sentences)\n",
    "summary = LongTensor(padZeros(sumry_list, K_tokens * J_sentences)):t()\n",
    "\n",
    "--- Inserting data into tensors            \n",
    "sentences = LongTensor(xs):t()\n",
    "query = LongTensor(qrep):t()\n",
    "\n",
    "--- Backprop model\n",
    "loss = loss + crit:forward(pred_rougue, labels)\n",
    "grads = crit:backward(pred_rougue, labels)\n",
    "model:zeroGradParameters()\n",
    "model:backward({sentences, summary, query}, grads)\n",
    "model:updateParameters(learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summary:t()[1]:sum(), unpackZeros(torch.totable(sentences:t()[1])), query:t()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.totable(pred_rougue)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
