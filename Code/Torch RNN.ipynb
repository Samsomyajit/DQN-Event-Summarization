{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Plot = require 'itorch.Plot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "...Utils file loaded\t\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<csv>\tparsing file: /Users/franciscojavierarceo/GitHub/DeepNLPQLearning/DO_NOT_UPLOAD_THIS_DATA/0-output/2012_aurora_shooting_first_sentence_numtext2.csv\t\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<csv>\tparsing done\t\n",
       "<csv>\tparsing file: /Users/franciscojavierarceo/GitHub/DeepNLPQLearning/DO_NOT_UPLOAD_THIS_DATA/0-output/aurora_nuggets_numtext.csv\t\n",
       "<csv>\tparsing done\t\n",
       "<csv>\tparsing file: /Users/franciscojavierarceo/GitHub/DeepNLPQLearning/DO_NOT_UPLOAD_THIS_DATA/0-output/queries_numtext.csv\t\n",
       "<csv>\tparsing done\t\n",
       "<csv>\tparsing file: /Users/franciscojavierarceo/GitHub/DeepNLPQLearning/DO_NOT_UPLOAD_THIS_DATA/0-output/2012_aurora_sentence_numtext2.csv\t\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<csv>\tparsing done\t\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "require 'torch'\n",
    "require 'nn'\n",
    "require 'rnn'\n",
    "require 'csvigo'\n",
    "require 'cutorch'\n",
    "require 'cunn'\n",
    "require 'cunnx'\n",
    "\n",
    "--- Loading utility script\n",
    "dofile(\"utils.lua\")\n",
    "dofile(\"model_utils.lua\")\n",
    "\n",
    "aurora_fn = '~/GitHub/DeepNLPQLearning/DO_NOT_UPLOAD_THIS_DATA/0-output/2012_aurora_shooting_first_sentence_numtext2.csv'\n",
    "nugget_fn = '~/GitHub/DeepNLPQLearning/DO_NOT_UPLOAD_THIS_DATA/0-output/aurora_nuggets_numtext.csv'\n",
    "query_fn = '~/GitHub/DeepNLPQLearning/DO_NOT_UPLOAD_THIS_DATA/0-output/queries_numtext.csv'\n",
    "auroras_fn = '~/GitHub/DeepNLPQLearning/DO_NOT_UPLOAD_THIS_DATA/0-output/2012_aurora_sentence_numtext2.csv'\n",
    "\n",
    "data_file = csvigo.load({path = aurora_fn, mode = \"large\"})\n",
    "nugget_file = csvigo.load({path = nugget_fn, mode = \"large\"})\n",
    "query_file =  csvigo.load({path = query_fn, mode = \"large\"})\n",
    "sent_file =  csvigo.load({path = auroras_fn, mode = \"large\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Running BOW model\t\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mymodel = 'bow'\n",
    "rK = 100\n",
    "batch_size = 500\n",
    "nepochs = 1\n",
    "print_every = 1\n",
    "embed_dim = 10\n",
    "learning_rate = 0.00001\n",
    "usecuda=false\n",
    "\n",
    "epsilon = 0.1\n",
    "cuts = 4.                  --- This is the number of cuts we want\n",
    "base_explore_rate = 0.1\n",
    "delta = 1./(nepochs/cuts) --- Only using epsilon greedy strategy for (nepochs/cuts)% of the epochs\n",
    "\n",
    "torch.manualSeed(420)\n",
    "\n",
    "vocab_sized = getVocabSize(data_file)                       --- getting length of dictionary\n",
    "vocab_sizeq = getVocabSize(query_file)                      --- getting length of dictionary\n",
    "vocab_sizes = getVocabSize(sent_file)                      --- getting length of dictionary\n",
    "vocab_size = math.max(vocab_sized, vocab_sizeq, vocab_sizes)\n",
    "\n",
    "queries = grabNsamples(query_file, #query_file-1, nil)      --- Extracting all queries\n",
    "nuggets = grabNsamples(nugget_file, #nugget_file-1, nil)    --- Extracting all samples\n",
    "maxseqlend = getMaxseq(data_file)                             --- Extracting maximum sequence length\n",
    "maxseqlenq = getMaxseq(query_file)                            --- Extracting maximum sequence length\n",
    "maxseqlen = math.max(maxseqlenq, maxseqlend)\n",
    "\n",
    "batch_model  = build_model(mymodel, vocab_size, embed_dim, 1, usecuda)\n",
    "crit = nn.MSECriterion()\n",
    "\n",
    "qs = queries[3]\n",
    "x = data_file\n",
    "K = rK\n",
    "model = batch_model\n",
    "\n",
    "use_cuda = usecuda\n",
    "mxl = maxseqlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "...running on CPU\t\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if use_cuda then\n",
    "  Tensor = torch.CudaTensor\n",
    "  LongTensor = torch.CudaLongTensor\n",
    "  crit = crit:cuda()\n",
    "  print(\"...running on GPU\")\n",
    "else\n",
    "  Tensor = torch.Tensor\n",
    "  LongTensor = torch.LongTensor\n",
    "  print(\"...running on CPU\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yrouge = torch.totable(torch.randn(#x))\n",
    "action_list = torch.totable(torch.round(torch.rand(#x)))\n",
    "preds_list = torch.totable(torch.round(torch.rand(#x)))\n",
    "preds_list[1] = 0\n",
    "ss_list = grabNsamples(sent_file, 1, #sent_file)\n",
    "epoch = 0\n",
    "minibatch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = 0.                    --- Compute a new MSE loss each time\n",
    "--- Reset the rougue each epoch\n",
    "--- Looping over each bach of sentences for a given query\n",
    "nbatches = torch.floor( #x / batch_size)\n",
    "-- for minibatch = 1, nbatches do\n",
    "if minibatch == 1 then          -- Need +1 to skip the first row\n",
    "    nstart = 2\n",
    "    nend = torch.round(batch_size * minibatch)\n",
    "end\n",
    "if minibatch == nbatches then \n",
    "    nstart = nend + 1\n",
    "    nend = #x\n",
    "end\n",
    "if minibatch > 1 and minibatch < nbatches then \n",
    "    nstart = nend + 1\n",
    "    nend = torch.round(batch_size * minibatch)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nstart= nend+1\n",
    "nend = nend + batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  1 : \n",
       "    {\n",
       "      1 : 0.32737220974448\n",
       "    }\n",
       "  2 : \n",
       "    {\n",
       "      1 : 0.13720760825237\n",
       "    }\n",
       "  3 : \n",
       "    {\n",
       "      1 : 0.31425874763308\n",
       "    }\n",
       "  4 : \n",
       "    {\n",
       "      1 : 0.13559391343291\n",
       "    }\n",
       "  5 : \n",
       "    {\n",
       "      1 : 0.31896872188639\n",
       "    }\n",
       "}\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 0, epsilon =0.100, minibatch 1/2, sum(y)/len(y) = 499/499, {Recall = 0.838028, Precision = 0.149110, F1 = 0.253174}\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_ss  = geti_n(x, nstart, nend)\n",
    "xout  = grabNsamples(x_ss, 1, #x_ss)     --- Extracting N samples\n",
    "xs  = padZeros(xout, mxl)                 --- Padding the data by the maximum length\n",
    "qs2 = padZeros({qs}, 5)\n",
    "qrep = repeatQuery(qs2[1], #xs)\n",
    "preds = geti_n(preds_list, nstart, nend)\n",
    "\n",
    "sumry_list = buildKSummary(preds_list, ss_list, K)\n",
    "sumry_ss = geti_n(sumry_list, nstart, nend)\n",
    "\n",
    "summary = LongTensor(sumry_ss):t()\n",
    "sentences = LongTensor(xs):t()\n",
    "query = LongTensor(qrep):t()\n",
    "actions = Tensor(geti_n(preds_list, nstart, nend)):resize(#xs, 1)\n",
    "\n",
    "rscores, pscores, fscores = {}, {}, {}\n",
    "\n",
    "yrouge = torch.totable(torch.randn(#x))\n",
    "action_list = torch.totable(torch.round(torch.rand(#x)))\n",
    "preds_list = torch.totable(torch.round(torch.ones(#x)))\n",
    "ss_list = grabNsamples(sent_file, 1, #sent_file)\n",
    "\n",
    "if use_cuda then\n",
    "     actions =  actions:cuda()\n",
    "end\n",
    "\n",
    "myPreds = model:forward({sentences, summary, query, actions})\n",
    "print(geti_n(torch.totable(myPreds), 1 , 5) )\n",
    "\n",
    "if use_cuda then\n",
    "    myPreds = myPreds:double()\n",
    "end\n",
    "\n",
    "preds = policy(myPreds, epsilon, #xs)\n",
    "--- Concatenating predictions into a summary\n",
    "\n",
    "predsummary = buildPredSummary(preds, xs, K)\n",
    "--- Initializing rouge metrics at time {t-1} and save scores\n",
    "r_t1 , p_t1, f_t1 = 0., 0., 0.\n",
    "for i=1, #predsummary do\n",
    "    --- Calculating rouge scores; Call get_i_n() to cumulatively compute rouge\n",
    "    rscores[i] = rougeRecall(buildPredSummary(geti_n(preds, 1, i), geti_n(xout, 1, i)), nuggets) - r_t1\n",
    "    pscores[i] = rougePrecision(buildPredSummary(geti_n(preds, 1, i), geti_n(xout, 1, i)), nuggets) - p_t1\n",
    "    fscores[i] = rougeF1(buildPredSummary(geti_n(preds, 1, i), geti_n(xout, 1, i)), nuggets) - f_t1\n",
    "    r_t1, p_t1, f_t1 = rscores[i], pscores[i], fscores[i]\n",
    "end\n",
    "\n",
    "labels = Tensor(fscores)\n",
    "\n",
    "if use_cuda then\n",
    "     labels = labels:cuda()\n",
    "     myPreds = myPreds:cuda()\n",
    "end\n",
    "\n",
    "-- print(#myPreds, #labels)\n",
    "loss = loss + crit:forward(myPreds, labels)\n",
    "grads = crit:backward(myPreds, labels)\n",
    "model:zeroGradParameters()\n",
    "model:backward({sentences, summary, query, actions}, grads)\n",
    "model:updateParameters(learning_rate)        -- Update parameters after each minibatch\n",
    "\n",
    "yrouge = updateTable(yrouge, fscores, nstart)\n",
    "preds_list = updateTable(preds_list, preds, nstart)\n",
    "predsummary2 = buildPredSummary(preds_list, xs, K)\n",
    "\n",
    "rscore = rougeRecall(predsummary2, nuggets, K)\n",
    "pscore = rougePrecision(predsummary2, nuggets, K)\n",
    "fscore = rougeF1(predsummary2, nuggets, K)\n",
    "--         if (epoch % print_every)==0 then\n",
    "perf_string = string.format(\n",
    "    \"Epoch %i, epsilon =%.3f, minibatch %i/%i, sum(y)/len(y) = %i/%i, {Recall = %.6f, Precision = %.6f, F1 = %.6f}\", \n",
    "    epoch, epsilon, minibatch, nbatches, sumTable(preds), #preds, rscore, pscore, fscore\n",
    "    )\n",
    "print(perf_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  1 : 4\n",
       "  2 : 3\n",
       "  3 : 2\n",
       "}\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getLastKTokens({1,2,3,4}, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  1 : 0\n",
       "}\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_or_x({0}, {1,2,3,4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function buildPredSummary2(preds, xs, K)\n",
    "    local out = {}\n",
    "    for i=1, #xs do\n",
    "        if i == 1 then \n",
    "            out[i] = zero_or_x(preds[i], unpackZeros(xs[i]))\n",
    "        else \n",
    "            --- Update it by adding xs_i and out_{i-1}\n",
    "            out[i] =  getLastKTokens(tableConcat(zero_or_x(preds[i], \n",
    "                                    unpackZeros(xs[i]) ) , out[i-1]), K)\n",
    "        end\n",
    "    end\n",
    "    return out\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tst_preds = geti_n(preds_list, 1, 3)\n",
    "tst_xs = geti_n(xs, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_samples = grabNsamples(geti_n(x, 2, 4), nil, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  1 : \n",
       "    {\n",
       "      1 : 21\n",
       "      2 : 16\n",
       "      3 : 20\n",
       "    }\n",
       "  2 : \n",
       "    {\n",
       "      1 : 235\n",
       "      2 : 218\n",
       "      3 : 129\n",
       "    }\n",
       "  3 : \n",
       "    {\n",
       "      1 : 329\n",
       "      2 : 334\n",
       "      3 : 332\n",
       "    }\n",
       "}\n"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "-- unpackZeros(tst_xs[1]), unpackZeros(tst_xs[2]), unpackZeros(tst_xs[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tst = buildPredSummary2(geti_n(preds_list, 2, 4), test_samples, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  1 : \n",
       "    {\n",
       "      1 : 21\n",
       "      2 : 16\n",
       "      3 : 20\n",
       "    }\n",
       "  2 : \n",
       "    {\n",
       "      1 : 20\n",
       "      2 : 16\n",
       "      3 : 21\n",
       "      4 : 129\n",
       "    }\n",
       "  3 : \n",
       "    {\n",
       "      1 : 129\n",
       "      2 : 21\n",
       "      3 : 16\n",
       "      4 : 20\n",
       "    }\n",
       "}\n"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mx = 0\n",
    "for k,v in pairs(tst) do\n",
    "    mx = math.max(#v, mx)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query:t()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary:t()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences:t()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myPreds2 = model:forward({sentences, summary, query, actions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "myPreds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sent_test = LongTensor{{0, 0}, {9, 0}}\n",
    "sumry_test = LongTensor{{0, 1, 3, 5}, {4, 8, 3, 0}}\n",
    "query_test = LongTensor{{0, 1, 3, 5}, {4, 8, 3, 1}}\n",
    "action_test = Tensor({1, 0}):resize(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "action_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model:forward({sent_test:t(), sumry_test:t(), query_test:t(), action_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mod = nn.Sequential()\n",
    ":add(nn.LookupTableMaskZero(5, 5))\n",
    ":add(nn.SplitTable(1,2))  -- convert tensor to list of subtensors\n",
    ":add(nn.Sequencer(nn.MaskZero(nn.FastLSTM(5, 3), 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xin = torch.LongTensor{{0, 1, 3, 5}, {0, 0, 4, 2}}:t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mod:forward(xin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
