{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  1 : ..\n",
       "  2 : Code\n",
       "  3 : .\n",
       "  4 : data\n",
       "  5 : .gitignore\n",
       "  6 : .git\n",
       "  7 : Presentation\n",
       "  8 : Untitled.ipynb\n",
       "  9 : README.md\n",
       "  10 : .ipynb_checkpoints\n",
       "  11 : Paper\n",
       "}\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths.dir(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "...Utils file loaded\t\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "...Utils file loaded\t\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dofile(\"Code/utils.lua\")\n",
    "dofile(\"Code/Utils/load_cnn.lua\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data loaded\t\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputpath = '/home/francisco/GitHub/DQN-Event-Summarization/data/training/'\n",
    "queries, sentences, trueSummaries = loadCNN(outputpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabsize = 0\n",
    "for i=1, 124 do\n",
    "    vocabsize = math.max(queries[i]:max(), sentences[i]:max(), trueSummaries[i]:max(), vocabsize)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "require 'os'\n",
    "require 'nn'\n",
    "require 'rnn'\n",
    "require 'cunn'\n",
    "require 'cunnx'\n",
    "require 'optim'\n",
    "require 'cutorch'\n",
    "require 'parallel'\n",
    "\n",
    "dl = require 'dataload'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function buildModel(model, vocabSize, embeddingSize, metric, adapt, use_cuda)\n",
    "    -- Small experiments seem to show that the Tanh activations performed better\\\n",
    "    --      than the ReLU for the bow model\n",
    "    if model == 'bow' then\n",
    "        print(string.format(\"Running bag-of-words model to learn %s\", metric))\n",
    "        sentenceLookup = nn.Sequential()\n",
    "                    :add(nn.LookupTableMaskZero(vocabSize, embeddingSize))\n",
    "                    -- Not averaging blows up model so keep this true\n",
    "                    :add(nn.Sum(2, 3, true)) \n",
    "                    :add(nn.Tanh())\n",
    "    else\n",
    "    -- This needs to have a transpose in the model\n",
    "    -- lstms go \n",
    "        print(string.format(\"Running LSTM model to learn %s\", metric))\n",
    "        sentenceLookup = nn.Sequential()\n",
    "                    :add(nn.LookupTableMaskZero(vocabSize, embeddingSize))\n",
    "                    :add(nn.SplitTable(2))\n",
    "                    :add(nn.Sequencer(nn.LSTM(embeddingSize, embeddingSize)))\n",
    "                    :add(nn.SelectTable(-1))            -- selects last state of the LSTM\n",
    "                    :add(nn.Linear(embeddingSize, embeddingSize))\n",
    "                    :add(nn.ReLU())\n",
    "    end\n",
    "    local queryLookup = sentenceLookup:clone(\"weight\", \"gradWeight\") \n",
    "    local summaryLookup = sentenceLookup:clone(\"weight\", \"gradWeight\")\n",
    "    local pmodule = nn.ParallelTable()\n",
    "                :add(queryLookup)\n",
    "                :add(sentenceLookup)\n",
    "                :add(summaryLookup)\n",
    "\n",
    "    if model == 'bow' then\n",
    "        nnmodel = nn.Sequential()\n",
    "            :add(pmodule)\n",
    "            :add(nn.JoinTable(2))\n",
    "            :add(nn.Tanh())\n",
    "            :add(nn.Linear(embeddingSize * 3, 2))\n",
    "    else\n",
    "        nnmodel = nn.Sequential()\n",
    "            :add(pmodule)\n",
    "            :add(nn.JoinTable(2))\n",
    "            :add(nn.ReLU())\n",
    "            :add(nn.Linear(embeddingSize * 3, 2))\n",
    "    end\n",
    "\n",
    "    if adapt then \n",
    "        print(\"Adaptive regularization\")\n",
    "        local logmod = nn.Sequential()\n",
    "            :add(nn.Linear(embeddingSize * 3, 1))\n",
    "            :add(nn.LogSigmoid())\n",
    "            :add(nn.SoftMax())\n",
    "\n",
    "        local regmod = nn.Sequential()\n",
    "            :add(nn.Linear(embeddingSize * 3, 2))\n",
    "\n",
    "        local fullmod = nn.ConcatTable()\n",
    "            :add(regmod)\n",
    "            :add(logmod)\n",
    "\n",
    "        local final = nn.Sequential()\n",
    "            :add(pmodule)\n",
    "            :add(nn.JoinTable(2))\n",
    "            :add(fullmod)\n",
    "\n",
    "        nnmodel = final\n",
    "    end\n",
    "\n",
    "    if use_cuda then\n",
    "        return nnmodel:cuda()\n",
    "    end\n",
    "    return nnmodel\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "function Tokenize(inputdic)\n",
    "    --- This function tokenizes the words into a unigram dictionary\n",
    "    local out = {}\n",
    "    for k, v in pairs(inputdic) do\n",
    "        if v ~= 0 then \n",
    "            if out[v] == nil then\n",
    "                out[v] = 1\n",
    "            else \n",
    "                out[v] = 1 + out[v]\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return out\n",
    "end\n",
    "\n",
    "function rougeScores(genSummary, refSummary)\n",
    "    local genTotal = 0\n",
    "    local refTotal = 0\n",
    "    local intersection = 0\n",
    "    -- Inserting the missing keys\n",
    "    for k, genCount in pairs(genSummary) do\n",
    "        if refSummary[k] == nil then\n",
    "            refSummary[k] = 0\n",
    "        end\n",
    "    end\n",
    "    for k, refCount in pairs(refSummary) do\n",
    "        local genCount = genSummary[k]\n",
    "        if genCount == nil then \n",
    "            genCount = 0 \n",
    "        end\n",
    "        intersection = intersection + math.min(refCount, genCount)\n",
    "        refTotal = refTotal + refCount\n",
    "        genTotal = genTotal + genCount\n",
    "    end\n",
    "\n",
    "    recall = intersection / refTotal\n",
    "    prec = intersection / genTotal\n",
    "    if refTotal == 0 then\n",
    "        recall = 0\n",
    "    end \n",
    "    if genTotal == 0 then\n",
    "        prec = 0\n",
    "    end\n",
    "    -- tmp = {intersection, refTotal, genTotal}\n",
    "    if recall > 0 or prec > 0 then\n",
    "        f1 = (2 * recall * prec) / (recall + prec)\n",
    "    else \n",
    "        f1 = 0\n",
    "    end\n",
    "    return recall, prec, f1\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function buildPredsummaryFast(chosenactions, inputsentences, select_index)\n",
    "    local n = inputsentences:size(1)\n",
    "    local k = inputsentences:size(2)\n",
    "    local summary = torch.zeros(inputsentences:size())\n",
    "    actionmatrix = chosenactions:select(2, select_index):clone():resize(n, 1):view(n, 1):expand(n, k):clone()\n",
    "    --     This line didn't work for whatever reason...gives weird indexing...\n",
    "    return actionmatrix:cmul(inputsentences:double())\n",
    "end\n",
    "\n",
    "function buildTotalSummaryFast(predsummary, inputTotalSummary, usecuda)\n",
    "    tmpSummary = inputTotalSummary:clone()\n",
    "    nps = predsummary:size(1)\n",
    "    n_l = inputTotalSummary:size(2)    \n",
    "    indices = torch.linspace(1, n_l, n_l):long()\n",
    "    if usecuda then\n",
    "        indices = indices:cuda()\n",
    "    end\n",
    "    for i=1, predsummary:size(1) do\n",
    "        if predsummary[i]:sum() > 0 then\n",
    "            -- Finding the largest index with a zero\n",
    "            -- maxindex = torch.max(indices[torch.eq(tmpSummary[i], 0)])\n",
    "            -- lenx = predsummary[i]:size(1)\n",
    "            -- tmpSummary[i][{{maxindex - lenx + 1, maxindex}}]:copy(predsummary[i])\n",
    "            -- Finding the smallest index with a zero\n",
    "            minindex = torch.min(indices[torch.eq(tmpSummary[i], 0)])\n",
    "            lenx = predsummary[i]:size(1)\n",
    "            tmpSummary[i][{{minindex, minindex + lenx - 1}}]:copy(predsummary[i])\n",
    "        end\n",
    "    end\n",
    "    return tmpSummary\n",
    "end\n",
    "\n",
    "function runSimulation(n, n_s, q, k, a, b, learning_rate, embDim, gamma, batch_size, nepochs, epsilon, print_perf, mem_multiplier, cuts, base_explore_rate, endexplorerate, adapt, adapt_lambda, usecuda, seedval)\n",
    "    -- torch.setnumthreads(16)\n",
    "    torch.manualSeed(seedval)\n",
    "    if usecuda then\n",
    "        Tensor = torch.CudaTensor\n",
    "        LongTensor = torch.CudaLongTensor   \n",
    "        ByteTensor = torch.CudaByteTensor\n",
    "        maskLayer = nn.MaskedSelect():cuda()\n",
    "        print(\"...running on GPU\")\n",
    "    else\n",
    "        Tensor = torch.Tensor\n",
    "        LongTensor = torch.LongTensor\n",
    "        ByteTensor = torch.ByteTensor\n",
    "        maskLayer = nn.MaskedSelect()\n",
    "        print(\"...running on CPU\")\n",
    "    end\n",
    "\n",
    "    local SKIP = 1\n",
    "    local SELECT = 2\n",
    "\n",
    "    optimParams = { learningRate = learning_rate }\n",
    "    delta = cuts / nepochs\n",
    "    end_baserate = torch.round(nepochs * endexplorerate )\n",
    "\n",
    "    -- Simulating streams and queries\n",
    "    queries = genNbyK(n, q, a, b)\n",
    "\n",
    "    -- Note that the sentences are batched by sentence index so sentences[1] is the first sentence of each article\n",
    "    sentences = {}\n",
    "    for i=1, n_s do\n",
    "        sentences[i] = genNbyK(n, k, a, b)\n",
    "    end\n",
    "\n",
    "    -- Optimal predicted summary\n",
    "    trueSummary = LongTensor(n, k * n_s):fill(0)\n",
    "\n",
    "    -- Using this to generate the optimal actions\n",
    "    true_actions = {}\n",
    "    for i=1, n_s do \n",
    "        ---- Simulating the data\n",
    "        trueqValues = torch.rand(n, 2)\n",
    "         ---- Generating the max values and getting the indices\n",
    "        qMaxtrue, qindxtrue = torch.max(trueqValues, 2)\n",
    "        \n",
    "        --- I want to select the qindx elements for each row\n",
    "        true_actions[i] = torch.zeros(n, 2):scatter(2, qindxtrue, torch.ones(trueqValues:size()))\n",
    "        best_sentences = buildPredsummaryFast(true_actions[i], sentences[i], SELECT)\n",
    "        trueSummary = buildTotalSummaryFast(best_sentences, trueSummary, usecuda)\n",
    "    end\n",
    "\n",
    "    qTokens = {}\n",
    "    for i=1, n do\n",
    "        qTokens[i] = Tokenize(trueSummary[i]:totable())\n",
    "    end\n",
    "\n",
    "    -- Building the model\n",
    "    model = buildModel('bow', b, embDim, 'f1', adapt, usecuda)\n",
    "    params, gradParams = model:getParameters()\n",
    "\n",
    "    if adapt then \n",
    "        criterion = nn.ParallelCriterion():add(nn.MSECriterion()):add(nn.BCECriterion())\n",
    "        criterion[\"weights\"] = {1, adapt_lambda}\n",
    "    else \n",
    "        criterion = nn.MSECriterion()\n",
    "    end \n",
    "\n",
    "    qValues = {}\n",
    "    qActions = {}\n",
    "    qPreds = {}\n",
    "    rewards = {}\n",
    "    lossfull = {}\n",
    "    rouguef1 = {}\n",
    "    rougue_scores = {}\n",
    "\n",
    "    totalPredsummary = LongTensor(n, n_s * k):fill(0)\n",
    "\n",
    "    memfull = false\n",
    "    curr_memsize = 0\n",
    "    memsize = n * n_s * mem_multiplier\n",
    "    queryMemory = Tensor(memsize, q):fill(0)\n",
    "    qActionMemory = Tensor(memsize, 2):fill(0)\n",
    "    predSummaryMemory = Tensor(memsize, n_s * k):fill(0)\n",
    "    sentenceMemory = Tensor(memsize, k):fill(0)\n",
    "    sentencetp1Memory  = Tensor(memsize, k):fill(0)\n",
    "    predSummarytp1Memory = Tensor(memsize, n_s * k):fill(0)\n",
    "    qPredsMemory = Tensor(memsize, 2):fill(0)\n",
    "    qValuesMemory = Tensor(memsize, 1):fill(0)\n",
    "    rewardMemory = Tensor(memsize, 1):fill(0)\n",
    "\n",
    "    if adapt then\n",
    "        regPreds = {}\n",
    "        regMemory = Tensor(memsize, 1):fill(0) \n",
    "    end\n",
    "    --- Initializing thingss\n",
    "    for i = 1, n_s do\n",
    "        qPreds[i] = Tensor(n, 2):fill(0) \n",
    "        qValues[i] = Tensor(n, 1):fill(0)\n",
    "        qActions[i] = Tensor(n, 2):fill(0)\n",
    "        rewards[i] = Tensor(n, 1):fill(0)\n",
    "        rougue_scores[i] = Tensor(n, 1):fill(0)\n",
    "        if adapt then\n",
    "            regPreds[i] = Tensor(n, 1):fill(0)\n",
    "        end        \n",
    "    end \n",
    "\n",
    "    if usecuda then\n",
    "        criterion = criterion:cuda()\n",
    "        model = model:cuda()\n",
    "    end\n",
    "\n",
    "    nClock = os.clock()\n",
    "    for epoch=1, nepochs do\n",
    "        --- Reset things at the start of each epoch\n",
    "        for i=1, n_s do\n",
    "            qPreds[i]:fill(0)\n",
    "            qValues[i]:fill(0)\n",
    "            qActions[i]:fill(0)\n",
    "            rewards[i]:fill(0)\n",
    "            rougue_scores[i]:fill(0)\n",
    "            totalPredsummary:fill(0)\n",
    "            if adapt then\n",
    "                regMemory[i]:fill(0)\n",
    "            end        \n",
    "        end\n",
    "\n",
    "        for i=1, n_s do\n",
    "            totalPreds = model:forward({queries, sentences[i], totalPredsummary})\n",
    "\n",
    "            if adapt then \n",
    "                qPreds[i]:copy(totalPreds[1])\n",
    "                regPreds[i]:copy(totalPreds[2])\n",
    "            else\n",
    "                qPreds[i]:copy(totalPreds)\n",
    "            end\n",
    "\n",
    "            if torch.uniform(0, 1) <= epsilon then\n",
    "                -- randomly choosing actions\n",
    "                xrand = torch.rand(qPreds[i]:size())\n",
    "                qActions[i]:select(2, SELECT):copy(torch.ge(xrand:select(2, SELECT), xrand:select(2, SKIP)))\n",
    "                qActions[i]:select(2, SKIP):copy(torch.ge(xrand:select(2, SKIP), xrand:select(2, SELECT)))\n",
    "                qValues[i]:copy( maskLayer:forward({totalPreds, qActions[i]:byte()}) )\n",
    "            else \n",
    "                qMax, qindx = torch.max(qPreds[i], 2)  -- Pulling the best actions\n",
    "                -- Here's the fast way to select the optimal action for each query\n",
    "                qActions[i]:copy(\n",
    "                    qActions[i]:scatter(2, qindx, torch.ones(qPreds[i]:size())):clone()\n",
    "                )\n",
    "                qValues[i]:copy(\n",
    "                    qMax\n",
    "                )\n",
    "            end\n",
    "\n",
    "            -- This is where we begin to store the data in our memory \n",
    "                -- notice that we store the reward after this part\n",
    "            start_row = curr_memsize + 1\n",
    "            if memsize < (start_row + n) then \n",
    "                start_row = memsize - n + 1\n",
    "                end_row = start_row + n - 1\n",
    "                curr_memsize = 0\n",
    "                if (end_row + n) >= memsize then \n",
    "                    memfull = true\n",
    "                end \n",
    "            else \n",
    "                end_row = start_row + n - 1\n",
    "                curr_memsize = end_row\n",
    "            end\n",
    "\n",
    "            -- Update memory sequentially until it's full then restart updating it\n",
    "            queryMemory[{{start_row, end_row}}]:copy(queries)\n",
    "            sentenceMemory[{{start_row, end_row}}]:copy(sentences[i])\n",
    "            predSummaryMemory[{{start_row, end_row}}]:copy(totalPredsummary)\n",
    "            \n",
    "            -- Now that we've stored our memory, we can build the summary to evaluate our action\n",
    "            predsummary = buildPredsummaryFast(qActions[i], sentences[i], SELECT)\n",
    "            totalPredsummary = buildTotalSummaryFast(predsummary, totalPredsummary, usecuda)\n",
    "            \n",
    "            if i < n_s then\n",
    "                sentencetp1Memory[{{start_row, end_row}}]:copy(sentences[i + 1])\n",
    "                predSummarytp1Memory[{{start_row, end_row}}]:copy(totalPredsummary)\n",
    "            else \n",
    "                sentencetp1Memory[{{start_row, end_row}}]:copy(Tensor(sentences[i]:size()):fill(0) )\n",
    "                predSummarytp1Memory[{{start_row, end_row}}]:copy(Tensor(totalPredsummary:size()):fill(0) )\n",
    "            end \n",
    "            \n",
    "            qActionMemory[{{start_row, end_row}}]:copy(qActions[i])\n",
    "            qPredsMemory[{{start_row, end_row}}]:copy(qPreds[i])\n",
    "            qValuesMemory[{{start_row, end_row}}]:copy(qValues[i])\n",
    "\n",
    "            if adapt then\n",
    "                regMemory[{{start_row, end_row}}]:copy(regPreds[i])\n",
    "            end\n",
    "\n",
    "            for j = 1, n do\n",
    "                recall, prec, f1 = rougeScores( Tokenize(totalPredsummary[j]:totable()),\n",
    "                                                qTokens[j]\n",
    "                    )\n",
    "                rougue_scores[i][j]:fill(f1)\n",
    "            end\n",
    "\n",
    "            if i == n_s then \n",
    "                rouguef1[epoch] = rougue_scores[i]:mean()\n",
    "            end \n",
    "\n",
    "            if i == 1 then\n",
    "                -- Calculating change in rougue f1\n",
    "                rewards[i]:copy(rougue_scores[i])\n",
    "            else \n",
    "                rewards[i]:copy(rougue_scores[i] - rougue_scores[i-1])\n",
    "            end\n",
    "            rewardMemory[{{start_row, end_row}}]:copy(rewards[i])\n",
    "        end\n",
    "\n",
    "        if memfull then \n",
    "            memrows = memsize\n",
    "        else \n",
    "            memrows = curr_memsize\n",
    "        end\n",
    "\n",
    "        if usecuda then \n",
    "            dataloader = dl.TensorLoader({\n",
    "                            queryMemory[{{1, memrows}}]:cuda(), \n",
    "                            sentenceMemory[{{1, memrows}}]:cuda(), \n",
    "                            predSummaryMemory[{{1, memrows}}]:cuda(),\n",
    "                            qPredsMemory[{{1, memrows}}]:cuda(), \n",
    "                            ByteTensor(memrows, 2):copy(qActionMemory[{{1, memrows}}]), \n",
    "                            qValuesMemory[{{1, memrows}}]:cuda(),\n",
    "                            sentencetp1Memory[{{1, memrows}}]:cuda(),\n",
    "                            predSummarytp1Memory[{{1, memrows}}]:cuda()               \n",
    "                                }, \n",
    "                        rewardMemory[{{1, memrows}}]:cuda()\n",
    "                    )\n",
    "            if adapt then            \n",
    "                table.insert(dataloader['inputs'], regMemory[{{1, memrows}}]:cuda() )\n",
    "            end\n",
    "        else \n",
    "            dataloader = dl.TensorLoader({\n",
    "                        queryMemory[{{1, memrows}}], \n",
    "                        sentenceMemory[{{1, memrows}}], \n",
    "                        predSummaryMemory[{{1, memrows}}], \n",
    "                        qPredsMemory[{{1, memrows}}], \n",
    "                        ByteTensor(memrows, 2):copy(qActionMemory[{{1, memrows}}]), \n",
    "                        qValuesMemory[{{1, memrows}}],\n",
    "                        sentencetp1Memory[{{1, memrows}}],\n",
    "                        predSummarytp1Memory[{{1, memrows}}]                    \n",
    "                        }, \n",
    "                    rewardMemory[{{1, memrows}}]\n",
    "                )\n",
    "            if adapt then\n",
    "                table.insert(dataloader['inputs'], regMemory[{{1, memrows}}] )\n",
    "            end\n",
    "        end\n",
    "        loss = {}\n",
    "        c = 1\n",
    "        for k, xin, reward in dataloader:sampleiter(batch_size, memsize) do\n",
    "            local function feval(params)\n",
    "                gradParams:zero()\n",
    "                if adapt then\n",
    "                    local predtp1 = model:forward({xin[1], xin[7], xin[8]})\n",
    "                    local predQOnActions = maskLayer:forward({xin[4], xin[5]}) \n",
    "                    local ones = torch.ones(reward:size(1)):resize(reward:size(1))\n",
    "                    if usecuda then\n",
    "                        ones = ones:cuda()\n",
    "                    end\n",
    "                    lossf = criterion:forward({predQOnActions, xin[7]}, {reward, ones})\n",
    "                    local gradOutput = criterion:backward({predQOnActions, xin[6]}, {reward, ones})\n",
    "                    local gradMaskLayer = maskLayer:backward({xin[4], xin[5]}, gradOutput[1])\n",
    "                    model:backward({xin[1], xin[2], xin[3]}, {gradMaskLayer[1], gradOutput[2]})\n",
    "                else\n",
    "                    model:forget()\n",
    "                    local predtp1 = model:forward({xin[1], xin[7], xin[8]})\n",
    "                    local predtp1max, _ = torch.max(predtp1, 2)\n",
    "                    model:forget()\n",
    "                    local predt = model:forward({xin[1], xin[2], xin[3]})\n",
    "                    local y_j = reward + (gamma * predtp1max) \n",
    "                    local predQOnActions = maskLayer:forward({predt, xin[5]}) \n",
    "                    lossf = criterion:forward(predQOnActions, y_j )\n",
    "                    local gradOutput = criterion:backward(predQOnActions, y_j)\n",
    "                    local gradMaskLayer = maskLayer:backward({predt, xin[5]}, gradOutput)\n",
    "                    model:backward({xin[1], xin[2], xin[3]}, gradMaskLayer[1])\n",
    "\n",
    "                end \n",
    "                return lossf, gradParams\n",
    "            end\n",
    "            --- optim.rmsprop returns \\theta, f(\\theta):= loss function\n",
    "             _, lossv  = optim.rmsprop(feval, params, optimParams)\n",
    "            loss[c] = lossv[1]\n",
    "            c = c + 1\n",
    "        end\n",
    "\n",
    "        lossfull[epoch] = Tensor(loss):sum() / #lossv\n",
    "        if print_perf then\n",
    "            print(\n",
    "                string.format('epoch = %i; rougue = %.6f; epsilon = %.6f; loss = %.6f' , \n",
    "                    epoch, rouguef1[epoch], epsilon, lossfull[epoch])\n",
    "                )\n",
    "        end\n",
    "\n",
    "        if (epsilon - delta) <= base_explore_rate then\n",
    "            epsilon = base_explore_rate\n",
    "            if epoch > end_baserate then \n",
    "                base_explore_rate = 0.\n",
    "            end\n",
    "        else \n",
    "            epsilon = epsilon - delta\n",
    "        end\n",
    "\n",
    "    end\n",
    "    print(string.format(\"Elapsed time: %.5f\" % (os.clock()-nClock) ))\n",
    "    print(\n",
    "        string.format('First rougue = %.6f; Last rougue = %.6f',\n",
    "            rouguef1[1], rouguef1[nepochs]) \n",
    "        )\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
